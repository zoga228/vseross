{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "15807ca0-a0bf-4929-97b6-74bb14b4db67",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Использование предобученной модели из transformers для получения embeddings\n",
    "from transformers import AutoModel, AutoTokenizer\n",
    "import torch\n",
    "\n",
    "# Загружаем предобученную модель и токенизатор\n",
    "model_name = \"bert-base-uncased\"\n",
    "tokenizer = AutoTokenizer.from_pretrained(model_name)\n",
    "model = AutoModel.from_pretrained(model_name)\n",
    "\n",
    "# Подготавливаем текст\n",
    "text = \"Пример текста для получения векторного представления\"\n",
    "inputs = tokenizer(text, return_tensors=\"pt\")\n",
    "\n",
    "# Получаем embeddings (последний скрытый слой)\n",
    "with torch.no_grad():\n",
    "    outputs = model(**inputs)\n",
    "    \n",
    "# Берем состояние последнего слоя\n",
    "last_hidden_state = outputs.last_hidden_state\n",
    "\n",
    "# Для представления всего предложения можно использовать среднее значение\n",
    "sentence_embedding = torch.mean(last_hidden_state, dim=1)\n",
    "\n",
    "print(f\"Размерность embedding: {sentence_embedding.shape}\")\n",
    "print(f\"Embedding: {sentence_embedding[0][:5]}\")  # Показываем первые 5 значений"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "71bcbf85-f9d3-4f8f-abb6-a4cd1ca85483",
   "metadata": {},
   "outputs": [],
   "source": [
    "import torch\n",
    "import numpy as np\n",
    "from transformers import AutoModel, AutoTokenizer\n",
    "from sklearn.model_selection import train_test_split\n",
    "from sklearn.linear_model import LogisticRegression\n",
    "from sklearn.metrics import accuracy_score, classification_report\n",
    "\n",
    "# Функция для получения embeddings из BERT\n",
    "def get_bert_embeddings(texts, model, tokenizer, device=\"cpu\"):\n",
    "    embeddings = []\n",
    "    \n",
    "    for text in texts:\n",
    "        # Токенизация\n",
    "        encoded_input = tokenizer(text, padding=True, truncation=True, \n",
    "                                  max_length=128, return_tensors='pt')\n",
    "        # Перемещаем на нужное устройство\n",
    "        encoded_input = {k: v.to(device) for k, v in encoded_input.items()}\n",
    "        \n",
    "        # Получаем выходные данные модели\n",
    "        with torch.no_grad():\n",
    "            model_output = model(**encoded_input)\n",
    "            \n",
    "        # Извлекаем последний скрытый слой\n",
    "        last_hidden_state = model_output.last_hidden_state\n",
    "        \n",
    "        # Используем [CLS] токен (первый токен) как представление предложения\n",
    "        sentence_embedding = last_hidden_state[:, 0, :].cpu().numpy()\n",
    "        embeddings.append(sentence_embedding[0])\n",
    "    \n",
    "    return np.array(embeddings)\n",
    "\n",
    "# Загрузка модели\n",
    "model_name = \"bert-base-uncased\"\n",
    "tokenizer = AutoTokenizer.from_pretrained(model_name)\n",
    "model = AutoModel.from_pretrained(model_name)\n",
    "device = torch.device(\"cuda\" if torch.cuda.is_available() else \"cpu\")\n",
    "model.to(device)\n",
    "\n",
    "# Пример данных\n",
    "texts = [\n",
    "    \"I love this movie, it's amazing!\",\n",
    "    \"This film is terrible, I hated it.\",\n",
    "    \"What a great movie, I enjoyed every minute.\",\n",
    "    \"Absolutely disappointing, waste of time.\",\n",
    "    \"This is the best film I've seen in years.\",\n",
    "    \"The worst movie ever, avoid at all costs.\"\n",
    "]\n",
    "labels = [1, 0, 1, 0, 1, 0]  # 1 - положительный, 0 - отрицательный\n",
    "\n",
    "# Получаем embeddings\n",
    "embeddings = get_bert_embeddings(texts, model, tokenizer, device)\n",
    "\n",
    "# Разделяем данные на обучающую и тестовую выборки\n",
    "X_train, X_test, y_train, y_test = train_test_split(\n",
    "    embeddings, labels, test_size=0.3, random_state=42\n",
    ")\n",
    "\n",
    "# Обучаем простой классификатор\n",
    "clf = LogisticRegression()\n",
    "clf.fit(X_train, y_train)\n",
    "\n",
    "# Оцениваем результаты\n",
    "y_pred = clf.predict(X_test)\n",
    "accuracy = accuracy_score(y_test, y_pred)\n",
    "report = classification_report(y_test, y_pred)\n",
    "\n",
    "print(f\"Accuracy: {accuracy:.4f}\")\n",
    "print(\"Classification Report:\")\n",
    "print(report)\n",
    "\n",
    "# Классифицируем новый текст\n",
    "new_text = \"I watched this movie yesterday and really liked it.\"\n",
    "new_embedding = get_bert_embeddings([new_text], model, tokenizer, device)\n",
    "prediction = clf.predict(new_embedding)\n",
    "print(f\"Predicted sentiment for new text: {'Positive' if prediction[0] == 1 else 'Negative'}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "c9b8cddf-e126-41f2-8096-f6975c561f59",
   "metadata": {},
   "outputs": [],
   "source": [
    "import torch\n",
    "import torch.nn as nn\n",
    "import numpy as np\n",
    "from transformers import AutoModel, AutoTokenizer\n",
    "from sklearn.metrics.pairwise import cosine_similarity\n",
    "import matplotlib.pyplot as plt\n",
    "\n",
    "class AdvancedEmbeddingExtractor:\n",
    "    def __init__(self, model_name=\"bert-base-uncased\", device=None):\n",
    "        self.tokenizer = AutoTokenizer.from_pretrained(model_name)\n",
    "        self.model = AutoModel.from_pretrained(model_name, output_hidden_states=True)\n",
    "        self.device = device if device else torch.device(\"cuda\" if torch.cuda.is_available() else \"cpu\")\n",
    "        self.model.to(self.device)\n",
    "        self.model.eval()\n",
    "        \n",
    "        # Веса для разных слоев (можно оптимизировать под конкретную задачу)\n",
    "        # Здесь мы даем больший вес верхним слоям\n",
    "        num_layers = self.model.config.num_hidden_layers + 1  # +1 для embedding слоя\n",
    "        self.layer_weights = torch.nn.Parameter(\n",
    "            torch.tensor([i/num_layers for i in range(num_layers)], device=self.device)\n",
    "        )\n",
    "        self.layer_weights = torch.softmax(self.layer_weights, dim=0)\n",
    "        \n",
    "    def get_embeddings(self, texts, pooling_strategy=\"mean\", layers=\"all\"):\n",
    "        \"\"\"\n",
    "        Получение embeddings для текстов с разными стратегиями объединения.\n",
    "        \n",
    "        Args:\n",
    "            texts: Список текстов\n",
    "            pooling_strategy: Стратегия объединения токенов ('cls', 'mean', 'max', 'attention')\n",
    "            layers: Какие слои использовать ('all', 'last', or list of indices)\n",
    "            \n",
    "        Returns:\n",
    "            Numpy array с embeddings\n",
    "        \"\"\"\n",
    "        batch_embeddings = []\n",
    "        \n",
    "        for text in texts:\n",
    "            # Токенизация\n",
    "            inputs = self.tokenizer(text, return_tensors=\"pt\", padding=True, truncation=True, max_length=512)\n",
    "            inputs = {k: v.to(self.device) for k, v in inputs.items()}\n",
    "            \n",
    "            # Получаем выходы со всех слоев\n",
    "            with torch.no_grad():\n",
    "                outputs = self.model(**inputs)\n",
    "                \n",
    "            # Получаем hidden states со всех слоев\n",
    "            hidden_states = outputs.hidden_states\n",
    "            \n",
    "            # Выбираем слои для использования\n",
    "            if layers == \"last\":\n",
    "                selected_layers = [hidden_states[-1]]\n",
    "            elif layers == \"all\":\n",
    "                selected_layers = hidden_states\n",
    "            else:\n",
    "                selected_layers = [hidden_states[i] for i in layers]\n",
    "                \n",
    "            # Объединяем выбранные слои с весами\n",
    "            if len(selected_layers) == 1:\n",
    "                combined_states = selected_layers[0]\n",
    "            else:\n",
    "                # Используем веса для слоев\n",
    "                weights = self.layer_weights[-len(selected_layers):]\n",
    "                weights = weights / weights.sum()  # Нормализуем веса\n",
    "                \n",
    "                combined_states = torch.zeros_like(selected_layers[0])\n",
    "                for i, layer in enumerate(selected_layers):\n",
    "                    combined_states += layer * weights[i].item()\n",
    "            \n",
    "            # Применяем стратегию объединения токенов\n",
    "            if pooling_strategy == \"cls\":\n",
    "                # Используем токен [CLS] (первый токен)\n",
    "                embedding = combined_states[0, 0, :]\n",
    "            elif pooling_strategy == \"mean\":\n",
    "                # Среднее по всем токенам (исключая padding)\n",
    "                attention_mask = inputs['attention_mask']\n",
    "                embedding = self._mean_pooling(combined_states, attention_mask)\n",
    "            elif pooling_strategy == \"max\":\n",
    "                # Максимальное значение по каждой размерности\n",
    "                embedding = torch.max(combined_states[0], dim=0)[0]\n",
    "            elif pooling_strategy == \"attention\":\n",
    "                # Weighted pooling с использованием self-attention\n",
    "                embedding = self._attention_pooling(combined_states[0], outputs.attentions[-1])\n",
    "            else:\n",
    "                raise ValueError(f\"Unsupported pooling strategy: {pooling_strategy}\")\n",
    "                \n",
    "            batch_embeddings.append(embedding.cpu().numpy())\n",
    "            \n",
    "        return np.array(batch_embeddings)\n",
    "    \n",
    "    def _mean_pooling(self, token_embeddings, attention_mask):\n",
    "        # Среднее значение по токенам, с учетом маски внимания\n",
    "        input_mask_expanded = attention_mask.unsqueeze(-1).expand(token_embeddings.size()).float()\n",
    "        sum_embeddings = torch.sum(token_embeddings * input_mask_expanded, 1)\n",
    "        sum_mask = torch.clamp(input_mask_expanded.sum(1), min=1e-9)\n",
    "        return sum_embeddings / sum_mask\n",
    "    \n",
    "    def _attention_pooling(self, token_embeddings, attention):\n",
    "        # Используем веса из последнего слоя внимания для объединения токенов\n",
    "        # Берем среднее по головам внимания\n",
    "        mean_attention = torch.mean(attention[0], dim=0)\n",
    "        # Используем веса из первой строки (соответствующей [CLS] токену)\n",
    "        cls_attention_weights = mean_attention[0]\n",
    "        weighted_sum = torch.matmul(cls_attention_weights.unsqueeze(0), token_embeddings)\n",
    "        return weighted_sum.squeeze(0)\n",
    "    \n",
    "    def visualize_embeddings(self, texts, labels=None):\n",
    "        \"\"\"\n",
    "        Визуализация embeddings с помощью t-SNE\n",
    "        \"\"\"\n",
    "        from sklearn.manifold import TSNE\n",
    "        \n",
    "        embeddings = self.get_embeddings(texts)\n",
    "        \n",
    "        # Применяем t-SNE для уменьшения размерности до 2D\n",
    "        tsne = TSNE(n_components=2, random_state=42)\n",
    "        embeddings_2d = tsne.fit_transform(embeddings)\n",
    "        \n",
    "        # Визуализация\n",
    "        plt.figure(figsize=(10, 8))\n",
    "        scatter = plt.scatter(embeddings_2d[:, 0], embeddings_2d[:, 1], \n",
    "                             c=labels if labels else None)\n",
    "        \n",
    "        if labels is not None and len(set(labels)) < 10:\n",
    "            # Добавляем легенду, если количество уникальных меток небольшое\n",
    "            plt.legend(handles=scatter.legend_elements()[0], \n",
    "                      labels=[f\"Class {i}\" for i in set(labels)])\n",
    "        \n",
    "        # Добавляем аннотации с текстами\n",
    "        for i, txt in enumerate(texts):\n",
    "            # Сокращаем текст для отображения\n",
    "            short_txt = txt[:20] + \"...\" if len(txt) > 20 else txt\n",
    "            plt.annotate(short_txt, (embeddings_2d[i, 0], embeddings_2d[i, 1]))\n",
    "            \n",
    "        plt.title(\"t-SNE визуализация текстовых embeddings\")\n",
    "        plt.tight_layout()\n",
    "        plt.show()\n",
    "        \n",
    "    def semantic_search(self, query, corpus, top_k=5):\n",
    "        \"\"\"\n",
    "        Семантический поиск на основе embeddings\n",
    "        \"\"\"\n",
    "        query_embedding = self.get_embeddings([query])[0]\n",
    "        corpus_embeddings = self.get_embeddings(corpus)\n",
    "        \n",
    "        # Вычисляем косинусное сходство\n",
    "        similarities = cosine_similarity([query_embedding], corpus_embeddings)[0]\n",
    "        \n",
    "        # Сортируем результаты\n",
    "        results = [(i, similarities[i], corpus[i]) for i in range(len(similarities))]\n",
    "        results.sort(key=lambda x: x[1], reverse=True)\n",
    "        \n",
    "        return results[:top_k]\n",
    "\n",
    "# Пример использования\n",
    "extractor = AdvancedEmbeddingExtractor()\n",
    "\n",
    "texts = [\n",
    "    \"Machine learning is a field of study in artificial intelligence.\",\n",
    "    \"Deep learning is a subset of machine learning.\",\n",
    "    \"Neural networks are the foundation of deep learning.\",\n",
    "    \"Computer vision is an application of deep learning.\",\n",
    "    \"Natural language processing deals with text data.\",\n",
    "    \"Python is the most popular programming language for AI.\",\n",
    "    \"The Transformer architecture revolutionized NLP tasks.\",\n",
    "    \"GPT and BERT are examples of large language models.\",\n",
    "    \"Reinforcement learning is learning by trial and error.\",\n",
    "    \"Unsupervised learning doesn't require labeled data.\"\n",
    "]\n",
    "\n",
    "labels = [0, 0, 0, 1, 1, 2, 3, 3, 4, 4]  # Категории текстов\n",
    "\n",
    "# Получаем embeddings с разными стратегиями\n",
    "cls_embeddings = extractor.get_embeddings(texts, pooling_strategy=\"cls\")\n",
    "mean_embeddings = extractor.get_embeddings(texts, pooling_strategy=\"mean\")\n",
    "attention_embeddings = extractor.get_embeddings(texts, pooling_strategy=\"attention\", layers=\"all\")\n",
    "\n",
    "# Пример семантического поиска\n",
    "query = \"How do language models work?\"\n",
    "search_results = extractor.semantic_search(query, texts, top_k=3)\n",
    "\n",
    "print(\"Семантический поиск для запроса:\", query)\n",
    "for idx, score, text in search_results:\n",
    "    print(f\"Score: {score:.4f} - {text}\")\n",
    "\n",
    "# Визуализация embeddings\n",
    "extractor.visualize_embeddings(texts, labels)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "3da7ddca-953b-4a70-82ec-c691a3cd5c5b",
   "metadata": {},
   "outputs": [],
   "source": [
    "import torch\n",
    "import torch.nn as nn\n",
    "import torch.optim as optim\n",
    "import numpy as np\n",
    "from collections import Counter\n",
    "import nltk\n",
    "from nltk.tokenize import word_tokenize\n",
    "nltk.download('punkt')\n",
    "\n",
    "# Подготовка данных\n",
    "text = \"\"\"\n",
    "Artificial intelligence is intelligence demonstrated by machines, as opposed to intelligence of humans.\n",
    "The term \"artificial intelligence\" is often used to describe machines that mimic cognitive functions \n",
    "that humans associate with the human mind, such as learning and problem solving. As machines become \n",
    "increasingly capable, tasks considered to require intelligence are often removed from the definition of AI.\n",
    "\"\"\"\n",
    "\n",
    "# Токенизация и создание словаря\n",
    "words = word_tokenize(text.lower())\n",
    "vocab = Counter(words)\n",
    "vocab = {word: i for i, (word, _) in enumerate(vocab.most_common())}\n",
    "vocab_size = len(vocab)\n",
    "\n",
    "# Параметры модели\n",
    "embedding_dim = 10\n",
    "context_size = 2\n",
    "\n",
    "# Создаем пары контекст-целевое слово\n",
    "data = []\n",
    "for i in range(context_size, len(words) - context_size):\n",
    "    context = words[i-context_size:i] + words[i+1:i+context_size+1]\n",
    "    target = words[i]\n",
    "    data.append((context, target))\n",
    "\n",
    "# Простая модель CBOW (Continuous Bag of Words)\n",
    "class CBOW(nn.Module):\n",
    "    def __init__(self, vocab_size, embedding_dim):\n",
    "        super(CBOW, self).__init__()\n",
    "        self.embeddings = nn.Embedding(vocab_size, embedding_dim)\n",
    "        self.linear = nn.Linear(embedding_dim, vocab_size)\n",
    "        \n",
    "    def forward(self, inputs):\n",
    "        embeds = sum(self.embeddings(inputs))\n",
    "        out = self.linear(embeds)\n",
    "        log_probs = torch.log_softmax(out, dim=0)\n",
    "        return log_probs\n",
    "\n",
    "model = CBOW(vocab_size, embedding_dim)\n",
    "loss_function = nn.NLLLoss()\n",
    "optimizer = optim.SGD(model.parameters(), lr=0.1)\n",
    "\n",
    "# Обучение модели\n",
    "for epoch in range(100):\n",
    "    total_loss = 0\n",
    "    for context_words, target_word in data:\n",
    "        # Преобразуем слова в индексы\n",
    "        context_idxs = torch.tensor([vocab[w] for w in context_words], dtype=torch.long)\n",
    "        target_idx = torch.tensor([vocab[target_word]], dtype=torch.long)\n",
    "        \n",
    "        # Обнуляем градиенты\n",
    "        model.zero_grad()\n",
    "        \n",
    "        # Прямой проход\n",
    "        log_probs = model(context_idxs)\n",
    "        \n",
    "        # Вычисляем потерю\n",
    "        loss = loss_function(log_probs.unsqueeze(0), target_idx)\n",
    "        \n",
    "        # Обратное распространение\n",
    "        loss.backward()\n",
    "        optimizer.step()\n",
    "        \n",
    "        total_loss += loss.item()\n",
    "    \n",
    "    if epoch % 10 == 0:\n",
    "        print(f\"Epoch {epoch}, Loss: {total_loss}\")\n",
    "\n",
    "# Извлекаем embeddings\n",
    "word_embeddings = {}\n",
    "for word, idx in vocab.items():\n",
    "    word_embeddings[word] = model.embeddings.weight[idx].detach().numpy()\n",
    "\n",
    "# Пример использования\n",
    "def get_most_similar(word, top_n=3):\n",
    "    if word not in vocab:\n",
    "        return \"Слово не найдено в словаре\"\n",
    "    \n",
    "    word_vec = word_embeddings[word]\n",
    "    similarities = {}\n",
    "    \n",
    "    for w, vec in word_embeddings.items():\n",
    "        if w != word:\n",
    "            # Косинусное сходство\n",
    "            similarity = np.dot(word_vec, vec) / (np.linalg.norm(word_vec) * np.linalg.norm(vec))\n",
    "            similarities[w] = similarity\n",
    "    \n",
    "    # Сортируем по сходству\n",
    "    sorted_words = sorted(similarities.items(), key=lambda x: x[1], reverse=True)\n",
    "    return sorted_words[:top_n]\n",
    "\n",
    "# Проверяем результаты\n",
    "print(\"\\nНаиболее похожие слова:\")\n",
    "similar_words = get_most_similar(\"intelligence\")\n",
    "for word, similarity in similar_words:\n",
    "    print(f\"{word}: {similarity:.4f}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "a40c7c99-32d7-4147-86a0-97086cad6741",
   "metadata": {},
   "outputs": [],
   "source": [
    "import torch\n",
    "import torch.nn as nn\n",
    "import torch.optim as optim\n",
    "import torch.nn.functional as F\n",
    "import numpy as np\n",
    "from sklearn.model_selection import train_test_split\n",
    "import matplotlib.pyplot as plt\n",
    "from nltk.tokenize import word_tokenize\n",
    "import nltk\n",
    "nltk.download('punkt')\n",
    "\n",
    "# Создаем простой датасет для обучения\n",
    "sentences = [\n",
    "    \"I love machine learning\",\n",
    "    \"Deep learning is a subset of machine learning\",\n",
    "    \"Neural networks are powerful\",\n",
    "    \"Transformer models improved NLP\",\n",
    "    \"BERT is a transformer model\",\n",
    "    \"GPT is also a transformer model\",\n",
    "    \"Python is great for machine learning\",\n",
    "    \"I enjoy programming in Python\",\n",
    "    \"Data science uses machine learning\",\n",
    "    \"Artificial intelligence includes machine learning\",\n",
    "    \"Computers can learn from data\",\n",
    "    \"Algorithms power machine learning\"\n",
    "]\n",
    "\n",
    "labels = [0, 0, 0, 1, 1, 1, 2, 2, 0, 0, 0, 0]  # 0: ML, 1: Transformers, 2: Python\n",
    "\n",
    "# Подготовка данных\n",
    "all_words = []\n",
    "for sentence in sentences:\n",
    "    all_words.extend(word_tokenize(sentence.lower()))\n",
    "\n",
    "# Строим словарь\n",
    "word_counts = Counter(all_words)\n",
    "word_to_idx = {word: i+1 for i, (word, _) in enumerate(word_counts.most_common())}\n",
    "word_to_idx['<PAD>'] = 0  # Добавляем padding token\n",
    "vocab_size = len(word_to_idx)\n",
    "\n",
    "# Максимальная длина предложения\n",
    "max_length = max(len(word_tokenize(sentence)) for sentence in sentences)\n",
    "\n",
    "# Функция для преобразования предложения в последовательность индексов\n",
    "def tokenize_sentence(sentence, max_len):\n",
    "    tokens = word_tokenize(sentence.lower())\n",
    "    indices = [word_to_idx.get(token, 0) for token in tokens]\n",
    "    \n",
    "    # Дополняем или обрезаем до max_length\n",
    "    if len(indices) < max_len:\n",
    "        indices = indices + [0] * (max_len - len(indices))\n",
    "    else:\n",
    "        indices = indices[:max_len]\n",
    "    \n",
    "    return torch.tensor(indices, dtype=torch.long)\n",
    "\n",
    "# Подготавливаем данные для обучения\n",
    "X = [tokenize_sentence(sentence, max_length) for sentence in sentences]\n",
    "X = torch.stack(X)\n",
    "y = torch.tensor(labels, dtype=torch.long)\n",
    "\n",
    "X_train, X_test, y_train, y_test = train_test_split(X, y, test_size=0.3, random_state=42)\n",
    "\n",
    "# Создаем более продвинутую модель с BiLSTM и Self-Attention\n",
    "class SentenceEmbeddingModel(nn.Module):\n",
    "    def __init__(self, vocab_size, embedding_dim, hidden_dim, num_classes):\n",
    "        super(SentenceEmbeddingModel, self).__init__()\n",
    "        self.embedding = nn.Embedding(vocab_size, embedding_dim, padding_idx=0)\n",
    "        self.lstm = nn.LSTM(embedding_dim, hidden_dim, batch_first=True, bidirectional=True)\n",
    "        self.attention = nn.Linear(hidden_dim * 2, 1)\n",
    "        self.fc = nn.Linear(hidden_dim * 2, num_classes)\n",
    "        self.dropout = nn.Dropout(0.3)\n",
    "        \n",
    "    def get_embedding(self, x):\n",
    "        # x: (batch_size, seq_length)\n",
    "        embedded = self.embedding(x)  # (batch_size, seq_length, embedding_dim)\n",
    "        \n",
    "        # Пропускаем через BiLSTM\n",
    "        lstm_out, _ = self.lstm(embedded)  # (batch_size, seq_length, hidden_dim*2)\n",
    "        \n",
    "        # Self-attention механизм\n",
    "        attention_weights = F.softmax(self.attention(lstm_out), dim=1)\n",
    "        context_vector = torch.sum(attention_weights * lstm_out, dim=1)  # (batch_size, hidden_dim*2)\n",
    "        \n",
    "        return context_vector\n",
    "        \n",
    "    def forward(self, x):\n",
    "        # Получаем embedding предложения\n",
    "        sentence_embedding = self.get_embedding(x)\n",
    "        \n",
    "        # Пропускаем через dropout и классификатор\n",
    "        sentence_embedding = self.dropout(sentence_embedding)\n",
    "        logits = self.fc(sentence_embedding)\n",
    "        return logits\n",
    "\n",
    "# Параметры модели\n",
    "embedding_dim = 64\n",
    "hidden_dim = 32\n",
    "num_classes = 3\n",
    "learning_rate = 0.001\n",
    "num_epochs = 100\n",
    "\n",
    "# Создаем и обучаем модель\n",
    "model = SentenceEmbeddingModel(vocab_size, embedding_dim, hidden_dim, num_classes)\n",
    "criterion = nn.CrossEntropyLoss()\n",
    "optimizer = optim.Adam(model.parameters(), lr=learning_rate)\n",
    "\n",
    "# Обучение\n",
    "losses = []\n",
    "for epoch in range(num_epochs):\n",
    "    model.train()\n",
    "    optimizer.zero_grad()\n",
    "    \n",
    "    # Прямой проход\n",
    "    outputs = model(X_train)\n",
    "    loss = criterion(outputs, y_train)\n",
    "    \n",
    "    # Обратное распространение\n",
    "    loss.backward()\n",
    "    optimizer.step()\n",
    "    \n",
    "    losses.append(loss.item())\n",
    "    \n",
    "    if (epoch+1) % 10 == 0:\n",
    "        print(f'Epoch [{epoch+1}/{num_epochs}], Loss: {loss.item():.4f}')\n",
    "\n",
    "# Оценка модели\n",
    "model.eval()\n",
    "with torch.no_grad():\n",
    "    outputs = model(X_test)\n",
    "    _, predicted = torch.max(outputs.data, 1)\n",
    "    accuracy = (predicted == y_test).sum().item() / y_test.size(0)\n",
    "    print(f'Точность на тестовой выборке: {accuracy:.4f}')\n",
    "\n",
    "# Функция для получения embedding предложения\n",
    "def get_sentence_embedding(sentence):\n",
    "    model.eval()\n",
    "    with torch.no_grad():\n",
    "        tokens = tokenize_sentence(sentence, max_length).unsqueeze(0)\n",
    "        embedding = model.get_embedding(tokens)\n",
    "        return embedding.numpy()[0]\n",
    "\n",
    "# Визуализация embeddings с помощью t-SNE\n",
    "from sklearn.manifold import TSNE\n",
    "\n",
    "# Получаем embeddings для всех предложений\n",
    "all_embeddings = []\n",
    "for sentence in sentences:\n",
    "    embedding = get_sentence_embedding(sentence)\n",
    "    all_embeddings.append(embedding)\n",
    "\n",
    "all_embeddings = np.array(all_embeddings)\n",
    "\n",
    "# Применяем t-SNE\n",
    "tsne = TSNE(n_components=2, random_state=42)\n",
    "reduced_embeddings = tsne.fit_transform(all_embeddings)\n",
    "\n",
    "# Визуализируем\n",
    "plt.figure(figsize=(10, 8))\n",
    "for i, (x, y) in enumerate(reduced_embeddings):\n",
    "    plt.scatter(x, y, color=['blue', 'red', 'green'][labels[i]])\n",
    "    plt.text(x, y, f\"{i}: {sentences[i][:20]}...\", fontsize=9)\n",
    "\n",
    "plt.legend(['ML', 'Transformers', 'Python'])\n",
    "plt.title('t-SNE визуализация Sentence Embeddings')\n",
    "plt.tight_layout()\n",
    "plt.show()\n",
    "\n",
    "# Проверяем модель на новых предложениях\n",
    "new_sentences = [\n",
    "    \"I like artificial intelligence\",\n",
    "    \"BERT and GPT are popular models\",\n",
    "    \"I code in Python every day\"\n",
    "]\n",
    "\n",
    "print(\"\\nКлассификация новых предложений:\")\n",
    "for sentence in new_sentences:\n",
    "    # Получаем embedding\n",
    "    tokens = tokenize_sentence(sentence, max_length).unsqueeze(0)\n",
    "    \n",
    "    # Получаем предсказание модели\n",
    "    model.eval()\n",
    "    with torch.no_grad():\n",
    "        output = model(tokens)\n",
    "        _, predicted = torch.max(output.data, 1)\n",
    "        \n",
    "    # Определяем метку класса\n",
    "    class_name = ['ML', 'Transformers', 'Python'][predicted.item()]\n",
    "    \n",
    "    print(f\"Предложение: '{sentence}'\")\n",
    "    print(f\"Класс: {class_name}\\n\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "8323c4f5-4e40-4b3b-b003-6f7103f603ba",
   "metadata": {},
   "outputs": [],
   "source": [
    "import torch\n",
    "import torch.nn as nn\n",
    "import torch.nn.functional as F\n",
    "import torch.optim as optim\n",
    "import math\n",
    "import numpy as np\n",
    "import random\n",
    "import matplotlib.pyplot as plt\n",
    "from collections import Counter, defaultdict\n",
    "from nltk.tokenize import word_tokenize\n",
    "import nltk\n",
    "nltk.download('punkt')\n",
    "\n",
    "# Загрузим небольшой корпус текстов для обучения\n",
    "corpus = [\n",
    "    \"The transformer architecture revolutionized natural language processing.\",\n",
    "    \"Self-attention mechanisms allow models to focus on different parts of the input.\",\n",
    "    \"BERT is a bidirectional encoder representation from transformers.\",\n",
    "    \"GPT models are autoregressive transformers for text generation.\",\n",
    "    \"Embeddings represent words as dense vectors in a continuous space.\",\n",
    "    \"Contextualized embeddings capture word meaning based on surrounding context.\",\n",
    "    \"Traditional word embeddings like Word2Vec use static representations.\",\n",
    "    \"Language models are trained to predict words based on context.\",\n",
    "    \"Machine learning models require numerical representations of text.\",\n",
    "    \"Deep learning has become the dominant approach in NLP.\",\n",
    "    \"Neural networks consist of layers of interconnected neurons.\",\n",
    "    \"Attention is all you need is a famous paper introducing transformers.\",\n",
    "    \"Transfer learning allows reuse of pretrained representations.\",\n",
    "    \"Fine-tuning adapts pretrained models to specific downstream tasks.\",\n",
    "    \"Tokenization is the process of splitting text into smaller units.\",\n",
    "    \"Subword tokenization helps handle out-of-vocabulary words.\",\n",
    "    \"Word embeddings capture semantic relationships between words.\",\n",
    "    \"Transformer models process input in parallel rather than sequentially.\",\n",
    "    \"Positional encodings help transformers understand word order.\",\n",
    "    \"Multi-head attention computes attention multiple times in parallel.\"\n",
    "]\n",
    "\n",
    "# Подготовка данных\n",
    "all_sentences = [s.lower() for s in corpus]\n",
    "all_words = []\n",
    "for sentence in all_sentences:\n",
    "    all_words.extend(word_tokenize(sentence))\n",
    "\n",
    "# Создаем словарь\n",
    "word_counts = Counter(all_words)\n",
    "word_to_idx = {\n",
    "    '<PAD>': 0,  # Padding token\n",
    "    '<UNK>': 1,  # Unknown token\n",
    "    '<CLS>': 2,  # Classification token (начало предложения)\n",
    "    '<SEP>': 3,  # Separator token (конец предложения)\n",
    "    '<MASK>': 4  # Mask token для MLM задачи\n",
    "}\n",
    "\n",
    "# Добавляем слова с частотой больше 1\n",
    "for word, count in word_counts.items():\n",
    "    if count > 1 and word not in word_to_idx:\n",
    "        word_to_idx[word] = len(word_to_idx)\n",
    "\n",
    "idx_to_word = {idx: word for word, idx in word_to_idx.items()}\n",
    "vocab_size = len(word_to_idx)\n",
    "print(f\"Размер словаря: {vocab_size}\")\n",
    "\n",
    "# Функция для токенизации и преобразования предложения в индексы\n",
    "def tokenize_and_encode(sentence, max_len=32):\n",
    "    tokens = ['<CLS>'] + word_tokenize(sentence.lower()) + ['<SEP>']\n",
    "    # Преобразуем в индексы, неизвестные слова заменяем на <UNK>\n",
    "    token_ids = [word_to_idx.get(token, word_to_idx['<UNK>']) for token in tokens]\n",
    "    \n",
    "    # Проверяем длину\n",
    "    if len(token_ids) > max_len:\n",
    "        token_ids = token_ids[:max_len]\n",
    "    else:\n",
    "        # Добавляем padding\n",
    "        token_ids = token_ids + [word_to_idx['<PAD>']] * (max_len - len(token_ids))\n",
    "    \n",
    "    # Создаем маску внимания (1 для реальных токенов, 0 для padding)\n",
    "    attention_mask = [1 if token_id != word_to_idx['<PAD>'] else 0 for token_id in token_ids]\n",
    "    \n",
    "    return token_ids, attention_mask\n",
    "\n",
    "# Класс для позиционного кодирования\n",
    "class PositionalEncoding(nn.Module):\n",
    "    def __init__(self, d_model, max_len=5000):\n",
    "        super(PositionalEncoding, self).__init__()\n",
    "        \n",
    "        # Создаем матрицу позиционного кодирования\n",
    "        pe = torch.zeros(max_len, d_model)\n",
    "        position = torch.arange(0, max_len, dtype=torch.float).unsqueeze(1)\n",
    "        div_term = torch.exp(torch.arange(0, d_model, 2).float() * (-math.log(10000.0) / d_model))\n",
    "        \n",
    "        pe[:, 0::2] = torch.sin(position * div_term)\n",
    "        pe[:, 1::2] = torch.cos(position * div_term)\n",
    "        pe = pe.unsqueeze(0)\n",
    "        \n",
    "        # Регистрируем буфер (не параметр, не будет обучаться)\n",
    "        self.register_buffer('pe', pe)\n",
    "        \n",
    "    def forward(self, x):\n",
    "        # x: [batch_size, seq_len, embedding_dim]\n",
    "        return x + self.pe[:, :x.size(1)]\n",
    "\n",
    "# Класс для механизма Self-Attention\n",
    "class MultiHeadAttention(nn.Module):\n",
    "    def __init__(self, d_model, num_heads):\n",
    "        super(MultiHeadAttention, self).__init__()\n",
    "        self.d_model = d_model\n",
    "        self.num_heads = num_heads\n",
    "        self.head_dim = d_model // num_heads\n",
    "        \n",
    "        assert self.head_dim * num_heads == d_model, \"d_model must be divisible by num_heads\"\n",
    "        \n",
    "        self.query = nn.Linear(d_model, d_model)\n",
    "        self.key = nn.Linear(d_model, d_model)\n",
    "        self.value = nn.Linear(d_model, d_model)\n",
    "        \n",
    "        self.fc_out = nn.Linear(d_model, d_model)\n",
    "        \n",
    "    def forward(self, query, key, value, mask=None):\n",
    "        batch_size = query.shape[0]\n",
    "        \n",
    "        # Linear projections\n",
    "        Q = self.query(query)  # (batch_size, seq_len, d_model)\n",
    "        K = self.key(key)      # (batch_size, seq_len, d_model)\n",
    "        V = self.value(value)  # (batch_size, seq_len, d_model)\n",
    "        \n",
    "        # Reshape для multi-head attention\n",
    "        Q = Q.view(batch_size, -1, self.num_heads, self.head_dim).permute(0, 2, 1, 3)\n",
    "        K = K.view(batch_size, -1, self.num_heads, self.head_dim).permute(0, 2, 1, 3)\n",
    "        V = V.view(batch_size, -1, self.num_heads, self.head_dim).permute(0, 2, 1, 3)\n",
    "        \n",
    "        # Вычисляем скалярное произведение и масштабируем\n",
    "        energy = torch.matmul(Q, K.permute(0, 1, 3, 2)) / math.sqrt(self.head_dim)\n",
    "        \n",
    "        # Маскируем padding, если mask задана\n",
    "        if mask is not None:\n",
    "            # Преобразуем маску для broadcast по батчу и числу голов\n",
    "            mask = mask.unsqueeze(1).unsqueeze(2)  # (batch_size, 1, 1, seq_len)\n",
    "            energy = energy.masked_fill(mask == 0, float(\"-1e20\"))\n",
    "        \n",
    "        # Применяем softmax и вычисляем взвешенную сумму\n",
    "        attention = F.softmax(energy, dim=-1)  # (batch_size, num_heads, seq_len, seq_len)\n",
    "        out = torch.matmul(attention, V)  # (batch_size, num_heads, seq_len, head_dim)\n",
    "        \n",
    "        # Конкатенируем головы и пропускаем через финальный линейный слой\n",
    "        out = out.permute(0, 2, 1, 3).contiguous().view(batch_size, -1, self.d_model)\n",
    "        out = self.fc_out(out)  # (batch_size, seq_len, d_model)\n",
    "        \n",
    "        return out, attention\n",
    "\n",
    "# Feed-Forward Network\n",
    "class FeedForward(nn.Module):\n",
    "    def __init__(self, d_model, d_ff, dropout=0.1):\n",
    "        super(FeedForward, self).__init__()\n",
    "        self.fc1 = nn.Linear(d_model, d_ff)\n",
    "        self.fc2 = nn.Linear(d_ff, d_model)\n",
    "        self.dropout = nn.Dropout(dropout)\n",
    "        \n",
    "    def forward(self, x):\n",
    "        # x: [batch_size, seq_len, d_model]\n",
    "        x = F.gelu(self.fc1(x))\n",
    "        x = self.dropout(x)\n",
    "        x = self.fc2(x)\n",
    "        return x\n",
    "\n",
    "# Encoder Layer\n",
    "class EncoderLayer(nn.Module):\n",
    "    def __init__(self, d_model, num_heads, d_ff, dropout=0.1):\n",
    "        super(EncoderLayer, self).__init__()\n",
    "        self.self_attention = MultiHeadAttention(d_model, num_heads)\n",
    "        self.norm1 = nn.LayerNorm(d_model)\n",
    "        self.feed_forward = FeedForward(d_model, d_ff, dropout)\n",
    "        self.norm2 = nn.LayerNorm(d_model)\n",
    "        self.dropout = nn.Dropout(dropout)\n",
    "        \n",
    "    def forward(self, x, mask=None):\n",
    "        # Self Attention\n",
    "        attention_out, _ = self.self_attention(x, x, x, mask)\n",
    "        x = self.norm1(x + self.dropout(attention_out))\n",
    "        \n",
    "        # Feed Forward\n",
    "        ff_out = self.feed_forward(x)\n",
    "        x = self.norm2(x + self.dropout(ff_out))\n",
    "        \n",
    "        return x\n",
    "\n",
    "# Полная модель Transformer Encoder\n",
    "class TransformerEncoder(nn.Module):\n",
    "    def __init__(self, vocab_size, d_model, num_heads, num_layers, d_ff, max_seq_len=128, dropout=0.1):\n",
    "        super(TransformerEncoder, self).__init__()\n",
    "        self.d_model = d_model\n",
    "        \n",
    "        # Embedding слой\n",
    "        self.embedding = nn.Embedding(vocab_size, d_model)\n",
    "        self.positional_encoding = PositionalEncoding(d_model, max_seq_len)\n",
    "        self.dropout = nn.Dropout(dropout)\n",
    "        \n",
    "        # Encoder layers\n",
    "        self.layers = nn.ModuleList([\n",
    "            EncoderLayer(d_model, num_heads, d_ff, dropout)\n",
    "            for _ in range(num_layers)\n",
    "        ])\n",
    "        \n",
    "        # Layer Norm\n",
    "        self.norm = nn.LayerNorm(d_model)\n",
    "        \n",
    "    def forward(self, x, mask=None):\n",
    "        # x: [batch_size, seq_len]\n",
    "        \n",
    "        # Получаем embeddings и добавляем позиционное кодирование\n",
    "        x = self.embedding(x) * math.sqrt(self.d_model)\n",
    "        x = self.positional_encoding(x)\n",
    "        x = self.dropout(x)\n",
    "        \n",
    "        # Проходим через энкодер слои\n",
    "        for layer in self.layers:\n",
    "            x = layer(x, mask)\n",
    "            \n",
    "        # Применяем финальную нормализацию\n",
    "        x = self.norm(x)\n",
    "        \n",
    "        return x\n",
    "\n",
    "# Создаем модель для Masked Language Modeling (MLM)\n",
    "class MLMModel(nn.Module):\n",
    "    def __init__(self, vocab_size, d_model=128, num_heads=2, num_layers=2, d_ff=256, dropout=0.1):\n",
    "        super(MLMModel, self).__init__()\n",
    "        \n",
    "        self.transformer = TransformerEncoder(\n",
    "            vocab_size=vocab_size,\n",
    "            d_model=d_model,\n",
    "            num_heads=num_heads,\n",
    "            num_layers=num_layers,\n",
    "            d_ff=d_ff,\n",
    "            dropout=dropout\n",
    "        )\n",
    "        \n",
    "        # Проекция для MLM задачи\n",
    "        self.mlm_head = nn.Linear(d_model, vocab_size)\n",
    "        \n",
    "    def forward(self, x, mask=None):\n",
    "        # x: [batch_size, seq_len]\n",
    "        \n",
    "        # Получаем contextualized embeddings\n",
    "        transformer_output = self.transformer(x, mask)  # [batch_size, seq_len, d_model]\n",
    "        \n",
    "        # Предсказываем вероятности токенов\n",
    "        mlm_output = self.mlm_head(transformer_output)  # [batch_size, seq_len, vocab_size]\n",
    "        \n",
    "        return mlm_output\n",
    "    \n",
    "    def get_embeddings(self, x, mask=None, layer_idx=-1):\n",
    "        \"\"\"\n",
    "        Извлекает embeddings из указанного слоя трансформера\n",
    "        \"\"\"\n",
    "        # Получаем embeddings и добавляем позиционное кодирование\n",
    "        embeddings = self.transformer.embedding(x) * math.sqrt(self.transformer.d_model)\n",
    "        x = self.transformer.positional_encoding(embeddings)\n",
    "        x = self.transformer.dropout(x)\n",
    "        \n",
    "        # Проходим через слои до указанного\n",
    "        all_layer_outputs = []\n",
    "        for i, layer in enumerate(self.transformer.layers):\n",
    "            x = layer(x, mask)\n",
    "            all_layer_outputs.append(x)\n",
    "            \n",
    "            if i == layer_idx and layer_idx != -1:\n",
    "                break\n",
    "        \n",
    "        # Если нужен последний слой или прошли все слои\n",
    "        if layer_idx == -1:\n",
    "            x = self.transformer.norm(x)\n",
    "            \n",
    "        return {\n",
    "            'token_embeddings': x,  # contextualized embeddings\n",
    "            'input_embeddings': embeddings,  # чистые embeddings без позиционного кодирования\n",
    "            'all_layer_outputs': all_layer_outputs  # выходы всех промежуточных слоев\n",
    "        }\n",
    "\n",
    "# Функция для создания батчей данных для MLM задачи\n",
    "def create_mlm_batch(sentences, max_len=32, mask_prob=0.15):\n",
    "    batch_inputs = []\n",
    "    batch_labels = []\n",
    "    batch_masks = []\n",
    "    \n",
    "    for sentence in sentences:\n",
    "        token_ids, attention_mask = tokenize_and_encode(sentence, max_len)\n",
    "        \n",
    "        # Копируем исходные токены для labels\n",
    "        labels = token_ids.copy()\n",
    "        \n",
    "        # Случайно маскируем токены\n",
    "        for i in range(len(token_ids)):\n",
    "            # Не маскируем специальные токены\n",
    "            if token_ids[i] in [word_to_idx['<PAD>'], word_to_idx['<CLS>'], word_to_idx['<SEP>']]:\n",
    "                continue\n",
    "                \n",
    "            prob = random.random()\n",
    "            if prob < mask_prob:\n",
    "                # 80% случаев заменяем на [MASK]\n",
    "                if prob < mask_prob * 0.8:\n",
    "                    token_ids[i] = word_to_idx['<MASK>']\n",
    "                # 10% случаев заменяем на случайный токен\n",
    "                elif prob < mask_prob * 0.9:\n",
    "                    token_ids[i] = random.randint(5, vocab_size - 1)  # Исключаем специальные токены\n",
    "                # 10% случаев оставляем без изменений\n",
    "        \n",
    "        batch_inputs.append(token_ids)\n",
    "        batch_labels.append(labels)\n",
    "        batch_masks.append(attention_mask)\n",
    "    \n",
    "    # Преобразуем в тензоры\n",
    "    batch_inputs = torch.tensor(batch_inputs)\n",
    "    batch_labels = torch.tensor(batch_labels)\n",
    "    batch_masks = torch.tensor(batch_masks)\n",
    "    \n",
    "    return batch_inputs, batch_labels, batch_masks\n",
    "\n",
    "# Функция обучения модели\n",
    "def train_mlm_model(model, corpus, num_epochs=50, batch_size=4, learning_rate=1e-4):\n",
    "    # Подготавливаем оптимизатор\n",
    "    optimizer = optim.Adam(model.parameters(), lr=learning_rate)\n",
    "    criterion = nn.CrossEntropyLoss(ignore_index=word_to_idx['<PAD>'])  # Игнорируем padding при расчете потерь\n",
    "    \n",
    "    losses = []\n",
    "    \n",
    "    # Разделяем корпус на батчи\n",
    "    num_batches = (len(corpus) + batch_size - 1) // batch_size\n",
    "    \n",
    "    for epoch in range(num_epochs):\n",
    "        total_loss = 0\n",
    "        random.shuffle(corpus)  # Перемешиваем данные\n",
    "        \n",
    "        for i in range(0, len(corpus), batch_size):\n",
    "            batch_sentences = corpus[i:i+batch_size]\n",
    "            inputs, labels, masks = create_mlm_batch(batch_sentences)\n",
    "            \n",
    "            # Обнуляем градиенты\n",
    "            optimizer.zero_grad()\n",
    "            \n",
    "            # Прямой проход\n",
    "            outputs = model(inputs, masks)\n",
    "            \n",
    "            # Изменяем форму выходов и меток для расчета потерь\n",
    "            outputs = outputs.view(-1, vocab_size)\n",
    "            labels = labels.view(-1)\n",
    "            \n",
    "            # Расчет потерь\n",
    "            loss = criterion(outputs, labels)\n",
    "            \n",
    "            # Обратное распространение\n",
    "            loss.backward()\n",
    "            \n",
    "            # Оптимизация\n",
    "            optimizer.step()\n",
    "            \n",
    "            total_loss += loss.item()\n",
    "        \n",
    "        avg_loss = total_loss / num_batches\n",
    "        losses.append(avg_loss)\n",
    "        \n",
    "        if (epoch + 1) % 5 == 0:\n",
    "            print(f\"Epoch {epoch+1}/{num_epochs}, Loss: {avg_loss:.4f}\")\n",
    "    \n",
    "    # Визуализация потерь\n",
    "    plt.figure(figsize=(10, 6))\n",
    "    plt.plot(losses)\n",
    "    plt.title('MLM Training Loss')\n",
    "    plt.xlabel('Epoch')\n",
    "    plt.ylabel('Loss')\n",
    "    plt.grid(True)\n",
    "    plt.show()\n",
    "    \n",
    "    return model\n",
    "\n",
    "# Создаем и обучаем модель\n",
    "d_model = 128  # Размерность embeddings\n",
    "num_heads = 2  # Количество голов в механизме внимания\n",
    "num_layers = 2  # Количество слоев Encoder\n",
    "d_ff = 256  # Размерность Feed-Forward слоя\n",
    "\n",
    "model = MLMModel(\n",
    "    vocab_size=vocab_size,\n",
    "    d_model=d_model,\n",
    "    num_heads=num_heads,\n",
    "    num_layers=num_layers,\n",
    "    d_ff=d_ff,\n",
    "    dropout=0.1\n",
    ")\n",
    "\n",
    "# Обучаем модель\n",
    "model = train_mlm_model(model, corpus, num_epochs=50, batch_size=4, learning_rate=1e-4)\n",
    "\n",
    "# Получение embeddings из обученной модели\n",
    "def get_contextual_embeddings(model, sentence, mode='mean', layer_idx=-1):\n",
    "    \"\"\"\n",
    "    Извлекает embeddings предложения из модели\n",
    "    \n",
    "    Args:\n",
    "        model: обученная модель\n",
    "        sentence: текст предложения\n",
    "        mode: метод объединения токенов ('cls', 'mean', 'max')\n",
    "        layer_idx: индекс слоя (по умолчанию -1, последний слой)\n",
    "    \n",
    "    Returns:\n",
    "        numpy array с embedding предложения\n",
    "    \"\"\"\n",
    "    # Токенизация\n",
    "    token_ids, attention_mask = tokenize_and_encode(sentence)\n",
    "    inputs = torch.tensor([token_ids])\n",
    "    masks = torch.tensor([attention_mask])\n",
    "    \n",
    "    # Получаем embeddings\n",
    "    model.eval()\n",
    "    with torch.no_grad():\n",
    "        outputs = model.get_embeddings(inputs, masks, layer_idx=layer_idx)\n",
    "    \n",
    "    # Извлекаем токенные embeddings\n",
    "    token_embeddings = outputs['token_embeddings'][0]  # Берем первый элемент батча\n",
    "    \n",
    "    # Применяем выбранный метод объединения\n",
    "    if mode == 'cls':\n",
    "        # Используем embedding токена [CLS]\n",
    "        return token_embeddings[0].numpy()\n",
    "    \n",
    "    elif mode == 'mean':\n",
    "        # Среднее по всем токенам, исключая padding\n",
    "        mask = torch.tensor(attention_mask).unsqueeze(-1)\n",
    "        return torch.sum(token_embeddings * mask, dim=0) / torch.sum(mask).numpy()\n",
    "    \n",
    "    elif mode == 'max':\n",
    "        # Max pooling по всем токенам\n",
    "        # Перед max pooling замаскируем padding нулями\n",
    "        mask = torch.tensor(attention_mask).unsqueeze(-1)\n",
    "        masked_embeddings = token_embeddings * mask\n",
    "        return torch.max(masked_embeddings, dim=0)[0].numpy()\n",
    "    \n",
    "    else:\n",
    "        raise ValueError(\"Unsupported pooling mode\")\n",
    "\n",
    "# Визуализируем embeddings предложений с помощью t-SNE\n",
    "def visualize_embeddings(model, sentences, labels=None, mode='mean', layer_idx=-1):\n",
    "    \"\"\"\n",
    "    Визуализирует embeddings предложений с помощью t-SNE\n",
    "    \"\"\"\n",
    "    from sklearn.manifold import TSNE\n",
    "    \n",
    "    # Получаем embeddings для всех предложений\n",
    "    embeddings = []\n",
    "    for sentence in sentences:\n",
    "        embedding = get_contextual_embeddings(model, sentence, mode=mode, layer_idx=layer_idx)\n",
    "        embeddings.append(embedding)\n",
    "    \n",
    "    # Применяем t-SNE для снижения размерности\n",
    "    tsne = TSNE(n_components=2, random_state=42)\n",
    "    reduced_embeddings = tsne.fit_transform(embeddings)\n",
    "    \n",
    "    # Визуализируем\n",
    "    plt.figure(figsize=(12, 10))\n",
    "    \n",
    "    if labels is not None:\n",
    "        unique_labels = set(labels)\n",
    "        colors = plt.cm.rainbow(np.linspace(0, 1, len(unique_labels)))\n",
    "        color_map = dict(zip(unique_labels, colors))\n",
    "        \n",
    "        for i, (x, y) in enumerate(reduced_embeddings):\n",
    "            plt.scatter(x, y, color=color_map[labels[i]])\n",
    "            plt.text(x, y, f\"{i}\", fontsize=9)\n",
    "        \n",
    "        # Создаем легенду\n",
    "        for label, color in zip(unique_labels, colors):\n",
    "            plt.scatter([], [], color=color, label=label)\n",
    "        plt.legend()\n",
    "    else:\n",
    "        for i, (x, y) in enumerate(reduced_embeddings):\n",
    "            plt.scatter(x, y)\n",
    "            plt.text(x, y, f\"{i}\", fontsize=9)\n",
    "    \n",
    "    plt.title(f't-SNE visualization of sentence embeddings (mode: {mode}, layer: {layer_idx})')\n",
    "    plt.tight_layout()\n",
    "    plt.show()\n",
    "    \n",
    "    return reduced_embeddings\n",
    "\n",
    "# Создадим пример с тестовыми предложениями для визуализации\n",
    "test_sentences = [\n",
    "    \"Embeddings represent words as vectors.\",\n",
    "    \"Word vectors capture semantic relationships.\",\n",
    "    \"Neural networks process word embeddings.\",\n",
    "    \"BERT produces contextualized embeddings.\",\n",
    "    \"GPT generates text based on context.\",\n",
    "    \"Transformers use self-attention mechanisms.\",\n",
    "    \"Language models predict the next word.\",\n",
    "    \"Transfer learning reuses pretrained models.\",\n",
    "    \"Machine learning requires data preprocessing.\",\n",
    "    \"Deep learning has transformed NLP research.\"\n",
    "]\n",
    "\n",
    "# Присвоим категории предложениям\n",
    "test_labels = [\n",
    "    \"Embeddings\", \"Embeddings\", \"Neural Networks\", \n",
    "    \"Models\", \"Models\", \"Architecture\",\n",
    "    \"Language Models\", \"Training\", \"ML Process\", \"General\"\n",
    "]\n",
    "\n",
    "# Визуализируем embeddings\n",
    "visualize_embeddings(model, test_sentences, test_labels, mode='mean')\n",
    "\n",
    "# Посмотрим, как отличаются представления из разных слоев\n",
    "visualize_embeddings(model, test_sentences, test_labels, mode='mean', layer_idx=0)\n",
    "\n",
    "# Функция для семантического поиска\n",
    "def semantic_search(model, query, corpus, top_k=3, mode='mean'):\n",
    "    \"\"\"\n",
    "    Выполняет семантический поиск: находит предложения из корпуса,\n",
    "    наиболее похожие на запрос по косинусному сходству embeddings\n",
    "    \"\"\"\n",
    "    # Получаем embedding запроса\n",
    "    query_embedding = get_contextual_embeddings(model, query, mode=mode)\n",
    "    \n",
    "    # Получаем embeddings всех предложений из корпуса\n",
    "    corpus_embeddings = []\n",
    "    for sentence in corpus:\n",
    "        embedding = get_contextual_embeddings(model, sentence, mode=mode)\n",
    "        corpus_embeddings.append(embedding)\n",
    "    \n",
    "    # Вычисляем косинусное сходство\n",
    "    similarities = []\n",
    "    for i, embedding in enumerate(corpus_embeddings):\n",
    "        # Косинусное сходство\n",
    "        similarity = np.dot(query_embedding, embedding) / (\n",
    "            np.linalg.norm(query_embedding) * np.linalg.norm(embedding)\n",
    "        )\n",
    "        similarities.append((i, similarity, corpus[i]))\n",
    "    \n",
    "    # Сортируем по убыванию сходства\n",
    "    similarities.sort(key=lambda x: x[1], reverse=True)\n",
    "    \n",
    "    return similarities[:top_k]\n",
    "\n",
    "# Пример семантического поиска\n",
    "query = \"How do language models use embeddings?\"\n",
    "search_results = semantic_search(model, query, corpus, top_k=3)\n",
    "\n",
    "print(\"\\nРезультаты семантического поиска для запроса:\", query)\n",
    "for idx, score, text in search_results:\n",
    "    print(f\"Score: {score:.4f} - {text}\")\n",
    "\n",
    "# Функция для аналогии слов (word analogy)\n",
    "def word_analogy(model, word_a, word_b, word_c, top_n=5):\n",
    "    \"\"\"\n",
    "    Реализует операцию аналогии слов: word_a относится к word_b, как word_c относится к ?\n",
    "    Например: king - man + woman = queen\n",
    "    \"\"\"\n",
    "    # Проверяем, есть ли слова в словаре\n",
    "    for word in [word_a, word_b, word_c]:\n",
    "        if word not in word_to_idx:\n",
    "            print(f\"Слово '{word}' не найдено в словаре\")\n",
    "            return []\n",
    "    \n",
    "    # Получаем embeddings слов\n",
    "    embedding_a = model.transformer.embedding(torch.tensor([word_to_idx[word_a]])).detach().squeeze().numpy()\n",
    "    embedding_b = model.transformer.embedding(torch.tensor([word_to_idx[word_b]])).detach().squeeze().numpy()\n",
    "    embedding_c = model.transformer.embedding(torch.tensor([word_to_idx[word_c]])).detach().squeeze().numpy()\n",
    "    \n",
    "    # Вычисляем вектор аналогии: embedding_b - embedding_a + embedding_c\n",
    "    target_embedding = embedding_b - embedding_a + embedding_c\n",
    "    \n",
    "    # Находим ближайшие слова к целевому вектору\n",
    "    similarities = []\n",
    "    for word, idx in word_to_idx.items():\n",
    "        # Пропускаем специальные токены и исходные слова\n",
    "        if idx < 5 or word in [word_a, word_b, word_c]:\n",
    "            continue\n",
    "        \n",
    "        word_embedding = model.transformer.embedding(torch.tensor([idx])).detach().squeeze().numpy()\n",
    "        \n",
    "        # Косинусное сходство\n",
    "        similarity = np.dot(target_embedding, word_embedding) / (\n",
    "            np.linalg.norm(target_embedding) * np.linalg.norm(word_embedding)\n",
    "        )\n",
    "        similarities.append((word, similarity))\n",
    "    \n",
    "    # Сортируем по убыванию сходства\n",
    "    similarities.sort(key=lambda x: x[1], reverse=True)\n",
    "    \n",
    "    return similarities[:top_n]\n",
    "\n",
    "# Класс для визуализации внимания\n",
    "def visualize_attention(model, sentence):\n",
    "    \"\"\"\n",
    "    Визуализирует матрицу внимания для предложения\n",
    "    \"\"\"\n",
    "    # Токенизация\n",
    "    token_ids, attention_mask = tokenize_and_encode(sentence)\n",
    "    tokens = [idx_to_word[idx] for idx in token_ids if idx in idx_to_word]\n",
    "    \n",
    "    inputs = torch.tensor([token_ids])\n",
    "    mask = torch.tensor([attention_mask])\n",
    "    \n",
    "    # Получаем выходы модели с матрицами внимания\n",
    "    model.eval()\n",
    "    with torch.no_grad():\n",
    "        # Получаем embeddings и добавляем позиционное кодирование\n",
    "        embeddings = model.transformer.embedding(inputs) * math.sqrt(model.transformer.d_model)\n",
    "        x = model.transformer.positional_encoding(embeddings)\n",
    "        x = model.transformer.dropout(x)\n",
    "        \n",
    "        # Проходим через слои и сохраняем матрицы внимания\n",
    "        attention_matrices = []\n",
    "        for layer in model.transformer.layers:\n",
    "            # Получаем выход self-attention\n",
    "            attention_out, attention_weights = layer.self_attention(x, x, x, mask)\n",
    "            x = layer.norm1(x + layer.dropout(attention_out))\n",
    "            \n",
    "            # Feed Forward\n",
    "            ff_out = layer.feed_forward(x)\n",
    "            x = layer.norm2(x + layer.dropout(ff_out))\n",
    "            \n",
    "            # Сохраняем веса внимания\n",
    "            attention_matrices.append(attention_weights)\n",
    "    \n",
    "    # Визуализируем матрицы внимания для каждого слоя и головы\n",
    "    num_layers = len(model.transformer.layers)\n",
    "    num_heads = model.transformer.layers[0].self_attention.num_heads\n",
    "    \n",
    "    fig, axes = plt.subplots(num_layers, num_heads, figsize=(15, 4 * num_layers))\n",
    "    if num_layers == 1:\n",
    "        axes = np.array([axes])\n",
    "    \n",
    "    # Максимальная длина предложения для визуализации\n",
    "    max_token_len = min(len(tokens), 20)  # Ограничиваем для лучшей визуализации\n",
    "    tokens = tokens[:max_token_len]\n",
    "    \n",
    "    for layer_idx in range(num_layers):\n",
    "        for head_idx in range(num_heads):\n",
    "            ax = axes[layer_idx, head_idx]\n",
    "            \n",
    "            # Извлекаем матрицу внимания для конкретного слоя и головы\n",
    "            attn_matrix = attention_matrices[layer_idx][0, head_idx, :max_token_len, :max_token_len].cpu().numpy()\n",
    "            \n",
    "            # Создаем тепловую карту\n",
    "            im = ax.imshow(attn_matrix, cmap='viridis')\n",
    "            \n",
    "            # Добавляем метки токенов\n",
    "            ax.set_xticks(range(max_token_len))\n",
    "            ax.set_yticks(range(max_token_len))\n",
    "            ax.set_xticklabels(tokens, rotation=90, fontsize=8)\n",
    "            ax.set_yticklabels(tokens, fontsize=8)\n",
    "            \n",
    "            ax.set_title(f\"Layer {layer_idx+1}, Head {head_idx+1"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "423c003b-078c-4b32-bd07-7ae36ccaa6be",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "ac644d49-c7cb-4e02-a620-4c4f3dfc5757",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "3829071d-6825-49dd-9cb6-8c57f7441043",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "94a9abe7-8473-4ebe-be91-8d1574f354a8",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "810dfa1f-0290-4d48-85f6-ecfcfda37ee0",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "072ad6b8-b222-4c55-a653-eb9d2a383660",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "8ff2f0a4-9ae0-45ca-aa93-12ffe5aa961e",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "80d4abbb-6a28-4ca5-a24f-8d87e3b1b97e",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "7bd282a8-6052-4dc1-b4bf-8b90f07e8cce",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "95db085b-0cc8-486e-ac10-00d9b9855dc1",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.13.2"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
