{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "\"\"\"\n",
    "In this cell, complete \n",
    "1) relu function\n",
    "2) max_pooling_with_mask function\n",
    "3) conv2d_multi_channel_forward function\n",
    "4) forward method in the SimpleCNN class\n",
    "\n",
    "There are comments in the exact locations where modifications should be made\n",
    "\n",
    "DO NOT TOUCH ANYTHING ELSE IN THIS CELL\n",
    "\"\"\"\n",
    "import os\n",
    "import numpy as np\n",
    "from PIL import Image\n",
    "\n",
    "def relu(x):\n",
    "    # implement the relu function.\n",
    "    return np.maximum(x, 0)\n",
    "    \n",
    "def max_pooling_with_mask(x, pool_size=2, stride=2):\n",
    "    \"\"\"\n",
    "    Forward pass for max pooling on a 3D input (H, W, C).\n",
    "    Returns both the pooled output and a mask indicating which locations were chosen.\n",
    "    \"\"\"\n",
    "    H, W, C = x.shape\n",
    "    outH = (H - pool_size) // stride + 1\n",
    "    outW = (W - pool_size) // stride + 1\n",
    "    pooled = np.zeros((outH, outW, C))\n",
    "    mask = np.zeros_like(x)\n",
    "    for c in range(C):\n",
    "        for i in range(outH):\n",
    "            for j in range(outW):\n",
    "                h_start = i * stride\n",
    "                h_end = h_start + pool_size\n",
    "                w_start = j * stride\n",
    "                w_end = w_start + pool_size\n",
    "                window = x[h_start:h_end, w_start:w_end, c]\n",
    "                # finish the pooling layer logic by completing the following two lines of code:\n",
    "                max_val = np.max(window)\n",
    "                pooled[i, j, c] = max_val\n",
    "                found = False\n",
    "                for m in range(pool_size):\n",
    "                    for n in range(pool_size):\n",
    "                        if window[m, n] == max_val and not found:\n",
    "                            mask[h_start + m, w_start + n, c] = 1\n",
    "                            found = True\n",
    "    return pooled, mask\n",
    "\n",
    "def conv2d_multi_channel_forward(input_volume, kernels):\n",
    "    \"\"\"\n",
    "    Forward pass for multi-channel convolution.\n",
    "    input_volume: shape (H, W, C_in)\n",
    "    kernels: shape (kH, kW, C_in, num_filters)\n",
    "    Returns output of shape (H - kH + 1, W - kW + 1, num_filters)\n",
    "    \"\"\"\n",
    "    H, W, C = input_volume.shape\n",
    "    kH, kW, _, num_filters = kernels.shape\n",
    "    outH = H - kH + 1\n",
    "    outW = W - kW + 1\n",
    "    output = np.zeros((outH, outW, num_filters))\n",
    "    for f in range(num_filters):\n",
    "        for i in range(outH):\n",
    "            for j in range(outW):\n",
    "                conv_sum = 0\n",
    "                for c in range(C):\n",
    "                    for m in range(kH):\n",
    "                        for n in range(kW):\n",
    "                            # finish the multi-channel convolution logic by completing the following line of code:\n",
    "                            conv_sum += input_volume[i + m, j + n, c] * kernels[m, n, c, f]\n",
    "                output[i, j, f] = conv_sum\n",
    "    return output\n",
    "\n",
    "def relu_backward(x, d_out):\n",
    "    d_x = d_out.copy()\n",
    "    d_x[x <= 0] = 0\n",
    "    return d_x\n",
    "\n",
    "def max_pooling_backward(mask, d_out, pool_size=2, stride=2):\n",
    "    \"\"\"\n",
    "    Backward pass for max pooling.\n",
    "    The gradient is distributed only to the location which had the maximum value.\n",
    "    \"\"\"\n",
    "    H, W, C = mask.shape\n",
    "    outH, outW, _ = d_out.shape\n",
    "    d_x = np.zeros_like(mask)\n",
    "    for c in range(C):\n",
    "        for i in range(outH):\n",
    "            for j in range(outW):\n",
    "                h_start = i * stride\n",
    "                h_end = h_start + pool_size\n",
    "                w_start = j * stride\n",
    "                w_end = w_start + pool_size\n",
    "                d_x[h_start:h_end, w_start:w_end, c] += mask[h_start:h_end, w_start:w_end, c] * d_out[i, j, c]\n",
    "    return d_x\n",
    "\n",
    "def conv2d_multi_channel_backward(input_volume, kernels, d_out):\n",
    "    \"\"\"\n",
    "    Backward pass for multi-channel convolution.\n",
    "    Returns gradients with respect to the input_volume and the kernels.\n",
    "    \"\"\"\n",
    "    H, W, C = input_volume.shape\n",
    "    kH, kW, _, num_filters = kernels.shape\n",
    "    outH, outW, _ = d_out.shape\n",
    "    d_input = np.zeros_like(input_volume)\n",
    "    d_kernels = np.zeros_like(kernels)\n",
    "    for f in range(num_filters):\n",
    "        for i in range(outH):\n",
    "            for j in range(outW):\n",
    "                for c in range(C):\n",
    "                    for m in range(kH):\n",
    "                        for n in range(kW):\n",
    "                            d_kernels[m, n, c, f] += input_volume[i + m, j + n, c] * d_out[i, j, f]\n",
    "                            d_input[i + m, j + n, c] += kernels[m, n, c, f] * d_out[i, j, f]\n",
    "    return d_input, d_kernels\n",
    "\n",
    "# ---------------------- SimpleCNN Class ----------------------\n",
    "class SimpleCNN:\n",
    "    def __init__(self):\n",
    "        # For two-class classification (ferrari vs. jeep), we output 2 classes.\n",
    "        # Layer 1: Convolution with 4 filters (3x3) for a single-channel input.\n",
    "        self.conv1_kernels = np.random.randn(3, 3, 1, 4) * 0.1\n",
    "        # Layer 2: Convolution with 8 filters (3x3) spanning 4 input channels.\n",
    "        self.conv2_kernels = np.random.randn(3, 3, 4, 8) * 0.1\n",
    "        # After two conv layers and pooling:\n",
    "        # (50,50) -> Conv1: (48,48,4) -> Pool1: (24,24,4)\n",
    "        # -> Conv2: (22,22,8) -> Pool2: (11,11,8) => 11*11*8 features.\n",
    "        self.fc_input_dim = 11 * 11 * 8\n",
    "        self.fc_weights = np.random.randn(self.fc_input_dim, 2) * 0.1  # 2 classes: ferrari and jeep.\n",
    "        self.fc_bias = np.random.randn(2) * 0.1\n",
    "\n",
    "        self.lr = 0.005  # Learning rate.\n",
    "        self.cache = {}  # Cache intermediate forward-pass results for backprop.\n",
    "\n",
    "    def forward(self, x):\n",
    "        \"\"\"\n",
    "        Forward pass through the network.\n",
    "        x: 2D numpy array (50x50)\n",
    "        Returns: logits (2-dimensional vector) for the 2 classes.\n",
    "        \"\"\"\n",
    "        # Reshape input to (50,50,1)\n",
    "        x = x.reshape(50, 50, 1)\n",
    "        self.cache['x'] = x\n",
    "\n",
    "        # finish the forward function logic by completing the expressions for conv1, relu1, (pool1, mask1), conv2, relu2, (pool2, mask2)\n",
    "        # Layer 1: Convolution\n",
    "        conv1 = conv2d_multi_channel_forward(x, self.conv1_kernels)  # Shape: (48,48,4)\n",
    "        self.cache['conv1'] = conv1\n",
    "\n",
    "        # ReLU activation\n",
    "        relu1 = relu(conv1)\n",
    "        self.cache['relu1'] = relu1\n",
    "\n",
    "        # Max Pooling 1\n",
    "        pool1, mask1 = max_pooling_with_mask(relu1, pool_size=2, stride=2)  # Shape: (24,24,4)\n",
    "        self.cache['pool1'] = pool1\n",
    "        self.cache['mask1'] = mask1\n",
    "\n",
    "        # Layer 2: Convolution\n",
    "        conv2 = conv2d_multi_channel_forward(pool1, self.conv2_kernels)  # Shape: (22,22,8)\n",
    "        self.cache['conv2'] = conv2\n",
    "\n",
    "        # ReLU activation\n",
    "        relu2 = relu(conv2)\n",
    "        self.cache['relu2'] = relu2\n",
    "\n",
    "        # Max Pooling 2\n",
    "        pool2, mask2 = max_pooling_with_mask(relu2, pool_size=2, stride=2)  # Shape: (11,11,8)\n",
    "        self.cache['pool2'] = pool2\n",
    "        self.cache['mask2'] = mask2\n",
    "\n",
    "        # Flatten\n",
    "        flattened = pool2.flatten()  # Shape: (11*11*8,)\n",
    "        self.cache['flattened'] = flattened\n",
    "\n",
    "        # Fully Connected Layer\n",
    "        fc = np.dot(flattened, self.fc_weights) + self.fc_bias  # Shape: (2,)\n",
    "        self.cache['fc'] = fc\n",
    "\n",
    "        return fc\n",
    "\n",
    "    def backward(self, d_fc):\n",
    "        \"\"\"\n",
    "        Backward pass through the network.\n",
    "        d_fc: Gradient with respect to the FC output (shape: (2,))\n",
    "        Updates parameters using SGD.\n",
    "        Returns gradients for debugging.\n",
    "        \"\"\"\n",
    "        # Fully Connected Layer Backpropagation.\n",
    "        flattened = self.cache['flattened']  # Shape: (11*11*8,)\n",
    "        d_fc_weights = np.outer(flattened, d_fc)  # Shape: (flattened_dim,2)\n",
    "        d_fc_bias = d_fc  # Shape: (2,)\n",
    "        d_flattened = np.dot(self.fc_weights, d_fc)  # Shape: (flattened_dim,)\n",
    "\n",
    "        # Unflatten to match pool2 shape: (11,11,8)\n",
    "        d_pool2 = d_flattened.reshape(self.cache['pool2'].shape)\n",
    "\n",
    "        # Max Pooling 2 Backpropagation.\n",
    "        mask2 = self.cache['mask2']\n",
    "        d_relu2 = max_pooling_backward(mask2, d_pool2, pool_size=2, stride=2)  # Shape: (22,22,8)\n",
    "\n",
    "        # ReLU 2 Backpropagation.\n",
    "        conv2 = self.cache['conv2']\n",
    "        d_conv2 = relu_backward(conv2, d_relu2)\n",
    "\n",
    "        # Convolution Layer 2 Backpropagation.\n",
    "        pool1 = self.cache['pool1']\n",
    "        d_pool1_from_conv2, d_conv2_kernels = conv2d_multi_channel_backward(pool1, self.conv2_kernels, d_conv2)\n",
    "\n",
    "        # Max Pooling 1 Backpropagation.\n",
    "        mask1 = self.cache['mask1']\n",
    "        d_relu1 = max_pooling_backward(mask1, d_pool1_from_conv2, pool_size=2, stride=2)  # Shape: (48,48,4)\n",
    "\n",
    "        # ReLU 1 Backpropagation.\n",
    "        conv1 = self.cache['conv1']\n",
    "        d_conv1 = relu_backward(conv1, d_relu1)\n",
    "\n",
    "        # Convolution Layer 1 Backpropagation.\n",
    "        x = self.cache['x']\n",
    "        d_x, d_conv1_kernels = conv2d_multi_channel_backward(x, self.conv1_kernels, d_conv1)\n",
    "\n",
    "        # SGD Weight Updates.\n",
    "        self.fc_weights -= self.lr * d_fc_weights\n",
    "        self.fc_bias    -= self.lr * d_fc_bias\n",
    "        self.conv2_kernels -= self.lr * d_conv2_kernels\n",
    "        self.conv1_kernels -= self.lr * d_conv1_kernels\n",
    "\n",
    "        grads = {\n",
    "            'd_fc_weights': d_fc_weights,\n",
    "            'd_fc_bias': d_fc_bias,\n",
    "            'd_conv2_kernels': d_conv2_kernels,\n",
    "            'd_conv1_kernels': d_conv1_kernels,\n",
    "        }\n",
    "        return grads\n",
    "\n",
    "def mse_loss(output, target):\n",
    "    \"\"\"\n",
    "    Mean Squared Error loss.\n",
    "    Returns loss and gradient with respect to output.\n",
    "    \"\"\"\n",
    "    loss = 0.5 * np.sum((output - target) ** 2)\n",
    "    d_loss = output - target\n",
    "    return loss, d_loss\n",
    "\n",
    "def test(cnn, x, y):\n",
    "\n",
    "  # Final evaluation on the test set.\n",
    "  test_loss = 0\n",
    "  correct = 0\n",
    "  for i in range(len(x)):\n",
    "      x1 = x[i]\n",
    "      target = y[i]\n",
    "      output = cnn.forward(x1)\n",
    "      loss, _ = mse_loss(output, target)\n",
    "      test_loss += loss\n",
    "      pred = np.argmax(output)\n",
    "      true = np.argmax(target)\n",
    "      if pred == true:\n",
    "          correct += 1\n",
    "  avg_loss = test_loss / len(x)\n",
    "  accuracy = correct / len(x)\n",
    "  print(f\"{avg_loss:.4f}, {accuracy:.4f}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Training samples: 456, Test samples: 20\n"
     ]
    }
   ],
   "source": [
    "\"\"\"\n",
    "This sell contains the code for image loading and preprocessing.\n",
    "\n",
    "You can do whatever you want with the GIVEN training dataset - preprocess and augment in any way.\n",
    "At the end of the preprocessing step, however, the output MUST be resized to (50,50) and converted to grayscale.\n",
    "\n",
    "DO NOT use any additional data from the wherever, only work with the GIVEN training dataset.\n",
    "\n",
    "\"\"\"\n",
    "from PIL import ImageEnhance\n",
    "\n",
    "def augment_image(img): # доавил\n",
    "    rotation = np.random.uniform(-15, 15)\n",
    "    img = img.rotate(rotation)\n",
    "    \n",
    "    shift_x, shift_y = np.random.randint(-5, 6), np.random.randint(-5, 6)\n",
    "    img = img.transform(img.size, Image.AFFINE, (1, 0, shift_x, 0, 1, shift_y))\n",
    "    \n",
    "    factor = np.random.uniform(0.8, 1.2)\n",
    "    img = ImageEnhance.Brightness(img).enhance(factor)\n",
    "    img = ImageEnhance.Contrast(img).enhance(factor)\n",
    "    \n",
    "    return img\n",
    "\n",
    "def preprocess_pipeline(path, image_size=(50, 50), augment=False):\n",
    "    \"\"\"\n",
    "    Open an image from the given path and process it through a pipeline.\n",
    "    \n",
    "    Steps:\n",
    "      - Convert to grayscale.\n",
    "      - Resize to image_size.\n",
    "      - (Optional) Do whatever you want to preprocess the image\n",
    "      - Convert to a NumPy array and normalize pixel values to [0,1].\n",
    "    \n",
    "    Parameters:\n",
    "      path (str): Path to the image file.\n",
    "      image_size (tuple): Target size for resizing (width, height).\n",
    "    \n",
    "    Returns:\n",
    "      np.ndarray: Processed image as a NumPy array.\n",
    "      - it is required that the output size is (50,50) and the images are converted to grayscale.\n",
    "    \"\"\"\n",
    "    try:\n",
    "        img = Image.open(path)\n",
    "        # Convert to grayscale.\n",
    "        img = img.convert(\"L\")\n",
    "        \n",
    "        # Resize image.\n",
    "        img = img.resize(image_size)\n",
    "        \n",
    "        # Convert to NumPy array and normalize.\n",
    "        if augment:\n",
    "            # копия ауг\n",
    "            augmented_img = img.copy()\n",
    "            augmented_img = augment_image(augmented_img)\n",
    "            img_array = np.array(augmented_img).astype(np.float32) / 255.0\n",
    "        else:\n",
    "            img_array = np.array(img).astype(np.float32) / 255.0\n",
    "        return img_array\n",
    "    except Exception as e:\n",
    "        print(f\"Error processing image {path}: {e}\")\n",
    "        return None\n",
    "\n",
    "def load_images_from_folder(folder, label, image_size=(50, 50), augment=False, augment_factor=1):\n",
    "    \"\"\"\n",
    "    Load images from a folder using the preprocessing pipeline.\n",
    "    \n",
    "    \n",
    "    Parameters:\n",
    "      folder (str): Folder path.\n",
    "      label (list): One-hot vector label (e.g., [1,0] or [0,1]).\n",
    "      image_size (tuple): Target size for resizing.\n",
    "    \n",
    "    Returns:\n",
    "      tuple: Two lists containing the processed images and their labels.\n",
    "    \"\"\"\n",
    "    images = []\n",
    "    labels = []\n",
    "    for filename in os.listdir(folder):\n",
    "        if filename.lower().endswith(('.png', '.jpg', '.jpeg', '.bmp')):\n",
    "            path = os.path.join(folder, filename)\n",
    "            img_array = preprocess_pipeline(path, image_size=image_size)\n",
    "            if img_array is not None:\n",
    "                images.append(img_array)\n",
    "                labels.append(label)\n",
    "                if augment:\n",
    "                    for _ in range(augment_factor):\n",
    "                        augmented_img_array = preprocess_pipeline(path, image_size=image_size, augment=True)\n",
    "                        if augmented_img_array is not None:\n",
    "                            images.append(augmented_img_array)\n",
    "                            labels.append(label)\n",
    "    return images, labels\n",
    "\n",
    "def add_noise(img_array, noise_factor=0.1): # дрбавил\n",
    "    noise = np.random.normal(loc=0.0, scale=noise_factor, size=img_array.shape)\n",
    "    noisy_array = img_array + noise\n",
    "    return np.clip(noisy_array, 0., 1.)\n",
    "\n",
    "# ---------------------- Dataset Loading ----------------------\n",
    "# Replace these with your actual folder paths.\n",
    "train_ferrari_folder = \"./ferrari\"  # e.g., \"/home/user/datasets/train_ferrari\"\n",
    "train_jeep_folder    = \"./jeep\"     # e.g., \"/home/user/datasets/train_jeep\"\n",
    "test_ferrari_folder  = \"./ferrari_test\"      # e.g., \"/home/user/datasets/test_ferrari\"\n",
    "test_jeep_folder     = \"./jeep_test\"         # e.g., \"/home/user/datasets/test_jeep\"\n",
    "\n",
    "images_ferrari_train, labels_ferrari_train = load_images_from_folder(train_ferrari_folder, [1, 0], image_size=(50, 50), augment=True, augment_factor=2)\n",
    "images_jeep_train,    labels_jeep_train    = load_images_from_folder(train_jeep_folder,    [0, 1], image_size=(50, 50), augment=True, augment_factor=2)\n",
    "\n",
    "# Combine training images.\n",
    "images = np.array(images_ferrari_train + images_jeep_train)\n",
    "labels = np.array(labels_ferrari_train + labels_jeep_train)\n",
    "\n",
    "# Shuffle the training dataset.\n",
    "indices = np.arange(len(images))\n",
    "np.random.shuffle(indices)\n",
    "images = images[indices]\n",
    "labels = labels[indices]\n",
    "\n",
    "# For this example, we use all images as training samples.\n",
    "x_train = images\n",
    "y_train = labels\n",
    "\n",
    "x_train = np.array([add_noise(img) for img in x_train]) # добавмл\n",
    "\n",
    "# Now load the test images.\n",
    "images_ferrari_test, labels_ferrari_test = load_images_from_folder(test_ferrari_folder, [1, 0], image_size=(50, 50))\n",
    "images_jeep_test,    labels_jeep_test    = load_images_from_folder(test_jeep_folder,    [0, 1], image_size=(50, 50))\n",
    "\n",
    "# Combine the test images.\n",
    "x_test = np.array(images_ferrari_test + images_jeep_test)\n",
    "y_test = np.array(labels_ferrari_test + labels_jeep_test)\n",
    "\n",
    "# Shuffle the test dataset.\n",
    "indices = np.arange(len(x_test))\n",
    "np.random.shuffle(indices)\n",
    "x_test = x_test[indices]\n",
    "y_test = y_test[indices]\n",
    "\n",
    "print(f\"Training samples: {len(x_train)}, Test samples: {len(x_test)}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Starting training on 456 samples.\n",
      "Current Train Loss: , Current Train Accuracy:\n",
      "0.4257, 0.5482\n",
      "Current Test Loss: , Current Test Accuracy:\n",
      "0.4367, 0.5500\n",
      "Epoch 1/10, Average Training Loss: 0.2620\n",
      "Current Train Loss: , Current Train Accuracy:\n",
      "0.2385, 0.6075\n",
      "Current Test Loss: , Current Test Accuracy:\n",
      "0.2612, 0.5500\n",
      "Epoch 2/10, Average Training Loss: 0.2365\n",
      "Current Train Loss: , Current Train Accuracy:\n",
      "0.2190, 0.6908\n",
      "Current Test Loss: , Current Test Accuracy:\n",
      "0.2460, 0.6000\n",
      "Epoch 3/10, Average Training Loss: 0.2169\n",
      "Current Train Loss: , Current Train Accuracy:\n",
      "0.2056, 0.6820\n",
      "Current Test Loss: , Current Test Accuracy:\n",
      "0.2193, 0.6500\n",
      "Epoch 4/10, Average Training Loss: 0.2005\n",
      "Current Train Loss: , Current Train Accuracy:\n",
      "0.1859, 0.7368\n",
      "Current Test Loss: , Current Test Accuracy:\n",
      "0.2085, 0.7500\n",
      "Epoch 5/10, Average Training Loss: 0.1869\n",
      "Current Train Loss: , Current Train Accuracy:\n",
      "0.1696, 0.7390\n",
      "Current Test Loss: , Current Test Accuracy:\n",
      "0.1792, 0.7000\n",
      "Epoch 6/10, Average Training Loss: 0.1816\n",
      "Current Train Loss: , Current Train Accuracy:\n",
      "0.1641, 0.7785\n",
      "Current Test Loss: , Current Test Accuracy:\n",
      "0.1891, 0.8500\n",
      "Epoch 7/10, Average Training Loss: 0.1720\n",
      "Current Train Loss: , Current Train Accuracy:\n",
      "0.1536, 0.8136\n",
      "Current Test Loss: , Current Test Accuracy:\n",
      "0.1764, 0.8500\n",
      "Epoch 8/10, Average Training Loss: 0.1626\n",
      "Current Train Loss: , Current Train Accuracy:\n",
      "0.1562, 0.7961\n",
      "Current Test Loss: , Current Test Accuracy:\n",
      "0.1708, 0.7500\n",
      "Epoch 9/10, Average Training Loss: 0.1550\n",
      "Current Train Loss: , Current Train Accuracy:\n",
      "0.1355, 0.8399\n",
      "Current Test Loss: , Current Test Accuracy:\n",
      "0.1606, 0.8500\n",
      "Epoch 10/10, Average Training Loss: 0.1503\n"
     ]
    }
   ],
   "source": [
    "\"\"\"\n",
    "This cell contains the code for training the CNN.\n",
    "\n",
    "DO NOT CHANGE ANYTHING IN THIS CELL, you are only allowed to run it and see the results.\n",
    "\n",
    "\"\"\"\n",
    "\n",
    "np.random.seed(42)\n",
    "cnn = SimpleCNN()\n",
    "\n",
    "num_train = len(x_train)\n",
    "num_test = len(x_test)\n",
    "print(f\"Starting training on {num_train} samples.\")\n",
    "\n",
    "epochs = 10\n",
    "for epoch in range(epochs):\n",
    "    print(f\"Current Train Loss: , Current Train Accuracy:\")\n",
    "    test(cnn,x_train,y_train)\n",
    "    print(f\"Current Test Loss: , Current Test Accuracy:\")\n",
    "    test(cnn,x_test,y_test)\n",
    "    total_loss = 0\n",
    "    # Shuffle training data at the beginning of each epoch.\n",
    "    indices = np.arange(num_train)\n",
    "    np.random.shuffle(indices)\n",
    "    x_train = x_train[indices]\n",
    "    y_train = y_train[indices]\n",
    "    for i in range(num_train):\n",
    "        x = x_train[i]\n",
    "        target = y_train[i]\n",
    "        # Forward pass.\n",
    "        output = cnn.forward(x)\n",
    "        loss, d_loss = mse_loss(output, target)\n",
    "        total_loss += loss\n",
    "        # Backward pass and update weights.\n",
    "        cnn.backward(d_loss)\n",
    "    avg_loss = total_loss / num_train\n",
    "    print(f\"Epoch {epoch+1}/{epochs}, Average Training Loss: {avg_loss:.4f}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [],
   "source": [
    "def save_model (model, filename):\n",
    "    np.savez(filename,\n",
    "            conv1_kernels = model.conv1_kernels,\n",
    "            conv2_kernels = model.conv2_kernels,\n",
    "            fc_weights = model.fc_weights,\n",
    "            fc_bias = model.fc_bias)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [],
   "source": [
    "save_model(cnn, \"my_cnn_model.npz\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.13.6"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
