{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "f8804305-0fc7-4872-bd0b-f664c11969aa",
   "metadata": {},
   "source": [
    "–ì–æ—Ç–æ–≤–æ! –ù–∏–∂–µ ‚Äî **–±—ã—Å—Ç—Ä—ã–π —É–Ω–∏–≤–µ—Ä—Å–∞–ª—å–Ω—ã–π toolkit –¥–ª—è CLIP** (–∑–∞–≥—Ä—É–∑–∫–∞, –ø—Ä–µ–ø—Ä–æ—Ü–µ—Å—Å, —ç–º–±–µ–¥–¥–∏–Ω–≥–∏, zero-shot –∫–ª–∞—Å—Å–∏—Ñ–∏–∫–∞—Ü–∏—è, –ø–æ–∏—Å–∫ –ø–æ —Å—Ö–æ–¥—Å—Ç–≤—É, –±–∞—Ç—á–∏, –∫—ç—à, —Ñ–æ–ª–±—ç–∫–∏).\n",
    "–§–æ—Ä–º–∞—Ç: **–∫–æ–¥ + –∫–æ—Ä–æ—Ç–∫–∏–µ –ø–æ—è—Å–Ω–µ–Ω–∏—è –≤ HTML-–≤—Å—Ç–∞–≤–∫–∞—Ö**. –•–æ—á–µ—à—å ‚Äî –ø—Ä–æ—Å—Ç–æ —É–¥–∞–ª–∏ HTML, –æ—Å—Ç–∞–Ω–µ—Ç—Å—è —á–∏—Å—Ç—ã–π Python.\n",
    "\n",
    "---\n",
    "\n",
    "## üöÄ –ë—ã—Å—Ç—Ä—ã–π —Å—Ç–∞—Ä—Ç\n",
    "\n",
    "```python\n",
    "# pip install torch torchvision pillow\n",
    "# pip install open-clip-torch  # (–∂–µ–ª–∞—Ç–µ–ª—å–Ω–æ) —Å–æ–≤—Ä–µ–º–µ–Ω–Ω–∞—è —Ä–µ–∞–ª–∏–∑–∞—Ü–∏—è\n",
    "# pip install ftfy regex tqdm   # –æ–ø—Ü–∏–æ–Ω–∞–ª—å–Ω–æ, –µ—Å–ª–∏ –∏—Å–ø–æ–ª—å–∑—É–µ—à—å openai/clip\n",
    "\n",
    "```\n",
    "\n",
    "---\n",
    "\n",
    "## 0) –ò–º–ø–æ—Ä—Ç—ã –∏ —É—Å—Ç—Ä–æ–π—Å—Ç–≤–æ\n",
    "\n",
    "```python\n",
    "import os, math, time, pathlib, warnings\n",
    "from typing import List, Tuple, Dict, Optional\n",
    "\n",
    "import numpy as np\n",
    "from PIL import Image\n",
    "\n",
    "import torch\n",
    "from torch import nn\n",
    "from torchvision import transforms\n",
    "\n",
    "warnings.filterwarnings(\"ignore\")\n",
    "\n",
    "def get_device():\n",
    "    if torch.cuda.is_available(): \n",
    "        return torch.device(\"cuda\")\n",
    "    if hasattr(torch.backends, \"mps\") and torch.backends.mps.is_available():\n",
    "        return torch.device(\"mps\")  # Apple Silicon\n",
    "    return torch.device(\"cpu\")\n",
    "\n",
    "DEVICE = get_device()\n",
    "print(\"Device:\", DEVICE)\n",
    "```\n",
    "\n",
    "```markdown\n",
    "<div>\n",
    "<b>–ó–∞—á–µ–º:</b> –≤—ã–±–∏—Ä–∞–µ–º –¥–æ—Å—Ç—É–ø–Ω—ã–π –¥–µ–≤–∞–π—Å. –ù–∞ —Å–æ—Ä–µ–≤–Ω–æ–≤–∞–Ω–∏—è—Ö –±–µ–∑ GPU –≤—Å—ë —Ä–∞–±–æ—Ç–∞–µ—Ç, –ø—Ä–æ—Å—Ç–æ –º–µ–¥–ª–µ–Ω–Ω–µ–µ.<br>\n",
    "<b>–°–æ–≤–µ—Ç:</b> –¥–ª—è –±–æ–ª—å—à–∏—Ö –±–∞—Ç—á–µ–π –∏ —É—Å–∫–æ—Ä–µ–Ω–∏—è –Ω–∞ GPU –∏—Å–ø–æ–ª—å–∑—É–µ–º <code>float16</code> + <code>torch.autocast</code>.\n",
    "</div>\n",
    "```\n",
    "\n",
    "---\n",
    "\n",
    "## 1) –ó–∞–≥—Ä—É–∑–∫–∞ CLIP (open-clip ‚Üí fallback –Ω–∞ openai/clip)\n",
    "\n",
    "```python\n",
    "class CLIPWrapper:\n",
    "    \"\"\"\n",
    "    –£–Ω–∏–≤–µ—Ä—Å–∞–ª—å–Ω–∞—è –æ–±—ë—Ä—Ç–∫–∞: —Å–Ω–∞—á–∞–ª–∞ –ø—ã—Ç–∞–µ—Ç—Å—è –∑–∞–≥—Ä—É–∑–∏—Ç—å open-clip,\n",
    "    –µ—Å–ª–∏ –Ω–µ –ø–æ–ª—É—á–∏–ª–æ—Å—å ‚Äî –ø—Ä–æ–±—É–µ—Ç openai/clip.\n",
    "    \"\"\"\n",
    "    def __init__(self, model_name: str = \"ViT-B-32\", pretrained: str = \"laion2b_s34b_b79k\"):\n",
    "        self.model_name = model_name\n",
    "        self.pretrained = pretrained\n",
    "        self.backend = None  # \"open_clip\" | \"openai_clip\"\n",
    "        self.model = None\n",
    "        self.preprocess = None\n",
    "        self.tokenize = None\n",
    "        self.image_mean = [0.48145466, 0.4578275, 0.40821073]\n",
    "        self.image_std  = [0.26862954, 0.26130258, 0.27577711]\n",
    "        self._load()\n",
    "\n",
    "    def _load(self):\n",
    "        # try open-clip first\n",
    "        try:\n",
    "            import open_clip\n",
    "            self.model, _, self.preprocess = open_clip.create_model_and_transforms(\n",
    "                self.model_name, pretrained=self.pretrained, device=DEVICE\n",
    "            )\n",
    "            self.model.eval().to(DEVICE)\n",
    "            self.tokenize = open_clip.get_tokenizer(self.model_name)\n",
    "            self.backend = \"open_clip\"\n",
    "            return\n",
    "        except Exception as e:\n",
    "            print(\"[warn] open-clip –Ω–µ —É–¥–∞–ª–æ—Å—å:\", e)\n",
    "\n",
    "        # fallback: openai/clip\n",
    "        try:\n",
    "            import clip  # pip install git+https://github.com/openai/clip.git  (–∏–ª–∏ pypi: clip-anytorch)\n",
    "            self.model, self.preprocess = clip.load(self.model_name, device=DEVICE, jit=False)\n",
    "            self.model.eval().to(DEVICE)\n",
    "            self.tokenize = clip.tokenize\n",
    "            self.backend = \"openai_clip\"\n",
    "        except Exception as e:\n",
    "            raise RuntimeError(\n",
    "                \"–ù–µ —É–¥–∞–ª–æ—Å—å –∑–∞–≥—Ä—É–∑–∏—Ç—å CLIP –Ω–∏ —á–µ—Ä–µ–∑ open-clip, –Ω–∏ —á–µ—Ä–µ–∑ openai/clip. \"\n",
    "                \"–ü—Ä–æ–≤–µ—Ä—å –∑–∞–≤–∏—Å–∏–º–æ—Å—Ç–∏ –∏ –≤–µ—Å–∞.\"\n",
    "            ) from e\n",
    "\n",
    "    def build_image_transform(self, size: int = 224):\n",
    "        # –Ω–∞ —Å–ª—É—á–∞–π, –µ—Å–ª–∏ —Ö–æ—á–µ—à—å —Å–≤–æ–π —Ç—Ä–∞–Ω—Å—Ñ–æ—Ä–º (–±—ã—Å—Ç—Ä–µ–µ, —á–µ–º PIL default)\n",
    "        return transforms.Compose([\n",
    "            transforms.Resize((size, size), interpolation=Image.BICUBIC),\n",
    "            transforms.ToTensor(),\n",
    "            transforms.Normalize(mean=self.image_mean, std=self.image_std),\n",
    "        ])\n",
    "\n",
    "    @torch.no_grad()\n",
    "    def encode_images(self, images: List[Image.Image], batch_size: int = 32, fp16: bool = True) -> np.ndarray:\n",
    "        \"\"\"\n",
    "        –í–æ–∑–≤—Ä–∞—â–∞–µ—Ç L2-–Ω–æ—Ä–º–∏—Ä–æ–≤–∞–Ω–Ω—ã–µ —ç–º–±–µ–¥–¥–∏–Ω–≥–∏ –∏–∑–æ–±—Ä–∞–∂–µ–Ω–∏–π: [N, D]\n",
    "        \"\"\"\n",
    "        embs = []\n",
    "        tfm = self.preprocess  # –≥–æ—Ç–æ–≤—ã–π –ø—Ä–µ–ø—Ä–æ—Ü–µ—Å—Å –æ—Ç –º–æ–¥–µ–ª–∏ (—á–∞—Å—Ç–æ fastest)\n",
    "        # –∏–ª–∏ –∫–∞—Å—Ç–æ–º–Ω—ã–π: tfm = self.build_image_transform(224)\n",
    "\n",
    "        use_autocast = fp16 and (DEVICE.type == \"cuda\")\n",
    "        for i in range(0, len(images), batch_size):\n",
    "            batch = images[i:i+batch_size]\n",
    "            x = torch.stack([tfm(im) for im in batch]).to(DEVICE, non_blocking=True)\n",
    "            with torch.autocast(device_type=\"cuda\", dtype=torch.float16, enabled=use_autocast):\n",
    "                feats = self.model.encode_image(x)\n",
    "            feats = feats / feats.norm(dim=-1, keepdim=True)\n",
    "            embs.append(feats.detach().float().cpu().numpy())\n",
    "        return np.concatenate(embs, axis=0) if embs else np.zeros((0, 512), dtype=np.float32)\n",
    "\n",
    "    @torch.no_grad()\n",
    "    def encode_texts(self, texts: List[str], batch_size: int = 64, fp16: bool = True) -> np.ndarray:\n",
    "        \"\"\"\n",
    "        –í–æ–∑–≤—Ä–∞—â–∞–µ—Ç L2-–Ω–æ—Ä–º–∏—Ä–æ–≤–∞–Ω–Ω—ã–µ —ç–º–±–µ–¥–¥–∏–Ω–≥–∏ —Ç–µ–∫—Å—Ç–æ–≤: [M, D]\n",
    "        \"\"\"\n",
    "        embs = []\n",
    "        use_autocast = fp16 and (DEVICE.type == \"cuda\")\n",
    "        for i in range(0, len(texts), batch_size):\n",
    "            batch = texts[i:i+batch_size]\n",
    "            tok = self.tokenize(batch).to(DEVICE, non_blocking=True)\n",
    "            with torch.autocast(device_type=\"cuda\", dtype=torch.float16, enabled=use_autocast):\n",
    "                feats = self.model.encode_text(tok)\n",
    "            feats = feats / feats.norm(dim=-1, keepdim=True)\n",
    "            embs.append(feats.detach().float().cpu().numpy())\n",
    "        return np.concatenate(embs, axis=0) if embs else np.zeros((0, 512), dtype=np.float32)\n",
    "\n",
    "    @torch.no_grad()\n",
    "    def zero_shot(self, images: List[Image.Image], classnames: List[str], templates: Optional[List[str]] = None,\n",
    "                  batch_size: int = 32, fp16: bool = True) -> np.ndarray:\n",
    "        \"\"\"\n",
    "        Zero-shot –∫–ª–∞—Å—Å–∏—Ñ–∏–∫–∞—Ü–∏—è: –≤–æ–∑–≤—Ä–∞—â–∞–µ—Ç –≤–µ—Ä–æ—è—Ç–Ω–æ—Å—Ç–∏ [N, C].\n",
    "        –®–∞–±–ª–æ–Ω—ã (prompt ensembling) –ø–æ–≤—ã—à–∞—é—Ç —Ç–æ—á–Ω–æ—Å—Ç—å.\n",
    "        \"\"\"\n",
    "        if templates is None:\n",
    "            templates = [\n",
    "                \"a photo of a {}\",\n",
    "                \"a close-up photo of a {}\",\n",
    "                \"a bright photo of a {}\",\n",
    "                \"a dark photo of a {}\",\n",
    "                \"a cropped photo of a {}\",\n",
    "                \"a photo of the {}\",\n",
    "            ]\n",
    "        # –ø–æ—Å—Ç—Ä–æ–∏–º —Ç–µ–∫—Å—Ç–æ–≤—ã–π –ø—Ä–æ—Ç–æ–∫–æ–ª (—É—Å—Ä–µ–¥–Ω–µ–Ω–∏–µ –ø–æ —à–∞–±–ª–æ–Ω–∞–º)\n",
    "        text_embs_all = []\n",
    "        for t in templates:\n",
    "            texts = [t.format(c) for c in classnames]\n",
    "            text_embs_all.append(self.encode_texts(texts, batch_size=batch_size, fp16=fp16))\n",
    "        text_embs = np.stack(text_embs_all, axis=0).mean(axis=0)  # [C, D]\n",
    "        text_embs = text_embs / np.linalg.norm(text_embs, axis=1, keepdims=True)\n",
    "\n",
    "        # —ç–º–±–µ–¥–¥–∏–Ω–≥–∏ –∏–∑–æ–±—Ä–∞–∂–µ–Ω–∏–π\n",
    "        img_embs = self.encode_images(images, batch_size=batch_size, fp16=fp16)  # [N, D]\n",
    "        # –ª–æ–≥–∏—Ç—ã ~ –∫–æ—Å–∏–Ω—É—Å*scale (–æ–±—ã—á–Ω–æ 100)\n",
    "        logits = (img_embs @ text_embs.T) * 100.0\n",
    "        # stable softmax\n",
    "        logits -= logits.max(axis=1, keepdims=True)\n",
    "        probs = np.exp(logits)\n",
    "        probs /= probs.sum(axis=1, keepdims=True)\n",
    "        return probs  # [N, C]\n",
    "```\n",
    "\n",
    "```markdown\n",
    "<div>\n",
    "<b>–ü–æ—á–µ–º—É —Ç–∞–∫:</b> <code>open-clip</code> ‚Äî –±–æ–ª–µ–µ —Å–≤–µ–∂–∞—è –∏ —É–¥–æ–±–Ω–∞—è —Ä–µ–∞–ª–∏–∑–∞—Ü–∏—è; –µ—Å–ª–∏ –µ—ë –Ω–µ—Ç ‚Äî –ø–∞–¥–∞–µ–º –Ω–∞ <code>openai/clip</code>.<br>\n",
    "<b>–ù–æ—Ä–º–∞–ª–∏–∑–∞—Ü–∏—è:</b> —ç–º–±–µ–¥–¥–∏–Ω–≥–∏ L2-–Ω–æ—Ä–º–∏—Ä—É—é—Ç—Å—è ‚Äî –∫–æ—Å–∏–Ω—É—Å ‚âà —Å–∫–∞–ª—è—Ä–Ω–æ–µ –ø—Ä–æ–∏–∑–≤–µ–¥–µ–Ω–∏–µ.<br>\n",
    "<b>–®–∞–±–ª–æ–Ω—ã:</b> prompt ensembling —á–∞—Å—Ç–æ –¥–∞—ë—Ç +1‚Äì5% —Ç–æ—á–Ω–æ—Å—Ç–∏ –≤ zero-shot.\n",
    "</div>\n",
    "```\n",
    "\n",
    "---\n",
    "\n",
    "## 2) –£—Ç–∏–ª–∏—Ç—ã –∑–∞–≥—Ä—É–∑–∫–∏ –¥–∞–Ω–Ω—ã—Ö\n",
    "\n",
    "```python\n",
    "def load_image(path: str, mode: str = \"RGB\") -> Image.Image:\n",
    "    im = Image.open(path)\n",
    "    return im.convert(mode) if mode else im\n",
    "\n",
    "def load_images_from_paths(paths: List[str], mode: str = \"RGB\") -> List[Image.Image]:\n",
    "    out=[]\n",
    "    for p in paths:\n",
    "        try: out.append(load_image(p, mode))\n",
    "        except Exception as e: print(\"[warn] –ø—Ä–æ–ø—É—Å–∫–∞—é\", p, e)\n",
    "    return out\n",
    "\n",
    "def list_images(folder: str, exts=(\".jpg\",\".jpeg\",\".png\",\".bmp\",\".webp\")) -> List[str]:\n",
    "    p = pathlib.Path(folder)\n",
    "    files = [str(x) for x in p.rglob(\"*\") if x.suffix.lower() in exts]\n",
    "    return sorted(files)\n",
    "```\n",
    "\n",
    "```markdown\n",
    "<div>\n",
    "<b>–°–æ–≤–µ—Ç:</b> –Ω–µ –∑–∞–≥—Ä—É–∂–∞–π —Å—Ä–∞–∑—É –¥–µ—Å—è—Ç–∫–∏ —Ç—ã—Å—è—á –∫–∞—Ä—Ç–∏–Ω–æ–∫ –≤ –ø–∞–º—è—Ç—å. –î–µ–ª–∞–π –±–∞—Ç—á–∏ (—Å–º. <code>encode_images</code>) –∏/–∏–ª–∏ —Å–æ—Ö—Ä–∞–Ω—è–π —ç–º–±–µ–¥–¥–∏–Ω–≥–∏ –Ω–∞ –¥–∏—Å–∫ (.npy).\n",
    "</div>\n",
    "```\n",
    "\n",
    "---\n",
    "\n",
    "## 3) –ë—ã—Å—Ç—Ä—ã–π –ø—Ä–∏–º–µ—Ä: zero-shot –Ω–∞ –ø–∞–ø–∫–µ —Å –∫–ª–∞—Å—Å–∞–º–∏\n",
    "\n",
    "**–°—Ç—Ä—É–∫—Ç—É—Ä–∞:**\n",
    "\n",
    "```\n",
    "dataset/\n",
    "  cat/  (–∫–∞—Ä—Ç–∏–Ω–∫–∏ –∫–æ—Ç–æ–≤)\n",
    "  dog/  (–∫–∞—Ä—Ç–∏–Ω–∫–∏ —Å–æ–±–∞–∫)\n",
    "```\n",
    "\n",
    "```python\n",
    "def infer_classes_from_subdirs(root: str) -> Tuple[List[str], Dict[int, str], Dict[str, List[str]]]:\n",
    "    \"\"\"\n",
    "    —Å–±–æ—Ä: classnames, –∏–Ω–¥–µ–∫—Å->–∫–ª–∞—Å—Å, –∫–ª–∞—Å—Å->—Å–ø–∏—Å–æ–∫ –ø—É—Ç–µ–π\n",
    "    \"\"\"\n",
    "    root = pathlib.Path(root)\n",
    "    classnames = sorted([d.name for d in root.iterdir() if d.is_dir()])\n",
    "    idx2cls = {i:c for i, c in enumerate(classnames)}\n",
    "    cls2paths = {c: list_images(str(root / c)) for c in classnames}\n",
    "    return classnames, idx2cls, cls2paths\n",
    "\n",
    "# –î–µ–º–æ–Ω—Å—Ç—Ä–∞—Ü–∏—è:\n",
    "# root = \"dataset\"\n",
    "# classnames, idx2cls, cls2paths = infer_classes_from_subdirs(root)\n",
    "# print(classnames[:5], \"‚Ä¶\")\n",
    "```\n",
    "\n",
    "```python\n",
    "def demo_zero_shot_folder(root: str, model_name=\"ViT-B-32\", pretrained=\"laion2b_s34b_b79k\",\n",
    "                          max_per_class: int = 8):\n",
    "    classnames, idx2cls, cls2paths = infer_classes_from_subdirs(root)\n",
    "    clip = CLIPWrapper(model_name=model_name, pretrained=pretrained)\n",
    "    templates = [\n",
    "        \"a photo of a {}\",\n",
    "        \"a close-up photo of a {}\",\n",
    "        \"a cropped photo of a {}\",\n",
    "        \"a bright photo of a {}\",\n",
    "        \"a dark photo of a {}\",\n",
    "    ]\n",
    "    # —Å–æ–±–µ—Ä—ë–º –º–∏–Ω–∏-–Ω–∞–±–æ—Ä –∏–∑–æ–±—Ä–∞–∂–µ–Ω–∏–π\n",
    "    imgs, gts = [], []\n",
    "    for c in classnames:\n",
    "        paths = cls2paths[c][:max_per_class]\n",
    "        imgs.extend(load_images_from_paths(paths, \"RGB\"))\n",
    "        gts.extend([c]*len(paths))\n",
    "\n",
    "    probs = clip.zero_shot(imgs, classnames, templates=templates, batch_size=32, fp16=True)  # [N, C]\n",
    "    pred_idx = probs.argmax(axis=1)\n",
    "    preds = [classnames[i] for i in pred_idx]\n",
    "\n",
    "    acc = (np.array(preds) == np.array(gts)).mean() if len(gts) else 0.0\n",
    "    print(f\"Zero-shot accuracy (subset): {acc:.4f} on {len(gts)} images\")\n",
    "\n",
    "    # –ø–æ–∫–∞–∂–µ–º —Ç–æ–ø-3 –¥–ª—è –ø–µ—Ä–≤—ã—Ö 3 –∫–∞—Ä—Ç–∏–Ω–æ–∫\n",
    "    for i in range(min(3, len(imgs))):\n",
    "        order = np.argsort(-probs[i])[:3]\n",
    "        print(f\"[{i}] GT={gts[i]} | top3:\", [(classnames[j], float(probs[i][j])) for j in order])\n",
    "\n",
    "# –ü—Ä–∏–º–µ—Ä:\n",
    "# demo_zero_shot_folder(\"dataset\")\n",
    "```\n",
    "\n",
    "```markdown\n",
    "<div>\n",
    "<b>–ì–¥–µ –ø–æ–ª–µ–∑–Ω–æ:</b> –∫–æ–≥–¥–∞ –Ω–µ—Ç —Ä–∞–∑–º–µ—á–µ–Ω–Ω–æ–≥–æ train, –∞ –∫–ª–∞—Å—Å—ã –∏–∑–≤–µ—Å—Ç–Ω—ã –ø–æ –Ω–∞–∑–≤–∞–Ω–∏—é. –¢–∏–ø–∏—á–Ω–æ –¥–ª—è –±—ã—Å—Ç—Ä—ã—Ö –ø—Ä–æ—Ç–æ—Ç–∏–ø–æ–≤/–ø—Ä–æ–≤–µ—Ä–æ–∫ –≥–∏–ø–æ—Ç–µ–∑.<br>\n",
    "<b>–ö–∞—á–µ—Å—Ç–≤–æ:</b> —Å–∏–ª—å–Ω–æ –∑–∞–≤–∏—Å–∏—Ç –æ—Ç –Ω–∞–∑–≤–∞–Ω–∏–π –∫–ª–∞—Å—Å–æ–≤ –∏ –∫–∞—á–µ—Å—Ç–≤–∞ —à–∞–±–ª–æ–Ω–æ–≤ (prompt engineering).\n",
    "</div>\n",
    "```\n",
    "\n",
    "---\n",
    "\n",
    "## 4) –ü–æ–∏—Å–∫ –ø–æ —Å—Ö–æ–¥—Å—Ç–≤—É (image‚Üítext, text‚Üíimage)\n",
    "\n",
    "```python\n",
    "def cosine_sim(a: np.ndarray, b: np.ndarray) -> np.ndarray:\n",
    "    # –ø—Ä–µ–¥–ø–æ–ª–∞–≥–∞–µ–º L2-–Ω–æ—Ä–º–∏—Ä–æ–≤–∫—É; –∏–Ω–∞—á–µ –Ω–æ—Ä–º–∞–ª–∏–∑—É–π: a/=||a||, b/=||b||\n",
    "    return a @ b.T\n",
    "\n",
    "def topk_sim(sims: np.ndarray, k: int = 5) -> List[List[Tuple[int, float]]]:\n",
    "    \"\"\"\n",
    "    sims: [N, M] ‚Äî —Å—Ö–æ–¥—Å—Ç–≤–∞ –º–µ–∂–¥—É N –∑–∞–ø—Ä–æ—Å–∞–º–∏ –∏ M –∫–∞–Ω–¥–∏–¥–∞—Ç–∞–º–∏\n",
    "    –í–æ–∑–≤—Ä–∞—â–∞–µ—Ç top-k –∏–Ω–¥–µ–∫—Å–æ–≤ –∏ –∑–Ω–∞—á–µ–Ω–∏–π –¥–ª—è –∫–∞–∂–¥–æ–π —Å—Ç—Ä–æ–∫–∏.\n",
    "    \"\"\"\n",
    "    N, M = sims.shape\n",
    "    out = []\n",
    "    for i in range(N):\n",
    "        idx = np.argpartition(-sims[i], k)[:k]\n",
    "        idx = idx[np.argsort(-sims[i, idx])]\n",
    "        out.append([(int(j), float(sims[i, j])) for j in idx])\n",
    "    return out\n",
    "\n",
    "def demo_retrieval(images: List[Image.Image], texts: List[str], model_name=\"ViT-B-32\", pretrained=\"laion2b_s34b_b79k\"):\n",
    "    clip = CLIPWrapper(model_name=model_name, pretrained=pretrained)\n",
    "    img_embs  = clip.encode_images(images, batch_size=32, fp16=True)   # [N, D]\n",
    "    text_embs = clip.encode_texts(texts,  batch_size=64, fp16=True)    # [M, D]\n",
    "    sims = cosine_sim(img_embs, text_embs)  # [N, M]\n",
    "\n",
    "    # image -> top texts\n",
    "    top_texts = topk_sim(sims, k=3)\n",
    "    # text -> top images (–ø–æ —Å—Ç–æ–ª–±—Ü–∞–º)\n",
    "    top_images = topk_sim(sims.T, k=3)\n",
    "\n",
    "    return top_texts, top_images\n",
    "\n",
    "# –ü—Ä–∏–º–µ—Ä:\n",
    "# imgs = load_images_from_paths(list_images(\"dataset/cat\")[:5])\n",
    "# texts = [\"a photo of a cat\", \"a photo of a dog\", \"a street\", \"a mountain\"]\n",
    "# it, ti = demo_retrieval(imgs, texts)\n",
    "# print(\"img->text:\", it[0])\n",
    "# print(\"text->img:\", ti[0])\n",
    "```\n",
    "\n",
    "```markdown\n",
    "<div>\n",
    "<b>–ó–∞—á–µ–º:</b> –±—ã—Å—Ç—Ä—ã–π —Ä–µ—Ç—Ä–∏–≤–ª (–ø–æ–∏—Å–∫) ‚Äî –æ—Å–Ω–æ–≤–∞ –º—É–ª—å—Ç–∏–º–æ–¥–∞–ª—å–Ω—ã—Ö –∑–∞–¥–∞—á: –ø–æ–∏—Å–∫–æ–≤–∏–∫–∏ –ø–æ –≥–∞–ª–µ—Ä–µ–µ, –ø–æ–¥–±–æ—Ä –ø–æ–¥–ø–∏—Å–µ–π, –ø—Ä–æ–≤–µ—Ä–∫–∞ –∫–∞—á–µ—Å—Ç–≤–∞ —ç–º–±–µ–¥–¥–∏–Ω–≥–æ–≤.<br>\n",
    "<b>–ù–æ—Ä–º–∞–ª–∏–∑–∞—Ü–∏—è:</b> –∑–¥–µ—Å—å –º—ã —É–∂–µ –≤–µ—Ä–Ω—É–ª–∏ L2-–Ω–æ—Ä–º–∏—Ä–æ–≤–∞–Ω–Ω—ã–µ —ç–º–±–µ–¥–¥–∏–Ω–≥–∏ ‚Äî –∫–æ—Å–∏–Ω—É—Å = —Å–∫–∞–ª—è—Ä–Ω–æ–µ –ø—Ä–æ–∏–∑–≤–µ–¥–µ–Ω–∏–µ.\n",
    "</div>\n",
    "```\n",
    "\n",
    "---\n",
    "\n",
    "## 5) –ö—ç—à–∏—Ä–æ–≤–∞–Ω–∏–µ —ç–º–±–µ–¥–¥–∏–Ω–≥–æ–≤ –Ω–∞ –¥–∏—Å–∫ (—É—Å–∫–æ—Ä—è–µ—Ç —ç–∫—Å–ø–µ—Ä–∏–º–µ–Ω—Ç—ã)\n",
    "\n",
    "```python\n",
    "def save_embeddings(path: str, array: np.ndarray):\n",
    "    os.makedirs(os.path.dirname(path) or \".\", exist_ok=True)\n",
    "    np.save(path, array)\n",
    "\n",
    "def load_embeddings(path: str) -> np.ndarray:\n",
    "    return np.load(path)\n",
    "\n",
    "# –ü—Ä–∏–º–µ—Ä:\n",
    "# img_embs = clip.encode_images(imgs)\n",
    "# save_embeddings(\"cache/img_embs.npy\", img_embs)\n",
    "# img_embs = load_embeddings(\"cache/img_embs.npy\")\n",
    "```\n",
    "\n",
    "```markdown\n",
    "<div>\n",
    "<b>–ó–∞—á–µ–º:</b> —á—Ç–æ–±—ã –Ω–µ –∫–æ–¥–∏—Ä–æ–≤–∞—Ç—å –∏–∑–æ–±—Ä–∞–∂–µ–Ω–∏—è/—Ç–µ–∫—Å—Ç—ã –∑–∞–Ω–æ–≤–æ –ø—Ä–∏ –∫–∞–∂–¥–æ–º –∑–∞–ø—É—Å–∫–µ (—ç–∫–æ–Ω–æ–º–∏—è –º–∏–Ω—É—Ç/—á–∞—Å–æ–≤).<br>\n",
    "<b>–°–æ–≤–µ—Ç:</b> —Ö—Ä–∞–Ω–∏ 1) —Å–ø–∏—Å–æ–∫ –ø—É—Ç–µ–π; 2) –º–∞—Å—Å–∏–≤ —ç–º–±–µ–¥–¥–∏–Ω–≥–æ–≤; 3) –≤–µ—Ä—Å–∏—é –º–æ–¥–µ–ª–∏ (—á—Ç–æ–±—ã –Ω–µ –ø–µ—Ä–µ–ø—É—Ç–∞—Ç—å).\n",
    "</div>\n",
    "```\n",
    "\n",
    "---\n",
    "\n",
    "## 6) –ú–∏–Ω–∏-CLI/—Å–∫—Ä–∏–ø—Ç (—Å–∞–º—ã–π –±—ã—Å—Ç—Ä—ã–π —Å—Ü–µ–Ω–∞—Ä–∏–π)\n",
    "\n",
    "```python\n",
    "if __name__ == \"__main__\":\n",
    "    # 1) –º–æ–¥–µ–ª—å\n",
    "    clip = CLIPWrapper(model_name=\"ViT-B-32\", pretrained=\"laion2b_s34b_b79k\")\n",
    "\n",
    "    # 2) –∫–∞—Ä—Ç–∏–Ω–∫–∏\n",
    "    img_paths = list_images(\"dataset/cat\")[:4] + list_images(\"dataset/dog\")[:4]\n",
    "    images = load_images_from_paths(img_paths)\n",
    "\n",
    "    # 3) zero-shot\n",
    "    classes = [\"cat\", \"dog\"]\n",
    "    probs = clip.zero_shot(images, classes)  # [N, C]\n",
    "    preds = [classes[i] for i in probs.argmax(axis=1)]\n",
    "    print(\"Zero-shot preds:\", preds[:8])\n",
    "\n",
    "    # 4) —Ä–µ—Ç—Ä–∏–≤–ª: image->text\n",
    "    texts = [f\"a photo of a {c}\" for c in classes]\n",
    "    img_embs  = clip.encode_images(images)\n",
    "    text_embs = clip.encode_texts(texts)\n",
    "    sims = cosine_sim(img_embs, text_embs)\n",
    "    print(\"Top text for first image:\", topk_sim(sims, 1)[0][0])\n",
    "```\n",
    "\n",
    "---\n",
    "\n",
    "## ‚ö†Ô∏è –¢–∏–ø–∏—á–Ω—ã–µ –æ—à–∏–±–∫–∏ –∏ —Ä–µ–∫–æ–º–µ–Ω–¥–∞—Ü–∏–∏\n",
    "\n",
    "```markdown\n",
    "<div>\n",
    "<ul>\n",
    "<li><b>–ù–µ—Ç –∏–Ω—Ç–µ—Ä–Ω–µ—Ç–∞ –Ω–∞ –∫–ª–∞—Å—Ç–µ—Ä–µ</b>: –ø–æ–¥–≥–æ—Ç–æ–≤—å –≤–µ—Å–∞ –∑–∞—Ä–∞–Ω–µ–µ (–ø–∞–ø–∫–∞ —Å –º–æ–¥–µ–ª—å—é). –í <code>open_clip.create_model_and_transforms</code> —É–∫–∞–∂–∏ –ø—É—Ç—å –∫ –ª–æ–∫–∞–ª—å–Ω—ã–º –≤–µ—Å–∞–º.</li>\n",
    "<li><b>CUDA OOM</b>: —É–º–µ–Ω—å—à–∞–π batch_size, –≤–∫–ª—é—á–∞–π fp16 (autocast), –∏—Å–ø–æ–ª—å–∑—É–π <code>with torch.no_grad()</code>.</li>\n",
    "<li><b>–ü–ª–æ—Ö–æ–µ –∫–∞—á–µ—Å—Ç–≤–æ zero-shot</b>: —Å–¥–µ–ª–∞–π prompt ensembling (–Ω–µ—Å–∫–æ–ª—å–∫–æ —à–∞–±–ª–æ–Ω–æ–≤), —É—Ç–æ—á–Ω—è–π —Ñ–æ—Ä–º—É–ª–∏—Ä–æ–≤–∫–∏ –∫–ª–∞—Å—Å–æ–≤ (–Ω–∞–ø—Ä–∏–º–µ—Ä, \"German shepherd dog\" –≤–º–µ—Å—Ç–æ \"dog\").</li>\n",
    "<li><b>–°–ª–∏—à–∫–æ–º –º–Ω–æ–≥–æ –∏–∑–æ–±—Ä–∞–∂–µ–Ω–∏–π</b>: –∫–æ–¥–∏—Ä—É–π –ø–∞—Ä—Ç–∏—è–º–∏ –∏ —Å—Ä–∞–∑—É —Å–æ—Ö—Ä–∞–Ω—è–π —ç–º–±–µ–¥–¥–∏–Ω–≥–∏ –Ω–∞ –¥–∏—Å–∫ (.npy).</li>\n",
    "<li><b>–†–∞–∑–Ω—ã–π —Ä–∞–∑–º–µ—Ä –≤—Ö–æ–¥–∞</b>: –≤—Å–µ–≥–¥–∞ –Ω–æ—Ä–º–∞–ª–∏–∑—É–π –ø—Ä–µ–ø—Ä–æ—Ü–µ—Å—Å–æ–º, –∫–æ—Ç–æ—Ä—ã–π –≤–µ—Ä–Ω—É–ª CLIP (–æ–Ω –±—ã—Å—Ç—Ä—ã–π –∏ —Å–æ–≤–º–µ—Å—Ç–∏–º—ã–π).</li>\n",
    "<li><b>–í–∞–ª–∏–¥–∞—Ü–∏—è</b>: zero-shot ‚Äî –Ω–µ –º–∞–≥–∏—è. –î–ª—è –∏—Ç–æ–≥–æ–≤–æ–π –º–æ–¥–µ–ª–∏ –ª—É—á—à–µ –¥–æ–æ–±—É—á–µ–Ω–∏–µ –∏–ª–∏ —Ö–æ—Ç—è –±—ã –∞–¥–∞–ø—Ç–∞—Ü–∏—è —Å –Ω–µ—Å–∫–æ–ª—å–∫–∏–º–∏ —Ä–∞–∑–º–µ—á–µ–Ω–Ω—ã–º–∏ –ø—Ä–∏–º–µ—Ä–∞–º–∏ (few-shot + linear probe).</li>\n",
    "</ul>\n",
    "</div>\n",
    "```\n",
    "\n",
    "---\n",
    "\n",
    "## üß© –ë—ã—Å—Ç—Ä—ã–π few-shot (linear probe –ø–æ–≤–µ—Ä—Ö —ç–º–±–µ–¥–¥–∏–Ω–≥–æ–≤)\n",
    "\n",
    "```python\n",
    "from sklearn.linear_model import LogisticRegression\n",
    "from sklearn.metrics import accuracy_score\n",
    "\n",
    "def linear_probe(train_imgs: List[Image.Image], y: List[int],\n",
    "                 val_imgs: List[Image.Image], y_val: List[int],\n",
    "                 model_name=\"ViT-B-32\", pretrained=\"laion2b_s34b_b79k\") -> float:\n",
    "    clip = CLIPWrapper(model_name, pretrained)\n",
    "    X_tr = clip.encode_images(train_imgs, batch_size=64, fp16=True)\n",
    "    X_va = clip.encode_images(val_imgs,   batch_size=64, fp16=True)\n",
    "\n",
    "    clf = LogisticRegression(max_iter=2000, n_jobs=4)\n",
    "    clf.fit(X_tr, y)\n",
    "    pred = clf.predict(X_va)\n",
    "    return accuracy_score(y_val, pred)\n",
    "\n",
    "# –ü—Ä–∏–º–µ—Ä:\n",
    "# acc = linear_probe(train_imgs, y, val_imgs, y_val)\n",
    "# print(\"Few-shot accuracy:\", acc)\n",
    "```\n",
    "\n",
    "```markdown\n",
    "<div>\n",
    "<b>–ö–æ–≥–¥–∞ –Ω—É–∂–Ω–æ:</b> –µ—Å–ª–∏ zero-shot —Å–ª–∞–±–æ–≤–∞—Ç, –∞ —É —Ç–µ–±—è –µ—Å—Ç—å –Ω–µ–º–Ω–æ–≥–æ —Ä–∞–∑–º–µ—á–µ–Ω–Ω—ã—Ö –¥–∞–Ω–Ω—ã—Ö. Linear probe —á–∞—Å—Ç–æ –¥–∞—ë—Ç –∑–∞–º–µ—Ç–Ω—ã–π –ø—Ä–∏—Ä–æ—Å—Ç –∑–∞ —Å—á–∏—Ç–∞–Ω–Ω—ã–µ –º–∏–Ω—É—Ç—ã.\n",
    "</div>\n",
    "```\n",
    "\n",
    "---\n",
    "\n",
    "### –ì–æ—Ç–æ–≤–æ!\n",
    "\n",
    "* –≠—Ç–æ **—É–Ω–∏–≤–µ—Ä—Å–∞–ª—å–Ω—ã–π, –±—ã—Å—Ç—Ä—ã–π** CLIP-toolkit: –∑–∞–≥—Ä—É–∑–∫–∞, –ø—Ä–µ–ø—Ä–æ—Ü–µ—Å—Å, —ç–º–±–µ–¥–¥–∏–Ω–≥–∏, zero-shot, —Ä–µ—Ç—Ä–∏–≤–ª, –∫—ç—à, few-shot.\n",
    "* –ú–æ–∂–Ω–æ –≤—Å—Ç–∞–≤–∏—Ç—å —Ü–µ–ª–∏–∫–æ–º –≤ –æ–¥–∏–Ω —Ñ–∞–π–ª –∏ —Å—Ä–∞–∑—É —Ä–∞–±–æ—Ç–∞—Ç—å.\n",
    "* –ù—É–∂–Ω–∞ –≤–µ—Ä—Å–∏—è –±–µ–∑ –∫–æ–º–º–µ–Ω—Ç–∞—Ä–∏–µ–≤/HTML ‚Äî –Ω–∞–ø–∏—à—É —á–∏—Å—Ç—ã–π `.py`.\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "d97aa685-7355-48dc-a1c5-5509fa1e9897",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.13.6"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
