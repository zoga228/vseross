{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "eaea7d33-855e-4f8f-baa9-e2413098b1a5",
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "import numpy as np\n",
    "from sklearn.linear_model import LinearRegression, Ridge\n",
    "from sklearn.preprocessing import PolynomialFeatures, StandardScaler\n",
    "from sklearn.pipeline import Pipeline\n",
    "import xgboost as xgb\n",
    "import lightgbm as lgb\n",
    "import catboost as cb\n",
    "import torch\n",
    "import torch.nn as nn\n",
    "import torch.optim as optim\n",
    "from torch.utils.data import DataLoader, TensorDataset\n",
    "from sklearn.ensemble import RandomForestRegressor\n",
    "from prophet import Prophet\n",
    "from statsmodels.tsa.arima.model import ARIMA\n",
    "import warnings\n",
    "warnings.filterwarnings('ignore')\n",
    "\n",
    "# Directory path\n",
    "dir_input = '/kaggle/input/kazakhstan-ai-respa-take-home'\n",
    "train_df = pd.read_csv(f\"{dir_input}/train.csv\", parse_dates=['submitted_date'])\n",
    "test_df = pd.read_csv(f\"{dir_input}/test.csv\", parse_dates=['week_start','week_end'])\n",
    "submission_df = pd.read_csv(f\"{dir_input}/sample_submission.csv\")\n",
    "\n",
    "# Prepare weekly data\n",
    "train_df['week_start'] = train_df['submitted_date'].dt.to_period('W').apply(lambda r: r.start_time)\n",
    "weekly = (\n",
    "    train_df.groupby(['category','week_start'])['num_papers']\n",
    "    .sum()\n",
    "    .reset_index()\n",
    "    .sort_values(['category','week_start'])\n",
    ")\n",
    "\n",
    "def create_time_features(df):\n",
    "    df = df.copy()\n",
    "    df['t'] = np.arange(len(df))\n",
    "    df['month'] = df['week_start'].dt.month\n",
    "    df['quarter'] = df['week_start'].dt.quarter\n",
    "    df['sin_woy'] = np.sin(2 * np.pi * df['week_start'].dt.isocalendar().week / 52)\n",
    "    df['cos_woy'] = np.cos(2 * np.pi * df['week_start'].dt.isocalendar().week / 52)\n",
    "    df['year'] = df['week_start'].dt.year\n",
    "    df['day_of_year'] = df['week_start'].dt.dayofyear\n",
    "\n",
    "    lags = [1, 2, 3, 4, 8, 12, 26, 52]\n",
    "    windows = [4, 12, 26, 52]\n",
    "\n",
    "    for lag in lags:\n",
    "        df[f'lag_{lag}'] = df['num_papers'].shift(lag)\n",
    "    for w in windows:\n",
    "        rolled = df['num_papers'].shift(1).rolling(w, min_periods=1)\n",
    "        df[f'roll_mean_{w}'] = rolled.mean()\n",
    "        df[f'roll_std_{w}'] = rolled.std()\n",
    "        df[f'roll_max_{w}'] = rolled.max()\n",
    "        df[f'roll_min_{w}'] = rolled.min()\n",
    "\n",
    "    # Fill missing values\n",
    "    for col in df.columns:\n",
    "        if col.startswith('lag_'):\n",
    "            n = int(col.split('_')[1])\n",
    "            q = 0.005 if n >= 12 else 0.75\n",
    "            fill = df[col].quantile(q) if not df[col].isna().all() else 0\n",
    "            df[col] = df[col].fillna(fill)\n",
    "\n",
    "        elif col.startswith('roll_'):\n",
    "            w = int(col.split('_')[2])\n",
    "            q = 0.005 if w >= 12 else 0.75\n",
    "            fill = df[col].quantile(q) if not df[col].isna().all() else 0\n",
    "            df[col] = df[col].fillna(fill)\n",
    "\n",
    "    return df.reset_index(drop=True)\n",
    "\n",
    "def create_lstm_dataset(X, y, timesteps=4):\n",
    "    \"\"\"Create dataset for LSTM model with specified timesteps\"\"\"\n",
    "    Xs, ys = [], []\n",
    "    for i in range(len(X) - timesteps):\n",
    "        Xs.append(X[i:(i + timesteps)].values)\n",
    "        ys.append(y[i + timesteps])\n",
    "    return np.array(Xs), np.array(ys)\n",
    "\n",
    "class LSTMModel(nn.Module):\n",
    "    \"\"\"PyTorch LSTM model for time series forecasting\"\"\"\n",
    "    def __init__(self, input_size, hidden_size1=64, hidden_size2=32, output_size=1):\n",
    "        super(LSTMModel, self).__init__()\n",
    "        self.lstm1 = nn.LSTM(input_size, hidden_size1, batch_first=True, return_sequences=True)\n",
    "        self.dropout1 = nn.Dropout(0.2)\n",
    "        self.lstm2 = nn.LSTM(hidden_size1, hidden_size2, batch_first=True)\n",
    "        self.dropout2 = nn.Dropout(0.2)\n",
    "        self.fc1 = nn.Linear(hidden_size2, 16)\n",
    "        self.relu = nn.ReLU()\n",
    "        self.fc2 = nn.Linear(16, output_size)\n",
    "        \n",
    "    def forward(self, x):\n",
    "        x, _ = self.lstm1(x)\n",
    "        x = self.dropout1(x)\n",
    "        x, _ = self.lstm2(x)\n",
    "        x = self.dropout2(x)\n",
    "        x = self.relu(self.fc1(x[:, -1, :]))  # Take only the last output\n",
    "        x = self.fc2(x)\n",
    "        return x\n",
    "\n",
    "def train_prophet_model(df):\n",
    "    \"\"\"Train Prophet model on dataframe with columns 'week_start' and 'num_papers'\"\"\"\n",
    "    model_data = df[['week_start', 'num_papers']].rename(columns={'week_start': 'ds', 'num_papers': 'y'})\n",
    "    model = Prophet(yearly_seasonality=True, weekly_seasonality=True)\n",
    "    model.fit(model_data)\n",
    "    return model\n",
    "\n",
    "print('Starting enhanced model training...\\n')\n",
    "\n",
    "# Define expanded model dictionary\n",
    "test_models = {\n",
    "    'linear': Pipeline([('scaler', StandardScaler()), ('poly', PolynomialFeatures(1, include_bias=False)), ('lr', LinearRegression())]),\n",
    "    'ridge1': Pipeline([('scaler', StandardScaler()), ('poly', PolynomialFeatures(1, include_bias=False)), ('ridge', Ridge(alpha=1.0))]),\n",
    "    'poly2_lr': Pipeline([('scaler', StandardScaler()), ('poly', PolynomialFeatures(2, include_bias=False)), ('lr', LinearRegression())]),\n",
    "    'ridge2': Pipeline([('scaler', StandardScaler()), ('poly', PolynomialFeatures(2, include_bias=False)), ('ridge', Ridge(alpha=1.0))]),\n",
    "    'xgboost': xgb.XGBRegressor(\n",
    "        n_estimators=100,\n",
    "        learning_rate=0.05,\n",
    "        max_depth=4,\n",
    "        subsample=0.8,\n",
    "        colsample_bytree=0.8,\n",
    "        objective='reg:squarederror',\n",
    "        random_state=42\n",
    "    ),\n",
    "    'lightgbm': lgb.LGBMRegressor(\n",
    "        n_estimators=100,\n",
    "        learning_rate=0.05,\n",
    "        num_leaves=31,\n",
    "        subsample=0.8,\n",
    "        colsample_bytree=0.8,\n",
    "        random_state=42\n",
    "    ),\n",
    "    'catboost': cb.CatBoostRegressor(\n",
    "        iterations=100,\n",
    "        learning_rate=0.05,\n",
    "        depth=6,\n",
    "        loss_function='RMSE',\n",
    "        random_seed=42,\n",
    "        verbose=0\n",
    "    ),\n",
    "    'random_forest': RandomForestRegressor(\n",
    "        n_estimators=100,\n",
    "        max_depth=8,\n",
    "        min_samples_split=5,\n",
    "        random_state=42\n",
    "    )\n",
    "}\n",
    "\n",
    "min_scale = 10\n",
    "val_weeks = 8\n",
    "results = []\n",
    "preds = []\n",
    "all_smape = []\n",
    "best_models = {}\n",
    "\n",
    "for cat, group in weekly.groupby('category'):\n",
    "    print(f\"\\nProcessing category: {cat}\")\n",
    "    df_feat = create_time_features(group)\n",
    "    X = df_feat.drop(['category', 'week_start', 'num_papers'], axis=1)\n",
    "    y = df_feat['num_papers']\n",
    "    \n",
    "    # Special handling for short series\n",
    "    if len(df_feat) <= val_weeks + 4:  # Need at least 4 points after validation for LSTM\n",
    "        print(f\"  Series too short ({len(df_feat)} points), using default model\")\n",
    "        best_model = test_models['poly2_lr']\n",
    "        best_name = 'poly2_lr'\n",
    "    else:\n",
    "        # Standard validation\n",
    "        df_tr = df_feat.iloc[:-val_weeks]\n",
    "        df_val = df_feat.iloc[-val_weeks:]\n",
    "        X_tr = df_tr.drop(['category', 'week_start', 'num_papers'], axis=1)\n",
    "        y_tr = df_tr['num_papers']\n",
    "        X_val = df_val.drop(['category', 'week_start', 'num_papers'], axis=1)\n",
    "        y_val = df_val['num_papers']\n",
    "        \n",
    "        # Evaluate standard models\n",
    "        smape_scores = {}\n",
    "        for name, model in test_models.items():\n",
    "            print(f\"  Training {name}...\")\n",
    "            m = model\n",
    "            m.fit(X_tr, y_tr)\n",
    "            y_pred = m.predict(X_val)\n",
    "            smape = np.mean(np.abs(y_pred - y_val) / np.maximum(np.abs(y_val), min_scale))\n",
    "            smape_scores[name] = smape\n",
    "            results.append({'category': cat, 'model': name, 'smape': smape})\n",
    "            \n",
    "        # Try LSTM if we have enough data\n",
    "        if len(df_tr) >= 8:  # Need at least 8 points for a meaningful LSTM\n",
    "            print(f\"  Training LSTM with PyTorch...\")\n",
    "            try:\n",
    "                # Set device (CPU or GPU)\n",
    "                device = torch.device('cuda' if torch.cuda.is_available() else 'cpu')\n",
    "                \n",
    "                # Prepare LSTM data\n",
    "                X_lstm_cols = X_tr.columns\n",
    "                scaler_X = StandardScaler()\n",
    "                scaler_y = StandardScaler()\n",
    "                \n",
    "                X_tr_scaled = pd.DataFrame(scaler_X.fit_transform(X_tr), columns=X_lstm_cols)\n",
    "                y_tr_scaled = scaler_y.fit_transform(y_tr.values.reshape(-1, 1)).flatten()\n",
    "                \n",
    "                X_val_scaled = pd.DataFrame(scaler_X.transform(X_val), columns=X_lstm_cols)\n",
    "                \n",
    "                # Create sequences for LSTM\n",
    "                timesteps = min(4, len(X_tr) // 2)  # Adjust timesteps based on data size\n",
    "                X_lstm_tr, y_lstm_tr = create_lstm_dataset(X_tr_scaled, y_tr_scaled, timesteps)\n",
    "                \n",
    "                if len(X_lstm_tr) > 0:\n",
    "                    # Convert to PyTorch tensors\n",
    "                    X_tensor = torch.FloatTensor(X_lstm_tr).to(device)\n",
    "                    y_tensor = torch.FloatTensor(y_lstm_tr).reshape(-1, 1).to(device)\n",
    "                    \n",
    "                    # Create DataLoader\n",
    "                    batch_size = min(8, len(X_lstm_tr))\n",
    "                    dataset = TensorDataset(X_tensor, y_tensor)\n",
    "                    dataloader = DataLoader(dataset, batch_size=batch_size, shuffle=True)\n",
    "                    \n",
    "                    # Build the model\n",
    "                    input_size = X_lstm_tr.shape[2]  # Number of features\n",
    "                    lstm_model = LSTMModel(input_size=input_size).to(device)\n",
    "                    \n",
    "                    # Define loss function and optimizer\n",
    "                    criterion = nn.MSELoss()\n",
    "                    optimizer = optim.Adam(lstm_model.parameters(), lr=0.001)\n",
    "                    \n",
    "                    # Training loop\n",
    "                    num_epochs = 50\n",
    "                    best_loss = float('inf')\n",
    "                    best_model_state = None\n",
    "                    patience = 5\n",
    "                    patience_counter = 0\n",
    "                    \n",
    "                    for epoch in range(num_epochs):\n",
    "                        lstm_model.train()\n",
    "                        epoch_loss = 0\n",
    "                        \n",
    "                        for batch_X, batch_y in dataloader:\n",
    "                            optimizer.zero_grad()\n",
    "                            outputs = lstm_model(batch_X)\n",
    "                            loss = criterion(outputs, batch_y)\n",
    "                            loss.backward()\n",
    "                            optimizer.step()\n",
    "                            epoch_loss += loss.item()\n",
    "                        \n",
    "                        avg_loss = epoch_loss / len(dataloader)\n",
    "                        \n",
    "                        # Early stopping\n",
    "                        if avg_loss < best_loss:\n",
    "                            best_loss = avg_loss\n",
    "                            best_model_state = lstm_model.state_dict().copy()\n",
    "                            patience_counter = 0\n",
    "                        else:\n",
    "                            patience_counter += 1\n",
    "                            if patience_counter >= patience:\n",
    "                                break\n",
    "                    \n",
    "                    # Load the best model\n",
    "                    if best_model_state:\n",
    "                        lstm_model.load_state_dict(best_model_state)\n",
    "                    \n",
    "                    # Make predictions\n",
    "                    lstm_model.eval()\n",
    "                    lstm_preds = []\n",
    "                    last_sequence = torch.FloatTensor(X_tr_scaled.values[-timesteps:].reshape(1, timesteps, -1)).to(device)\n",
    "                    \n",
    "                    with torch.no_grad():\n",
    "                        for _ in range(len(X_val)):\n",
    "                            pred = lstm_model(last_sequence).item()\n",
    "                            lstm_preds.append(pred)\n",
    "                            \n",
    "                            # Update sequence for next prediction\n",
    "                            next_input = torch.FloatTensor(X_val_scaled.iloc[len(lstm_preds)-1:len(lstm_preds)].values).to(device)\n",
    "                            last_sequence = torch.cat((last_sequence[:, 1:, :], next_input.view(1, 1, -1)), dim=1)\n",
    "                    \n",
    "                    # Inverse transform predictions\n",
    "                    lstm_preds = scaler_y.inverse_transform(np.array(lstm_preds).reshape(-1, 1)).flatten()\n",
    "                    \n",
    "                    # Calculate SMAPE\n",
    "                    lstm_smape = np.mean(np.abs(lstm_preds - y_val) / np.maximum(np.abs(y_val), min_scale))\n",
    "                    smape_scores['lstm'] = lstm_smape\n",
    "                    results.append({'category': cat, 'model': 'lstm', 'smape': lstm_smape})\n",
    "            except Exception as e:\n",
    "                print(f\"  LSTM error: {str(e)}\")\n",
    "                \n",
    "        # Try Prophet if we have enough data\n",
    "        if len(df_tr) >= 10:  # Prophet needs a decent amount of data\n",
    "            print(f\"  Training Prophet...\")\n",
    "            try:\n",
    "                prophet_model = train_prophet_model(df_tr[['week_start', 'num_papers']])\n",
    "                \n",
    "                # Make predictions\n",
    "                future = prophet_model.make_future_dataframe(periods=val_weeks, freq='W')\n",
    "                forecast = prophet_model.predict(future)\n",
    "                prophet_preds = forecast['yhat'].iloc[-val_weeks:].values\n",
    "                \n",
    "                # Calculate SMAPE\n",
    "                prophet_smape = np.mean(np.abs(prophet_preds - y_val) / np.maximum(np.abs(y_val), min_scale))\n",
    "                smape_scores['prophet'] = prophet_smape\n",
    "                results.append({'category': cat, 'model': 'prophet', 'smape': prophet_smape})\n",
    "            except Exception as e:\n",
    "                print(f\"  Prophet error: {str(e)}\")\n",
    "                \n",
    "        # Try ARIMA if we have enough data\n",
    "        if len(df_tr) >= 12:  # ARIMA needs a decent amount of data\n",
    "            print(f\"  Training ARIMA...\")\n",
    "            try:\n",
    "                arima_model = ARIMA(df_tr['num_papers'], order=(1, 1, 1))\n",
    "                arima_res = arima_model.fit()\n",
    "                \n",
    "                # Make predictions\n",
    "                arima_preds = arima_res.forecast(steps=val_weeks)\n",
    "                \n",
    "                # Calculate SMAPE\n",
    "                arima_smape = np.mean(np.abs(arima_preds - y_val) / np.maximum(np.abs(y_val), min_scale))\n",
    "                smape_scores['arima'] = arima_smape\n",
    "                results.append({'category': cat, 'model': 'arima', 'smape': arima_smape})\n",
    "            except Exception as e:\n",
    "                print(f\"  ARIMA error: {str(e)}\")\n",
    "                \n",
    "        # Select best model\n",
    "        best_name = min(smape_scores, key=smape_scores.get)\n",
    "        print(f\"  Best model: {best_name} with sMAPE={smape_scores[best_name]:.4f}\")\n",
    "        all_smape.append(smape_scores[best_name])\n",
    "        \n",
    "        # For specialized models, we'll need to store them differently\n",
    "        if best_name == 'lstm':\n",
    "            best_models[cat] = {\n",
    "                'type': 'lstm',\n",
    "                'model': lstm_model,\n",
    "                'scaler_X': scaler_X,\n",
    "                'scaler_y': scaler_y,\n",
    "                'timesteps': timesteps,\n",
    "                'X_cols': X_lstm_cols\n",
    "            }\n",
    "            continue\n",
    "        elif best_name == 'prophet':\n",
    "            best_models[cat] = {\n",
    "                'type': 'prophet',\n",
    "                'model': prophet_model\n",
    "            }\n",
    "            continue\n",
    "        elif best_name == 'arima':\n",
    "            best_models[cat] = {\n",
    "                'type': 'arima',\n",
    "                'model': arima_res\n",
    "            }\n",
    "            continue\n",
    "        else:\n",
    "            best_model = test_models[best_name]\n",
    "            best_models[cat] = {\n",
    "                'type': 'standard',\n",
    "                'model': best_model,\n",
    "                'name': best_name\n",
    "            }\n",
    "    \n",
    "    # For standard models, fit on all data\n",
    "    if cat not in best_models:\n",
    "        best_model = test_models[best_name]\n",
    "        best_model.fit(X, y)\n",
    "        best_models[cat] = {\n",
    "            'type': 'standard',\n",
    "            'model': best_model,\n",
    "            'name': best_name\n",
    "        }\n",
    "\n",
    "# Make predictions on test data\n",
    "for cat, cat_test in test_df.groupby('category'):\n",
    "    print(f\"\\nGenerating predictions for {cat}...\")\n",
    "    cat_test = cat_test.sort_values('week_id').copy()\n",
    "    hist = weekly[weekly['category'] == cat].set_index('week_start')['num_papers'].copy()\n",
    "    test_preds = []\n",
    "    \n",
    "    model_info = best_models.get(cat)\n",
    "    if model_info is None:\n",
    "        print(f\"  No model found for {cat}, using default\")\n",
    "        model_info = {\n",
    "            'type': 'standard',\n",
    "            'model': test_models['ridge2'],\n",
    "            'name': 'ridge2'\n",
    "        }\n",
    "    \n",
    "    model_type = model_info['type']\n",
    "    \n",
    "    # Different prediction process depending on model type\n",
    "    if model_type == 'standard':\n",
    "        print(f\"  Using standard model: {model_info.get('name', 'unknown')}\")\n",
    "        for _, row in cat_test.iterrows():\n",
    "            wk = row['week_start']\n",
    "            hist = pd.concat([hist, pd.Series({wk: np.nan})])\n",
    "            temp = pd.DataFrame({\n",
    "                'category': cat,\n",
    "                'week_start': hist.index,\n",
    "                'num_papers': hist.values\n",
    "            })\n",
    "            feat = create_time_features(temp)\n",
    "            X_test = feat.drop(['category', 'week_start', 'num_papers'], axis=1).iloc[[-1]]\n",
    "            y_hat = model_info['model'].predict(X_test)[0]\n",
    "            y_hat = max(0, y_hat)\n",
    "            test_preds.append(y_hat)\n",
    "            hist.iloc[-1] = y_hat\n",
    "            \n",
    "    elif model_type == 'lstm':\n",
    "        print(f\"  Using PyTorch LSTM model\")\n",
    "        # For LSTM we need a different approach since it uses sequences\n",
    "        lstm_model = model_info['model']\n",
    "        scaler_X = model_info['scaler_X']\n",
    "        scaler_y = model_info['scaler_y']\n",
    "        timesteps = model_info['timesteps']\n",
    "        X_cols = model_info['X_cols']\n",
    "        device = torch.device('cuda' if torch.cuda.is_available() else 'cpu')\n",
    "        \n",
    "        # Get the historical data with features\n",
    "        hist_df = weekly[weekly['category'] == cat].copy()\n",
    "        hist_feat = create_time_features(hist_df)\n",
    "        X_hist = hist_feat.drop(['category', 'week_start', 'num_papers'], axis=1)\n",
    "        \n",
    "        # Scale the data\n",
    "        X_hist_scaled = pd.DataFrame(scaler_X.transform(X_hist), columns=X_cols)\n",
    "        \n",
    "        # Create initial sequence as tensor\n",
    "        last_sequence = torch.FloatTensor(X_hist_scaled.values[-timesteps:].reshape(1, timesteps, -1)).to(device)\n",
    "        \n",
    "        # Put model in evaluation mode\n",
    "        lstm_model.eval()\n",
    "        \n",
    "        for _, row in cat_test.iterrows():\n",
    "            wk = row['week_start']\n",
    "            \n",
    "            # Predict next value\n",
    "            with torch.no_grad():\n",
    "                pred_scaled = lstm_model(last_sequence).item()\n",
    "            \n",
    "            y_hat = scaler_y.inverse_transform([[pred_scaled]])[0][0]\n",
    "            y_hat = max(0, y_hat)\n",
    "            test_preds.append(y_hat)\n",
    "            \n",
    "            # Update history\n",
    "            hist = pd.concat([hist, pd.Series({wk: y_hat})])\n",
    "            \n",
    "            # Create new features for this point\n",
    "            temp = pd.DataFrame({\n",
    "                'category': cat,\n",
    "                'week_start': hist.index,\n",
    "                'num_papers': hist.values\n",
    "            })\n",
    "            new_feat = create_time_features(temp)\n",
    "            new_X = new_feat.drop(['category', 'week_start', 'num_papers'], axis=1).iloc[[-1]]\n",
    "            \n",
    "            # Scale new features\n",
    "            new_X_scaled = scaler_X.transform(new_X)\n",
    "            \n",
    "            # Update sequence for next prediction as tensor\n",
    "            next_input = torch.FloatTensor(new_X_scaled).view(1, 1, -1).to(device)\n",
    "            last_sequence = torch.cat((last_sequence[:, 1:, :], next_input), dim=1)\n",
    "            \n",
    "    elif model_type == 'prophet':\n",
    "        print(f\"  Using Prophet model\")\n",
    "        prophet_model = model_info['model']\n",
    "        \n",
    "        # Create future dataframe with test weeks\n",
    "        future_dates = pd.concat([\n",
    "            pd.DataFrame({'ds': weekly[weekly['category'] == cat]['week_start']}),\n",
    "            pd.DataFrame({'ds': cat_test['week_start']})\n",
    "        ]).drop_duplicates().sort_values('ds')\n",
    "        \n",
    "        forecast = prophet_model.predict(future_dates)\n",
    "        \n",
    "        # Extract predictions for test weeks\n",
    "        for _, row in cat_test.iterrows():\n",
    "            wk = row['week_start']\n",
    "            y_hat = forecast[forecast['ds'] == wk]['yhat'].values[0]\n",
    "            y_hat = max(0, y_hat)\n",
    "            test_preds.append(y_hat)\n",
    "            hist = pd.concat([hist, pd.Series({wk: y_hat})])\n",
    "            \n",
    "    elif model_type == 'arima':\n",
    "        print(f\"  Using ARIMA model\")\n",
    "        arima_res = model_info['model']\n",
    "        \n",
    "        # For ARIMA, we need to forecast one step at a time and update\n",
    "        for _, row in cat_test.iterrows():\n",
    "            wk = row['week_start']\n",
    "            y_hat = arima_res.forecast(steps=1)[0]\n",
    "            y_hat = max(0, y_hat)\n",
    "            test_preds.append(y_hat)\n",
    "            \n",
    "            # Update history and refit model\n",
    "            hist = pd.concat([hist, pd.Series({wk: y_hat})])\n",
    "            try:\n",
    "                # Update ARIMA model with new observation\n",
    "                arima_model = ARIMA(hist, order=(1, 1, 1))\n",
    "                arima_res = arima_model.fit()\n",
    "            except:\n",
    "                # If refitting fails, keep using the last model\n",
    "                pass\n",
    "    \n",
    "    cat_test['num_papers'] = test_preds\n",
    "    preds.append(cat_test[['category', 'week_id', 'num_papers']])\n",
    "\n",
    "# Combine predictions and create submission\n",
    "if all_smape:\n",
    "    print(f\"\\nOverall average sMAPE: {np.mean(all_smape):.4f}\")\n",
    "\n",
    "pred_df = pd.concat(preds)\n",
    "pred_df['id'] = pred_df['category'] + '__' + pred_df['week_id'].astype(str)\n",
    "submission = (\n",
    "    submission_df[['id']]\n",
    "    .merge(pred_df[['id', 'num_papers']], on='id', how='left')\n",
    ")\n",
    "submission['num_papers'] = submission['num_papers'].fillna(0).round().astype(int)\n",
    "submission.to_csv('submission.csv', index=False)\n",
    "print(\"\\nSubmission file created: submission.csv\")\n",
    "\n",
    "# Analysis of model performance\n",
    "model_results = pd.DataFrame(results)\n",
    "if not model_results.empty:\n",
    "    print(\"\\nModel performance summary:\")\n",
    "    model_summary = model_results.groupby('model')['smape'].agg(['mean', 'std', 'count']).sort_values('mean')\n",
    "    print(model_summary)\n",
    "\n",
    "    print(\"\\nTop model by category:\")\n",
    "    top_models = model_results.loc[model_results.groupby('category')['smape'].idxmin()]\n",
    "    print(top_models[['category', 'model', 'smape']])"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.13.2"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
