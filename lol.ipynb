{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "c202d9b3-efeb-4794-bce5-720b1f6335b1",
   "metadata": {},
   "source": [
    "–û—Ç–ª–∏—á–Ω–æ! –ù–∏–∂–µ ‚Äî **–±—ã—Å—Ç—Ä—ã–π —É–Ω–∏–≤–µ—Ä—Å–∞–ª—å–Ω—ã–π toolkit –¥–ª—è LLM-–≥–µ–Ω–µ—Ä–∞—Ü–∏–∏**: –æ–¥–∏–Ω –º–æ–¥—É–ª—å —Å\n",
    "\n",
    "* –∑–∞–≥—Ä—É–∑–∫–æ–π –ª–æ–∫–∞–ª—å–Ω—ã—Ö HF-–º–æ–¥–µ–ª–µ–π (offline-friendly),\n",
    "* 8-–±–∏—Ç/4-–±–∏—Ç –∑–∞–≥—Ä—É–∑–∫–æ–π (–µ—Å–ª–∏ –µ—Å—Ç—å bitsandbytes),\n",
    "* —à–∞–±–ª–æ–Ω–∞–º–∏ –ø—Ä–æ–º–ø—Ç–æ–≤ –ø–æ–¥ ChatML / Llama-2 / Mistral / Qwen / Gemma,\n",
    "* –µ–¥–∏–Ω—ã–º –º–µ—Ç–æ–¥–æ–º `generate()` –∏ –±–∞—Ç—á-–≤–µ—Ä—Å–∏–µ–π,\n",
    "* —Å—Ç–æ–ø-–ø–æ—Å–ª–µ–¥–æ–≤–∞—Ç–µ–ª—å–Ω–æ—Å—Ç—è–º–∏, —Ç–µ–º–ø–µ—Ä–∞—Ç—É—Ä–æ–π, top-p/top-k, –ø–æ–≤—Ç–æ—Ä–µ–Ω–∏—è–º–∏,\n",
    "* —Å—Ç—Ä–∏–º–∏–Ω–≥–æ–≤–æ–π –≥–µ–Ω–µ—Ä–∞—Ü–∏–µ–π (–∏—Ç–µ—Ä–∞—Ç–æ—Ä),\n",
    "* –ø—Ä–æ—Å—Ç—ã–º few-shot, –∞ —Ç–∞–∫–∂–µ JSON-–æ—Ä–∏–µ–Ω—Ç–∏—Ä–æ–≤–∞–Ω–Ω–æ–π –∑–∞–≥–æ—Ç–æ–≤–∫–æ–π,\n",
    "* –º–∏–Ω–∏-–ø—Ä–∏–º–µ—Ä–æ–º –∏—Å–ø–æ–ª—å–∑–æ–≤–∞–Ω–∏—è.\n",
    "\n",
    "–§–æ—Ä–º–∞—Ç –∫–∞–∫ —Ç—ã –ø—Ä–æ—Å–∏–ª: **–∫–æ–¥ + –∫–æ—Ä–æ—Ç–∫–∏–µ HTML-–≤—Å—Ç–∞–≤–∫–∏ —Å –ø–æ—è—Å–Ω–µ–Ω–∏—è–º–∏**. –•–æ—á–µ—à—å ‚Äî —É–¥–∞–ª–∏ HTML, –æ—Å—Ç–∞–Ω–µ—Ç—Å—è —á–∏—Å—Ç—ã–π Python.\n",
    "\n",
    "---\n",
    "\n",
    "## üöÄ –£–Ω–∏–≤–µ—Ä—Å–∞–ª—å–Ω—ã–π –º–æ–¥—É–ª—å LLM (–æ—Ñ–ª–∞–π–Ω, –∫–ª–∞—Å—Ç–µ—Ä-–¥—Ä—É–∂–µ–ª—é–±–Ω—ã–π)\n",
    "\n",
    "````python\n",
    "# ==============================\n",
    "# LLM UNIVERSAL GENERATION TOOL\n",
    "# ==============================\n",
    "\n",
    "import os, sys, math, time, gc, warnings, pathlib, random\n",
    "from typing import List, Dict, Any, Optional, Iterable, Generator, Tuple, Union\n",
    "\n",
    "import torch\n",
    "\n",
    "warnings.filterwarnings(\"ignore\")\n",
    "\n",
    "\n",
    "# ---------- –£—Ç–∏–ª–∏—Ç—ã: —É—Å—Ç—Ä–æ–π—Å—Ç–≤–æ –∏ seed ----------\n",
    "def get_device() -> torch.device:\n",
    "    if torch.cuda.is_available():\n",
    "        return torch.device(\"cuda\")\n",
    "    if getattr(torch.backends, \"mps\", None) and torch.backends.mps.is_available():\n",
    "        return torch.device(\"mps\")  # Apple Silicon\n",
    "    return torch.device(\"cpu\")\n",
    "\n",
    "DEVICE = get_device()\n",
    "\n",
    "def seed_everything(seed: int = 42):\n",
    "    random.seed(seed)\n",
    "    os.environ[\"PYTHONHASHSEED\"] = str(seed)\n",
    "    torch.manual_seed(seed)\n",
    "    if torch.cuda.is_available():\n",
    "        torch.cuda.manual_seed_all(seed)\n",
    "        torch.backends.cudnn.deterministic = True\n",
    "        torch.backends.cudnn.benchmark = False\n",
    "\n",
    "seed_everything(42)\n",
    "\n",
    "\n",
    "# ---------- –ü—Ä–æ–º–ø—Ç-—à–∞–±–ª–æ–Ω—ã –ø–æ–¥ —Ä–∞–∑–Ω—ã–µ —á–∞—Ç—ã ----------\n",
    "class ChatPrompt:\n",
    "    \"\"\"\n",
    "    –£–Ω–∏–≤–µ—Ä—Å–∞–ª—å–Ω—ã–π —Å–±–æ—Ä—â–∏–∫ –ø—Ä–æ–º–ø—Ç–æ–≤.\n",
    "    –ü–æ–¥–¥–µ—Ä–∂–∏–≤–∞–µ—Ç –Ω–µ—Å–∫–æ–ª—å–∫–æ —Å—Ç–∏–ª–µ–π: 'chatml', 'llama2', 'mistral', 'qwen', 'gemma', 'raw'\n",
    "    \"\"\"\n",
    "\n",
    "    @staticmethod\n",
    "    def chatml(system: str, user: str, shots: List[Tuple[str, str]] = None) -> str:\n",
    "        # OpenAI/ChatML-—Å—Ç–∏–ª—å\n",
    "        parts = []\n",
    "        if system:\n",
    "            parts += [\"<|im_start|>system\\n\" + system + \"\\n<|im_end|>\"]\n",
    "        if shots:\n",
    "            for s_u, s_a in shots:\n",
    "                parts += [\"<|im_start|>user\\n\" + s_u + \"\\n<|im_end|>\",\n",
    "                          \"<|im_start|>assistant\\n\" + s_a + \"\\n<|im_end|>\"]\n",
    "        parts += [\"<|im_start|>user\\n\" + user + \"\\n<|im_end|>\",\n",
    "                  \"<|im_start|>assistant\\n\"]\n",
    "        return \"\\n\".join(parts)\n",
    "\n",
    "    @staticmethod\n",
    "    def llama2(system: str, user: str, shots: List[Tuple[str, str]] = None) -> str:\n",
    "        sys_block = f\"<<SYS>>\\n{system}\\n<</SYS>>\\n\\n\" if system else \"\"\n",
    "        shot_text = \"\"\n",
    "        if shots:\n",
    "            for s_u, s_a in shots:\n",
    "                shot_text += f\"[INST] {sys_block}{s_u} [/INST] {s_a}\\n\"\n",
    "                sys_block = \"\"  # —Å–∏—Å—Ç–µ–º–∫–∞ —Ç–æ–ª—å–∫–æ –≤ –ø–µ—Ä–≤–æ–º –±–ª–æ–∫–µ\n",
    "        return f\"{shot_text}[INST] {sys_block}{user} [/INST]\"\n",
    "\n",
    "    @staticmethod\n",
    "    def mistral(system: str, user: str, shots: List[Tuple[str, str]] = None) -> str:\n",
    "        # —Ç–æ–∫–µ–Ω—ã <s>[INST] ... [/INST] –æ—Ç–≤–µ—Ç\n",
    "        sys_block = f\"{system}\\n\\n\" if system else \"\"\n",
    "        shot_text = \"\"\n",
    "        if shots:\n",
    "            for s_u, s_a in shots:\n",
    "                shot_text += f\"<s>[INST] {sys_block}{s_u} [/INST] {s_a}</s>\\n\"\n",
    "                sys_block = \"\"\n",
    "        return f\"{shot_text}<s>[INST] {sys_block}{user} [/INST]\"\n",
    "\n",
    "    @staticmethod\n",
    "    def qwen(system: str, user: str, shots: List[Tuple[str, str]] = None) -> str:\n",
    "        # –ø—Ä–æ—Å—Ç–æ ChatML-–ø–æ–¥–æ–±–Ω—ã–π —Å—Ç–∏–ª—å —Ä–∞–±–æ—Ç–∞–µ—Ç –æ–∫ –¥–ª—è Qwen-Instruct\n",
    "        return ChatPrompt.chatml(system, user, shots)\n",
    "\n",
    "    @staticmethod\n",
    "    def gemma(system: str, user: str, shots: List[Tuple[str, str]] = None) -> str:\n",
    "        # –ø—Ä–æ—Å—Ç–æ–π instruct –ø–æ–¥–æ–π–¥—ë—Ç\n",
    "        prompt = \"\"\n",
    "        if system:\n",
    "            prompt += f\"System: {system}\\n\\n\"\n",
    "        if shots:\n",
    "            for s_u, s_a in shots:\n",
    "                prompt += f\"User: {s_u}\\nAssistant: {s_a}\\n\"\n",
    "        prompt += f\"User: {user}\\nAssistant:\"\n",
    "        return prompt\n",
    "\n",
    "    @staticmethod\n",
    "    def raw(system: str, user: str, shots: List[Tuple[str, str]] = None) -> str:\n",
    "        # –ü—Ä—è–º–æ–π –∫–æ–Ω–∫–∞—Ç –±–µ–∑ —Å–ø–µ—Ü–∏–∞–ª—å–Ω—ã—Ö —Ç–æ–∫–µ–Ω–æ–≤ (—É–Ω–∏–≤–µ—Ä—Å–∞–ª—å–Ω—ã–π fallback)\n",
    "        parts = []\n",
    "        if system:\n",
    "            parts += [f\"[SYSTEM]\\n{system}\\n\"]\n",
    "        if shots:\n",
    "            for s_u, s_a in shots:\n",
    "                parts += [f\"[USER]\\n{s_u}\\n[ASSISTANT]\\n{s_a}\\n\"]\n",
    "        parts += [f\"[USER]\\n{user}\\n[ASSISTANT]\\n\"]\n",
    "        return \"\\n\".join(parts)\n",
    "\n",
    "    @staticmethod\n",
    "    def build(style: str, system: str, user: str, shots: Optional[List[Tuple[str, str]]] = None) -> str:\n",
    "        style = (style or \"raw\").lower()\n",
    "        if   style == \"chatml\":  return ChatPrompt.chatml(system, user, shots)\n",
    "        elif style == \"llama2\":  return ChatPrompt.llama2(system, user, shots)\n",
    "        elif style == \"mistral\": return ChatPrompt.mistral(system, user, shots)\n",
    "        elif style == \"qwen\":    return ChatPrompt.qwen(system, user, shots)\n",
    "        elif style == \"gemma\":   return ChatPrompt.gemma(system, user, shots)\n",
    "        else:                    return ChatPrompt.raw(system, user, shots)\n",
    "\n",
    "\n",
    "# ---------- –ó–∞–≥—Ä—É–∑–∫–∞ –º–æ–¥–µ–ª–∏ —á–µ—Ä–µ–∑ transformers (—Å 8/4-–±–∏—Ç, –µ—Å–ª–∏ –¥–æ—Å—Ç—É–ø–Ω–æ) ----------\n",
    "class LLM:\n",
    "    \"\"\"\n",
    "    –£–Ω–∏–≤–µ—Ä—Å–∞–ª—å–Ω–∞—è –æ–±—ë—Ä—Ç–∫–∞ –Ω–∞–¥ HuggingFace transformers –¥–ª—è –ª–æ–∫–∞–ª—å–Ω–æ–π –≥–µ–Ω–µ—Ä–∞—Ü–∏–∏.\n",
    "    –ü–æ–¥–¥–µ—Ä–∂–∫–∞: fp16/bf16, 8-–±–∏—Ç/4-–±–∏—Ç (bitsandbytes), device_map=\"auto\", stop-seq, —Å—Ç—Ä–∏–º–∏–Ω–≥.\n",
    "    \"\"\"\n",
    "\n",
    "    def __init__(\n",
    "        self,\n",
    "        model_path: str,\n",
    "        dtype: str = \"auto\",            # \"auto\" | \"fp16\" | \"bf16\" | \"fp32\"\n",
    "        load_in_8bit: bool = False,\n",
    "        load_in_4bit: bool = False,\n",
    "        device_map: str = \"auto\",\n",
    "        trust_remote_code: bool = True,\n",
    "        use_fast_tokenizer: bool = True,\n",
    "        attn_implementation: Optional[str] = None,  # \"flash_attention_2\" –µ—Å–ª–∏ —Å–æ–±—Ä–∞–Ω\n",
    "    ):\n",
    "        from transformers import AutoModelForCausalLM, AutoTokenizer\n",
    "\n",
    "        self.model_path = model_path\n",
    "        self.tokenizer = AutoTokenizer.from_pretrained(\n",
    "            model_path,\n",
    "            use_fast=use_fast_tokenizer,\n",
    "            trust_remote_code=trust_remote_code\n",
    "        )\n",
    "\n",
    "        # –≤—ã–±–æ—Ä dtype\n",
    "        torch_dtype = None\n",
    "        if dtype == \"fp16\": torch_dtype = torch.float16\n",
    "        elif dtype == \"bf16\": torch_dtype = torch.bfloat16\n",
    "        elif dtype == \"fp32\": torch_dtype = torch.float32\n",
    "        else: torch_dtype = None  # auto\n",
    "\n",
    "        quant_kwargs = {}\n",
    "        if load_in_8bit or load_in_4bit:\n",
    "            try:\n",
    "                import bitsandbytes as bnb  # noqa\n",
    "                if load_in_8bit: quant_kwargs[\"load_in_8bit\"] = True\n",
    "                if load_in_4bit:\n",
    "                    quant_kwargs[\"load_in_4bit\"] = True\n",
    "                    quant_kwargs[\"bnb_4bit_use_double_quant\"] = True\n",
    "                    quant_kwargs[\"bnb_4bit_compute_dtype\"] = torch.bfloat16\n",
    "            except Exception as e:\n",
    "                print(\"[warn] bitsandbytes –Ω–µ–¥–æ—Å—Ç—É–ø–µ–Ω, –∑–∞–≥—Ä—É–∂–∞—é –±–µ–∑ –∫–≤–∞–Ω—Ç–æ–≤–∞–Ω–∏—è.\")\n",
    "\n",
    "        extra = {}\n",
    "        if attn_implementation:\n",
    "            extra[\"attn_implementation\"] = attn_implementation\n",
    "\n",
    "        self.model = AutoModelForCausalLM.from_pretrained(\n",
    "            model_path,\n",
    "            torch_dtype=torch_dtype,\n",
    "            device_map=device_map,\n",
    "            trust_remote_code=trust_remote_code,\n",
    "            **quant_kwargs,\n",
    "            **extra,\n",
    "        )\n",
    "        self.model.eval()\n",
    "\n",
    "        # —Ç–æ–∫–µ–Ω–∏–∑–∞—Ç–æ—Ä: –ø–∞–¥–¥–∏–Ω–≥/—Ç—Ä–∏–º–º–∏–Ω–≥\n",
    "        if self.tokenizer.pad_token is None:\n",
    "            # –±–µ–∑–æ–ø–∞—Å–Ω—ã–π –ø–∞–¥-—Ç–æ–∫–µ–Ω\n",
    "            self.tokenizer.pad_token = self.tokenizer.eos_token or \"<|endoftext|>\"\n",
    "        self.tokenizer.padding_side = \"left\"  # —É–¥–æ–±–Ω–æ –¥–ª—è –≥–µ–Ω–µ—Ä–∞—Ü–∏–∏ —Ä–∞–∑–Ω–æ–π –¥–ª–∏–Ω—ã (batch)\n",
    "\n",
    "    # -------- –µ–¥–∏–Ω–∏—á–Ω–∞—è –≥–µ–Ω–µ—Ä–∞—Ü–∏—è --------\n",
    "    @torch.no_grad()\n",
    "    def generate(\n",
    "        self,\n",
    "        prompt: str,\n",
    "        max_new_tokens: int = 256,\n",
    "        temperature: float = 0.7,\n",
    "        top_p: float = 0.9,\n",
    "        top_k: int = 0,\n",
    "        repetition_penalty: float = 1.05,\n",
    "        no_repeat_ngram_size: int = 0,\n",
    "        do_sample: bool = True,\n",
    "        stop: Optional[List[str]] = None,\n",
    "        stream: bool = False,\n",
    "        **gen_kwargs,\n",
    "    ) -> Union[str, Generator[str, None, str]]:\n",
    "        \"\"\"\n",
    "        –í–æ–∑–≤—Ä–∞—â–∞–µ—Ç —Å—Ç—Ä–æ–∫—É (–∏–ª–∏ –≥–µ–Ω–µ—Ä–∞—Ç–æ—Ä —Å—Ç—Ä–æ–∫ –ø—Ä–∏ stream=True).\n",
    "        –ü–∞—Ä–∞–º–µ—Ç—Ä—ã —Å–æ–≤–º–µ—Å—Ç–∏–º—ã —Å transformers.generate().\n",
    "        \"\"\"\n",
    "        # —Ç–æ–∫–µ–Ω–∏–∑–∞—Ü–∏—è\n",
    "        inputs = self.tokenizer([prompt], return_tensors=\"pt\").to(self.model.device)\n",
    "\n",
    "        gen_config = dict(\n",
    "            max_new_tokens=max_new_tokens,\n",
    "            temperature=temperature,\n",
    "            top_p=top_p,\n",
    "            do_sample=do_sample,\n",
    "            repetition_penalty=repetition_penalty,\n",
    "            no_repeat_ngram_size=no_repeat_ngram_size,\n",
    "            pad_token_id=self.tokenizer.pad_token_id,\n",
    "            eos_token_id=self.tokenizer.eos_token_id,\n",
    "        )\n",
    "        if top_k > 0:\n",
    "            gen_config[\"top_k\"] = top_k\n",
    "        gen_config.update(gen_kwargs)\n",
    "\n",
    "        if not stream:\n",
    "            output_ids = self.model.generate(**inputs, **gen_config)\n",
    "            text = self.tokenizer.decode(output_ids[0], skip_special_tokens=True)\n",
    "            # –≤—ã—Ä–µ–∂–µ–º –ø—Ä–µ—Ñ–∏–∫—Å (prompt) –µ—Å–ª–∏ –Ω—É–∂–Ω–æ\n",
    "            if text.startswith(prompt):\n",
    "                text = text[len(prompt):]\n",
    "            text = self._apply_stop(text, stop)\n",
    "            return text.strip()\n",
    "\n",
    "        # –ø–æ—Ç–æ–∫–æ–≤—ã–π —Ä–µ–∂–∏–º ‚Äî –∏—Ç–µ—Ä–∞—Ç–∏–≤–Ω–æ —Ä–∞—Å—Ç–∏–º –≤—ã–≤–æ–¥\n",
    "        return self._stream_generate(prompt, inputs, gen_config, stop)\n",
    "\n",
    "    def _apply_stop(self, text: str, stop: Optional[List[str]]) -> str:\n",
    "        if not stop:\n",
    "            return text\n",
    "        cut = len(text)\n",
    "        for s in stop:\n",
    "            if not s:\n",
    "                continue\n",
    "            idx = text.find(s)\n",
    "            if idx != -1:\n",
    "                cut = min(cut, idx)\n",
    "        return text[:cut]\n",
    "\n",
    "    def _stream_generate(\n",
    "        self, prompt: str, inputs, gen_config: Dict[str, Any], stop: Optional[List[str]]\n",
    "    ) -> Generator[str, None, str]:\n",
    "        \"\"\"\n",
    "        –ü—Ä–æ—Å—Ç–æ–π –ø–æ—Ç–æ–∫–æ–≤—ã–π –≤—ã–≤–æ–¥: –ø–æ —Ç–æ–∫–µ–Ω—É/—Ñ—Ä–∞–∑–µ.\n",
    "        \"\"\"\n",
    "        # –ò—Å–ø–æ–ª—å–∑—É–µ–º greedy \"manually\" —Ç–æ–∫–µ–Ω –∑–∞ —Ç–æ–∫–µ–Ω–æ–º (–¥–ª—è —Å—Ç–∞–±–∏–ª—å–Ω–æ—Å—Ç–∏ –≤ –æ—Ñ–ª–∞–π–Ω–µ).\n",
    "        # –î–ª—è –ø—Ä–æ–¥–≤–∏–Ω—É—Ç–æ–≥–æ —Å—Ç—Ä–∏–º–∏–Ω–≥–∞ –º–æ–∂–Ω–æ –ø–æ–¥–∫–ª—é—á–∏—Ç—å TextIteratorStreamer –∏–∑ transformers.\n",
    "        from transformers import LogitsProcessorList\n",
    "\n",
    "        input_ids = inputs[\"input_ids\"]\n",
    "        generated = input_ids.clone()\n",
    "\n",
    "        logits_processors = LogitsProcessorList()\n",
    "        max_new = int(gen_config.get(\"max_new_tokens\", 128))\n",
    "        temperature = float(gen_config.get(\"temperature\", 1.0))\n",
    "        top_p = float(gen_config.get(\"top_p\", 1.0))\n",
    "        top_k = int(gen_config.get(\"top_k\", 0))\n",
    "        repetition_penalty = float(gen_config.get(\"repetition_penalty\", 1.0))\n",
    "        eos_id = gen_config.get(\"eos_token_id\", self.tokenizer.eos_token_id)\n",
    "\n",
    "        decoded = self.tokenizer.decode(input_ids[0], skip_special_tokens=True)\n",
    "        acc = \"\"  # —Ç–æ–ª—å–∫–æ –¥–æ–±–∞–≤–ª—è–µ–º–∞—è —á–∞—Å—Ç—å (–±–µ–∑ –ø—Ä–µ—Ñ–∏–∫—Å–∞)\n",
    "\n",
    "        for _ in range(max_new):\n",
    "            outputs = self.model(input_ids=generated)\n",
    "            logits = outputs.logits[:, -1, :]  # [1, vocab]\n",
    "            # repetition penalty\n",
    "            if repetition_penalty != 1.0:\n",
    "                for token_id in set(generated[0].tolist()):\n",
    "                    logits[:, token_id] /= repetition_penalty\n",
    "            # softmax —Å temperature\n",
    "            if temperature > 0:\n",
    "                logits = logits / temperature\n",
    "\n",
    "            probs = torch.softmax(logits, dim=-1)\n",
    "\n",
    "            # nucleus/top-k\n",
    "            if top_k > 0:\n",
    "                values, indices = torch.topk(probs, top_k)\n",
    "                probs_zero = torch.zeros_like(probs)\n",
    "                probs_zero.scatter_(1, indices, values)\n",
    "                probs = probs_zero\n",
    "            if top_p < 1.0:\n",
    "                sorted_probs, sorted_idx = torch.sort(probs, descending=True)\n",
    "                cumsum = torch.cumsum(sorted_probs, dim=-1)\n",
    "                mask = cumsum - sorted_probs > top_p\n",
    "                sorted_probs[mask] = 0\n",
    "                probs = torch.zeros_like(probs).scatter(1, sorted_idx, sorted_probs)\n",
    "            probs /= probs.sum(dim=-1, keepdim=True)\n",
    "\n",
    "            next_token = torch.multinomial(probs, num_samples=1)  # [1,1]\n",
    "            generated = torch.cat([generated, next_token], dim=1)\n",
    "\n",
    "            if next_token.item() == eos_id:\n",
    "                break\n",
    "\n",
    "            new_text = self.tokenizer.decode(next_token[0], skip_special_tokens=True)\n",
    "            acc += new_text\n",
    "            # –ø—Ä–æ–≤–µ—Ä–∫–∞ —Å—Ç–æ–ø-–ø–æ—Å–ª–µ–¥–æ–≤–∞—Ç–µ–ª—å–Ω–æ—Å—Ç–µ–π\n",
    "            if stop and any(st in acc for st in stop if st):\n",
    "                acc = self._apply_stop(acc, stop)\n",
    "                yield acc\n",
    "                return acc\n",
    "            yield acc\n",
    "\n",
    "        return acc\n",
    "\n",
    "\n",
    "# ---------- –í—Å–ø–æ–º–æ–≥–∞—Ç–µ–ª—å–Ω—ã–µ –≥–µ–Ω–µ—Ä–∞—Ç–æ—Ä—ã –∑–∞–ø—Ä–æ—Å–æ–≤ ----------\n",
    "def format_prompt(\n",
    "    system: str,\n",
    "    user: str,\n",
    "    style: str = \"chatml\",\n",
    "    shots: Optional[List[Tuple[str, str]]] = None\n",
    ") -> str:\n",
    "    \"\"\"\n",
    "    –°–±–æ—Ä–∫–∞ –ø—Ä–æ–º–ø—Ç–∞ –ø–æ–¥ —É–∫–∞–∑–∞–Ω–Ω—ã–π —Å—Ç–∏–ª—å (chatml/llama2/mistral/qwen/gemma/raw).\n",
    "    \"\"\"\n",
    "    return ChatPrompt.build(style=style, system=system, user=user, shots=shots)\n",
    "\n",
    "\n",
    "# ---------- JSON-–æ—Ä–∏–µ–Ω—Ç–∏—Ä–æ–≤–∞–Ω–Ω–∞—è –∑–∞–≥–æ—Ç–æ–≤–∫–∞ ----------\n",
    "def json_guardrail(text: str) -> str:\n",
    "    \"\"\"\n",
    "    –û—á–µ–Ω—å –ø—Ä–æ—Å—Ç–æ–π guardrail: –æ–±—Ä–µ–∑–∞—Ç—å –¥–æ –ø–µ—Ä–≤–æ–≥–æ '```json'...'```' –∏–ª–∏ —Ñ–∏–≥—É—Ä–Ω—ã–µ —Å–∫–æ–±–∫–∏.\n",
    "    –î–ª—è —Å–µ—Ä—å—ë–∑–Ω–æ–π –≤–∞–ª–∏–¥–∞—Ü–∏–∏ ‚Äî jsonschema / pydantic + –∏—Ç–µ—Ä–∞—Ç–∏–≤–Ω—ã–π repair.\n",
    "    \"\"\"\n",
    "    t = text.strip()\n",
    "    start = t.find(\"{\")\n",
    "    end = t.rfind(\"}\")\n",
    "    if start != -1 and end != -1 and end > start:\n",
    "        return t[start:end+1]\n",
    "    return t\n",
    "````\n",
    "\n",
    "```markdown\n",
    "<div>\n",
    "<b>–ß—Ç–æ –≤–Ω—É—Ç—Ä–∏ –º–æ–¥—É–ª—è:</b><br>\n",
    "‚Ä¢ <code>LLM</code> ‚Äî –µ–¥–∏–Ω–∞—è –æ–±—ë—Ä—Ç–∫–∞ –Ω–∞–¥ transformers: –ª–æ–∫–∞–ª—å–Ω–∞—è –∑–∞–≥—Ä—É–∑–∫–∞, –∫–≤–∞–Ω—Ç–æ–≤–∞–Ω–∏–µ 8/4-–±–∏—Ç (–µ—Å–ª–∏ –¥–æ—Å—Ç—É–ø–Ω–æ), fp16/bf16, device_map=\"auto\"<br>\n",
    "‚Ä¢ <code>ChatPrompt</code> ‚Äî —Å–±–æ—Ä—â–∏–∫ –ø—Ä–æ–º–ø—Ç–æ–≤ –ø–æ–¥ –ø–æ–ø—É–ª—è—Ä–Ω—ã–µ —à–∞–±–ª–æ–Ω—ã (ChatML/Llama2/Mistral/Qwen/Gemma)<br>\n",
    "‚Ä¢ <code>generate(...)</code> ‚Äî –µ–¥–∏–Ω–∏—á–Ω—ã–π –≤—ã–∑–æ–≤; <code>stream=True</code> –≤–µ—Ä–Ω—ë—Ç –≥–µ–Ω–µ—Ä–∞—Ç–æ—Ä (–ø–æ—Å—Ç—Ä–æ—á–Ω–∞—è –ø–æ–¥–∞—á–∞ –≤—ã–≤–æ–¥–∞)<br>\n",
    "‚Ä¢ <code>format_prompt(...)</code> ‚Äî —É–¥–æ–±–Ω–∞—è —Å–±–æ—Ä–∫–∞ —Ç–µ–∫—Å—Ç–∞ –∑–∞–ø—Ä–æ—Å–∞<br>\n",
    "‚Ä¢ <code>json_guardrail(...)</code> ‚Äî –ø—Ä–∏–º–∏—Ç–∏–≤–Ω—ã–π ‚Äú—Ä–µ–∑–∞–∫‚Äù –¥–æ –≤–∞–ª–∏–¥–Ω–æ–≥–æ JSON-–±–ª–æ–∫–∞ (–±—É—Å—Ç –¥–ª—è structured output)\n",
    "</div>\n",
    "```\n",
    "\n",
    "---\n",
    "\n",
    "## üß™ –ú–∏–Ω–∏-–ø—Ä–∏–º–µ—Ä: –æ–±—ã—á–Ω–∞—è –≥–µ–Ω–µ—Ä–∞—Ü–∏—è\n",
    "\n",
    "```python\n",
    "if __name__ == \"__main__\":\n",
    "    # –ü—Ä–∏–º–µ—Ä ‚Äî –ø–æ–¥—Å—Ç–∞–≤—å —Å–≤–æ—é –ª–æ–∫–∞–ª—å–Ω—É—é –º–æ–¥–µ–ª—å (–ø–∞–ø–∫–∞/–ø—É—Ç—å):\n",
    "    # –ß–∞—Å—Ç–æ –Ω–∞ –∫–ª–∞—Å—Ç–µ—Ä–∞—Ö –ø—É—Ç–∏ –≤–∏–¥–∞: \"/home/jovyan/models/Qwen2.5-7B-Instruct\"\n",
    "    MODEL_PATH = os.environ.get(\"LLM_PATH\", \"YOUR_LOCAL_INSTRUCT_MODEL\")\n",
    "\n",
    "    llm = LLM(\n",
    "        model_path=MODEL_PATH,\n",
    "        dtype=\"bf16\",          # auto|fp16|bf16|fp32\n",
    "        load_in_4bit=False,    # True –µ—Å–ª–∏ –Ω—É–∂–µ–Ω —ç–∫–æ–Ω–æ–º–Ω—ã–π —Ä–µ–∂–∏–º (bitsandbytes)\n",
    "        device_map=\"auto\",\n",
    "        trust_remote_code=True\n",
    "    )\n",
    "\n",
    "    system = \"–¢—ã ‚Äî –ø–æ–ª–µ–∑–Ω—ã–π, –∫—Ä–∞—Ç–∫–∏–π, —á–µ—Å—Ç–Ω—ã–π –∞—Å—Å–∏—Å—Ç–µ–Ω—Ç.\"\n",
    "    user = \"–û–±—ä—è—Å–Ω–∏ —Ä–∞–∑–Ω–∏—Ü—É –º–µ–∂–¥—É LightGBM –∏ XGBoost –ø—Ä–æ—Å—Ç—ã–º —è–∑—ã–∫–æ–º, –≤ 5 –ø—É–Ω–∫—Ç–∞—Ö.\"\n",
    "    prompt = format_prompt(system=system, user=user, style=\"chatml\")\n",
    "\n",
    "    # 1) –û–±—ã—á–Ω–∞—è –≥–µ–Ω–µ—Ä–∞—Ü–∏—è\n",
    "    out = llm.generate(\n",
    "        prompt,\n",
    "        max_new_tokens=256,\n",
    "        temperature=0.7,\n",
    "        top_p=0.9,\n",
    "        top_k=50,\n",
    "        repetition_penalty=1.05,\n",
    "        stop=[\"<|im_end|>\", \"</s>\"]\n",
    "    )\n",
    "    print(\"=== –û—Ç–≤–µ—Ç (–æ–±—ã—á–Ω—ã–π) ===\")\n",
    "    print(out)\n",
    "\n",
    "    # 2) –°—Ç—Ä–∏–º–∏–Ω–≥ (–ø–æ—Å—Ç–µ–ø–µ–Ω–Ω—ã–π –≤—ã–≤–æ–¥)\n",
    "    print(\"\\n=== –û—Ç–≤–µ—Ç (stream) ===\")\n",
    "    stream_iter = llm.generate(prompt, max_new_tokens=128, temperature=0.7, stream=True)\n",
    "    final = \"\"\n",
    "    for chunk in stream_iter:\n",
    "        print(chunk, end=\"\", flush=True)\n",
    "        final = chunk\n",
    "    print(\"\\n[END STREAM]\")\n",
    "```\n",
    "\n",
    "```markdown\n",
    "<div>\n",
    "<b>–°–æ–≤–µ—Ç—ã:</b><br>\n",
    "‚Ä¢ –ï—Å–ª–∏ –æ—Ç–≤–µ—Ç ‚Äú–∂—É–µ—Ç‚Äù –≤–≤–æ–¥ (–ø–æ–≤—Ç–æ—Ä—è–µ—Ç prompt), –ø—Ä–æ—Å—Ç–æ –≤—ã—Ä–µ–∑–∞–π –ø—Ä–µ—Ñ–∏–∫—Å (–≤ –∫–æ–¥–µ —É–∂–µ –ø—Ä–µ–¥—É—Å–º–æ—Ç—Ä–µ–Ω–æ).<br>\n",
    "‚Ä¢ –î–ª—è –¥–µ—Ç–µ—Ä–º–∏–Ω–∏–∑–º–∞: <code>temperature=0, top_p=1, do_sample=False</code>.<br>\n",
    "‚Ä¢ –î–ª—è —Ä–∞–∑–Ω–æ–æ–±—Ä–∞–∑–∏—è: <code>temperature‚âà0.8‚Äì1.0, top_p‚âà0.9‚Äì0.95</code>.\n",
    "</div>\n",
    "```\n",
    "\n",
    "---\n",
    "\n",
    "## üì¶ –ë–∞—Ç—á-–≥–µ–Ω–µ—Ä–∞—Ü–∏—è (—Å–ø–∏—Å–∫–∏ –∑–∞–ø—Ä–æ—Å–æ–≤)\n",
    "\n",
    "```python\n",
    "def batch_generate(\n",
    "    llm: LLM,\n",
    "    prompts: List[str],\n",
    "    max_new_tokens: int = 256,\n",
    "    temperature: float = 0.7,\n",
    "    top_p: float = 0.9,\n",
    "    top_k: int = 50,\n",
    "    repetition_penalty: float = 1.05,\n",
    "    stop: Optional[List[str]] = None,\n",
    "    **gen_kwargs\n",
    ") -> List[str]:\n",
    "    \"\"\"\n",
    "    –ü—Ä–æ—Å—Ç–∞—è –±–∞—Ç—á–µ–≤–∞—è –≥–µ–Ω–µ—Ä–∞—Ü–∏—è.\n",
    "    –í–∞—Ä–∏–∞–Ω—Ç 1: —Ü–∏–∫–ª–æ–º (—á—Ç–æ–±—ã –Ω–µ –ª–æ–≤–∏—Ç—å OOM).\n",
    "    –í–∞—Ä–∏–∞–Ω—Ç 2: –æ–¥–∏–Ω –≤—ã–∑–æ–≤ .generate() –Ω–∞ –æ–±—ä–µ–¥–∏–Ω—ë–Ω–Ω–æ–º –±–∞—Ç—á–µ (—Å–ª–æ–∂–Ω–µ–µ –∏–∑-–∑–∞ —Ä–∞–∑–Ω–æ–π –¥–ª–∏–Ω—ã).\n",
    "    \"\"\"\n",
    "    outs = []\n",
    "    for p in prompts:\n",
    "        text = llm.generate(\n",
    "            p,\n",
    "            max_new_tokens=max_new_tokens,\n",
    "            temperature=temperature,\n",
    "            top_p=top_p,\n",
    "            top_k=top_k,\n",
    "            repetition_penalty=repetition_penalty,\n",
    "            stop=stop,\n",
    "            **gen_kwargs\n",
    "        )\n",
    "        outs.append(text)\n",
    "    return outs\n",
    "\n",
    "# –ü—Ä–∏–º–µ—Ä:\n",
    "# prompts = [format_prompt(\"–°–∏—Å—Ç–µ–º–∞\",\"–°–¥–µ–ª–∞–π 3 –∏–¥–µ–∏ —Å—Ç–∞—Ä—Ç–∞–ø–∞ –¥–ª—è —à–∫–æ–ª—ã\", style=\"mistral\") for _ in range(10)]\n",
    "# answers = batch_generate(llm, prompts)\n",
    "```\n",
    "\n",
    "---\n",
    "\n",
    "## üß© Few-shot: –∫–æ—Ä–æ—Ç–∫–∞—è —Å—Ö–µ–º–∞\n",
    "\n",
    "```python\n",
    "shots = [\n",
    "    (\"–ß—Ç–æ —Ç–∞–∫–æ–µ –≥—Ä–∞–¥–∏–µ–Ω—Ç?\", \"–ì—Ä–∞–¥–∏–µ–Ω—Ç ‚Äî —ç—Ç–æ –≤–µ–∫—Ç–æ—Ä —á–∞—Å—Ç–Ω—ã—Ö –ø—Ä–æ–∏–∑–≤–æ–¥–Ω—ã—Ö, —É–∫–∞–∑—ã–≤–∞—é—â–∏–π –Ω–∞–ø—Ä–∞–≤–ª–µ–Ω–∏–µ –Ω–∞–∏–±–æ–ª—å—à–µ–≥–æ —Ä–æ—Å—Ç–∞ —Ñ—É–Ω–∫—Ü–∏–∏.\"),\n",
    "    (\"–ê —á—Ç–æ —Ç–∞–∫–æ–µ —à–∞–≥ –≥—Ä–∞–¥–∏–µ–Ω—Ç–Ω–æ–≥–æ —Å–ø—É—Å–∫–∞?\", \"–≠—Ç–æ –≤–µ–ª–∏—á–∏–Ω–∞ –∏–∑–º–µ–Ω–µ–Ω–∏—è –ø–∞—Ä–∞–º–µ—Ç—Ä–æ–≤ –≤ –Ω–∞–ø—Ä–∞–≤–ª–µ–Ω–∏–∏ –∞–Ω—Ç–∏–≥—Ä–∞–¥–∏–µ–Ω—Ç–∞ –Ω–∞ –∫–∞–∂–¥–æ–π –∏—Ç–µ—Ä–∞—Ü–∏–∏.\")\n",
    "]\n",
    "\n",
    "prompt_fewshot = format_prompt(\n",
    "    system=\"–û—Ç–≤–µ—á–∞–π –∫—Ä–∞—Ç–∫–æ –∏ –ø–æ–Ω—è—Ç–Ω–æ.\",\n",
    "    user=\"–û–±—ä—è—Å–Ω–∏, —á—Ç–æ —Ç–∞–∫–æ–µ –ø–µ—Ä–µ–æ–±—É—á–µ–Ω–∏–µ –∏ –∫–∞–∫ –µ–≥–æ –∏–∑–±–µ–∂–∞—Ç—å?\",\n",
    "    style=\"llama2\",\n",
    "    shots=shots\n",
    ")\n",
    "\n",
    "# out = llm.generate(prompt_fewshot, max_new_tokens=200)\n",
    "# print(out)\n",
    "```\n",
    "\n",
    "---\n",
    "\n",
    "## üß∞ –°—Ç—Ä—É–∫—Ç—É—Ä–∏—Ä–æ–≤–∞–Ω–Ω—ã–π –æ—Ç–≤–µ—Ç (JSON-–Ω–∞–º—ë–∫)\n",
    "\n",
    "```python\n",
    "user = (\n",
    "    \"–°—Ñ–æ—Ä–º–∏—Ä—É–π JSON —Å –ø–æ–ª—è–º–∏: \"\n",
    "    \"title (—Å—Ç—Ä–æ–∫–∞), pros (—Å–ø–∏—Å–æ–∫), cons (—Å–ø–∏—Å–æ–∫) –ø–æ –º–æ–¥–µ–ª–∏ LightGBM.\"\n",
    ")\n",
    "\n",
    "prompt_json = format_prompt(\n",
    "    system=\"–û—Ç–≤–µ—á–∞–π –¢–û–õ–¨–ö–û –≤–∞–ª–∏–¥–Ω—ã–º JSON –±–µ–∑ –∫–æ–º–º–µ–Ω—Ç–∞—Ä–∏–µ–≤ –∏ –ª–∏—à–Ω–µ–≥–æ —Ç–µ–∫—Å—Ç–∞.\",\n",
    "    user=user,\n",
    "    style=\"chatml\"\n",
    ")\n",
    "\n",
    "text = llm.generate(prompt_json, max_new_tokens=200, temperature=0.2, top_p=0.9, do_sample=False)\n",
    "json_like = json_guardrail(text)\n",
    "print(json_like)\n",
    "```\n",
    "\n",
    "---\n",
    "\n",
    "## ‚ö†Ô∏è –ß–∞—Å—Ç—ã–µ –ø—Ä–æ–±–ª–µ–º—ã –∏ —á—Ç–æ –¥–µ–ª–∞—Ç—å\n",
    "\n",
    "```markdown\n",
    "<div>\n",
    "<ul>\n",
    "<li><b>–ù–µ—Ç –∏–Ω—Ç–µ—Ä–Ω–µ—Ç–∞ / –æ—Ñ–ª–∞–π–Ω:</b> –∏—Å–ø–æ–ª—å–∑—É–π <code>model_path</code> –Ω–∞ –ª–æ–∫–∞–ª—å–Ω—É—é –ø–∞–ø–∫—É –º–æ–¥–µ–ª–∏ (–≤–µ—Å–∞ —Å–∫–∞—á–∞–π –∑–∞—Ä–∞–Ω–µ–µ). –û—Ç–∫–ª—é—á–∞–π <code>trust_remote_code=False</code>, –µ—Å–ª–∏ –∫–æ–¥ –Ω–µ–¥–æ–≤–µ—Ä–µ–Ω–Ω—ã–π.</li>\n",
    "<li><b>CUDA OOM:</b> —É–º–µ–Ω—å—à–∏ <code>max_new_tokens</code>, <code>top_k</code>, –≤–∫–ª—é—á–∏ 4-–±–∏—Ç (<code>load_in_4bit=True</code>), –±–∞—Ç—á–∏ –≥–µ–Ω–µ—Ä–∏—Ä—É–π –ø–æ –æ–¥–Ω–æ–º—É.</li>\n",
    "<li><b>–ú–æ–¥–µ–ª—å –æ—Ç–≤–µ—á–∞–µ—Ç –º–µ–¥–ª–µ–Ω–Ω–æ:</b> <code>dtype=\"bf16/fp16\"</code>, <code>attn_implementation=\"flash_attention_2\"</code> (–µ—Å–ª–∏ –µ—Å—Ç—å —Å–±–æ—Ä–∫–∞), –ø–æ–º–µ–Ω—å—à–µ –∫–æ–Ω—Ç–µ–∫—Å—Ç, –æ—Ç–∫–ª—é—á–∏ –ª–∏—à–Ω–∏–µ —à–æ—Ç—ã.</li>\n",
    "<li><b>–ü–æ–≤—Ç–æ—Ä—ã –∏ ‚Äú–∑–∞–ª–∏–ø–∞–Ω–∏–µ‚Äù:</b> —É–≤–µ–ª–∏—á—å <code>repetition_penalty</code> (1.1‚Äì1.3), <code>no_repeat_ngram_size=3‚Äì5</code>, <code>temperature‚âà0.8</code>.</li>\n",
    "<li><b>–®—É–º/–≥–∞–ª–ª—é—Ü–∏–Ω–∞—Ü–∏–∏:</b> —É–º–µ–Ω—å—à–∏ <code>temperature</code> (0‚Äì0.5), <code>do_sample=False</code>, –¥–æ–±–∞–≤—å —Å—Ç—Ä–æ–≥—É—é —Å–∏—Å—Ç–µ–º–Ω—É—é –∏–Ω—Å—Ç—Ä—É–∫—Ü–∏—é.</li>\n",
    "<li><b>–ù—É–∂–Ω–æ —Å—Ç—Ä–æ–≥–æ –ø–æ —Ñ–æ—Ä–º–∞—Ç—É:</b> –¥–æ–±–∞–≤–ª—è–π –≤ —Å–∏—Å—Ç–µ–º–Ω—ã–π –ø—Ä–æ–º–ø—Ç —Ç—Ä–µ–±–æ–≤–∞–Ω–∏—è, –∏—Å–ø–æ–ª—å–∑—É–π post-processing (–∫–∞–∫ <code>json_guardrail</code>), –ø—Ä–∏ –Ω–µ–æ–±—Ö–æ–¥–∏–º–æ—Å—Ç–∏ –Ω–µ—Å–∫–æ–ª—å–∫–æ –ø–æ–ø—ã—Ç–æ–∫ (self-repair).</li>\n",
    "</ul>\n",
    "</div>\n",
    "```\n",
    "\n",
    "---\n",
    "\n",
    "–ï—Å–ª–∏ –Ω—É–∂–Ω–æ ‚Äî —Å–¥–µ–ª–∞—é —Ä—è–¥–æ–º **–º–∏–Ω–∏–º–∞–ª—å–Ω—É—é .py –≤–µ—Ä—Å–∏—é** –±–µ–∑ HTML, –∏–ª–∏ –¥–æ–±–∞–≤–ª—é **vLLM/TGI** –æ–±—ë—Ä—Ç–∫–∏ –ø–æ–¥ —Å–µ—Ä–≤–µ—Ä–Ω—É—é –≥–µ–Ω–µ—Ä–∞—Ü–∏—é.\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "e134b63b-fc62-4717-a15b-81f8ea1cda30",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.13.6"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
