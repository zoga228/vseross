{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "4bd40fcc-169f-49a6-b1b4-386ce35c74eb",
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "import pandas as pd\n",
    "from sklearn.model_selection import train_test_split\n",
    "from sklearn.linear_model import LogisticRegression\n",
    "from sklearn.metrics import accuracy_score, classification_report\n",
    "import nltk\n",
    "from nltk.tokenize import word_tokenize\n",
    "import gensim.downloader as api\n",
    "\n",
    "# Загрузка данных (пример с IMDB)\n",
    "# На реальных данных используйте:\n",
    "# data = pd.read_csv('ваш_файл.csv')\n",
    "data = pd.DataFrame({\n",
    "    'text': [\n",
    "        'Этот фильм просто потрясающий, я в восторге!',\n",
    "        'Ужасный фильм, худший из того, что я видел',\n",
    "        'Отличный сюжет и великолепная игра актеров',\n",
    "        'Не рекомендую, пустая трата времени',\n",
    "        'Шедевр кинематографа, обязательно к просмотру'\n",
    "    ],\n",
    "    'label': [1, 0, 1, 0, 1]  # 1 - положительный, 0 - отрицательный\n",
    "})\n",
    "\n",
    "# Загрузка предобученной модели Word2Vec\n",
    "word2vec_model = api.load('word2vec-google-news-300')\n",
    "\n",
    "# Функция для получения эмбеддинга текста\n",
    "def get_document_embedding(text, model):\n",
    "    # Токенизация\n",
    "    tokens = word_tokenize(text.lower())\n",
    "    \n",
    "    # Получение эмбеддингов для каждого слова, если оно есть в модели\n",
    "    embeddings = [model[word] for word in tokens if word in model]\n",
    "    \n",
    "    # Если ни одного слова не найдено, возвращаем нулевой вектор\n",
    "    if len(embeddings) == 0:\n",
    "        return np.zeros(model.vector_size)\n",
    "    \n",
    "    # Вычисление среднего вектора для всего документа\n",
    "    return np.mean(embeddings, axis=0)\n",
    "\n",
    "# Преобразование текстов в эмбеддинги\n",
    "X = np.array([get_document_embedding(text, word2vec_model) for text in data['text']])\n",
    "y = data['label']\n",
    "\n",
    "# Разделение на обучающую и тестовую выборки\n",
    "X_train, X_test, y_train, y_test = train_test_split(X, y, test_size=0.2, random_state=42)\n",
    "\n",
    "# Обучение модели логистической регрессии\n",
    "model = LogisticRegression()\n",
    "model.fit(X_train, y_train)\n",
    "\n",
    "# Прогнозирование и оценка\n",
    "y_pred = model.predict(X_test)\n",
    "print(f\"Точность: {accuracy_score(y_test, y_pred)}\")\n",
    "print(classification_report(y_test, y_pred))\n",
    "\n",
    "# Проверка модели на новом тексте\n",
    "new_text = \"Этот фильм действительно впечатляет своей глубиной\"\n",
    "new_embedding = get_document_embedding(new_text, word2vec_model)\n",
    "prediction = model.predict([new_embedding])[0]\n",
    "print(f\"Предсказание для '{new_text}': {'положительный' if prediction == 1 else 'отрицательный'}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "e3adecea-4e7e-4535-9cf0-41d44157738c",
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "import pandas as pd\n",
    "from sklearn.model_selection import train_test_split\n",
    "from sklearn.ensemble import RandomForestClassifier\n",
    "from sklearn.metrics import accuracy_score, classification_report\n",
    "from transformers import BertTokenizer, BertModel\n",
    "import torch\n",
    "\n",
    "# Загрузка тех же данных\n",
    "data = pd.DataFrame({\n",
    "    'text': [\n",
    "        'Этот фильм просто потрясающий, я в восторге!',\n",
    "        'Ужасный фильм, худший из того, что я видел',\n",
    "        'Отличный сюжет и великолепная игра актеров',\n",
    "        'Не рекомендую, пустая трата времени',\n",
    "        'Шедевр кинематографа, обязательно к просмотру'\n",
    "    ],\n",
    "    'label': [1, 0, 1, 0, 1]  # 1 - положительный, 0 - отрицательный\n",
    "})\n",
    "\n",
    "# Загрузка токенизатора и модели BERT\n",
    "tokenizer = BertTokenizer.from_pretrained('bert-base-multilingual-cased')\n",
    "model = BertModel.from_pretrained('bert-base-multilingual-cased')\n",
    "\n",
    "# Функция для получения эмбеддинга с использованием BERT\n",
    "def get_bert_embedding(text, tokenizer, model):\n",
    "    # Токенизация и подготовка входных данных\n",
    "    inputs = tokenizer(text, return_tensors='pt', padding=True, truncation=True, max_length=512)\n",
    "    \n",
    "    # Получение эмбеддингов из BERT\n",
    "    with torch.no_grad():\n",
    "        outputs = model(**inputs)\n",
    "    \n",
    "    # Используем векторное представление [CLS] токена в качестве эмбеддинга всего предложения\n",
    "    return outputs.last_hidden_state[:, 0, :].numpy().flatten()\n",
    "\n",
    "# Преобразование текстов в эмбеддинги BERT\n",
    "X = np.array([get_bert_embedding(text, tokenizer, model) for text in data['text']])\n",
    "y = data['label']\n",
    "\n",
    "# Разделение на обучающую и тестовую выборки\n",
    "X_train, X_test, y_train, y_test = train_test_split(X, y, test_size=0.2, random_state=42)\n",
    "\n",
    "# Обучение модели случайного леса\n",
    "rf_model = RandomForestClassifier(n_estimators=100, random_state=42)\n",
    "rf_model.fit(X_train, y_train)\n",
    "\n",
    "# Прогнозирование и оценка\n",
    "y_pred = rf_model.predict(X_test)\n",
    "print(f\"Точность: {accuracy_score(y_test, y_pred)}\")\n",
    "print(classification_report(y_test, y_pred))\n",
    "\n",
    "# Проверка модели на новом тексте\n",
    "new_text = \"Этот фильм действительно впечатляет своей глубиной\"\n",
    "new_embedding = get_bert_embedding(new_text, tokenizer, model)\n",
    "prediction = rf_model.predict([new_embedding])[0]\n",
    "print(f\"Предсказание для '{new_text}': {'положительный' if prediction == 1 else 'отрицательный'}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "480769cd-cbd2-4cce-9128-3ee6054db141",
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "import pandas as pd\n",
    "from sklearn.model_selection import cross_val_score, GridSearchCV\n",
    "from sklearn.svm import SVC\n",
    "from sklearn.preprocessing import StandardScaler\n",
    "from sklearn.pipeline import Pipeline\n",
    "import nltk\n",
    "from nltk.corpus import stopwords\n",
    "from nltk.tokenize import word_tokenize\n",
    "from nltk.stem import WordNetLemmatizer\n",
    "import gensim.downloader as api\n",
    "import matplotlib.pyplot as plt\n",
    "from sklearn.metrics import confusion_matrix, ConfusionMatrixDisplay\n",
    "\n",
    "# Загрузка данных (пример)\n",
    "data = pd.DataFrame({\n",
    "    'text': [\n",
    "        'Этот фильм просто потрясающий, я в восторге!',\n",
    "        'Ужасный фильм, худший из того, что я видел',\n",
    "        'Отличный сюжет и великолепная игра актеров',\n",
    "        'Не рекомендую, пустая трата времени',\n",
    "        'Шедевр кинематографа, обязательно к просмотру',\n",
    "        'Сценарий слабый, но актеры старались',\n",
    "        'В целом неплохо, но ожидал большего',\n",
    "        'Разочарование года, не советую тратить деньги',\n",
    "        'Потрясающий визуальный ряд и спецэффекты',\n",
    "        'Затянуто и скучно, уснул в середине фильма'\n",
    "    ],\n",
    "    'label': [1, 0, 1, 0, 1, 0, 0, 0, 1, 0]  # 1 - положительный, 0 - отрицательный\n",
    "})\n",
    "\n",
    "# Загрузка стоп-слов и инициализация лемматизатора\n",
    "nltk.download('stopwords')\n",
    "nltk.download('wordnet')\n",
    "nltk.download('punkt')\n",
    "stop_words = set(stopwords.words('russian') + stopwords.words('english'))\n",
    "lemmatizer = WordNetLemmatizer()\n",
    "\n",
    "# Загрузка предобученной модели GloVe\n",
    "glove_model = api.load('glove-wiki-gigaword-300')\n",
    "\n",
    "# Функция предобработки текста\n",
    "def preprocess_text(text):\n",
    "    tokens = word_tokenize(text.lower())\n",
    "    tokens = [lemmatizer.lemmatize(token) for token in tokens if token.isalpha() and token not in stop_words]\n",
    "    return tokens\n",
    "\n",
    "# Функция для получения эмбеддинга документа\n",
    "def get_glove_embedding(text, model):\n",
    "    tokens = preprocess_text(text)\n",
    "    embeddings = [model[word] for word in tokens if word in model]\n",
    "    if len(embeddings) == 0:\n",
    "        return np.zeros(model.vector_size)\n",
    "    return np.mean(embeddings, axis=0)\n",
    "\n",
    "# Преобразование текстов в эмбеддинги\n",
    "X = np.array([get_glove_embedding(text, glove_model) for text in data['text']])\n",
    "y = data['label']\n",
    "\n",
    "# Создание пайплайна с масштабированием и SVM\n",
    "pipeline = Pipeline([\n",
    "    ('scaler', StandardScaler()),\n",
    "    ('svm', SVC(probability=True))\n",
    "])\n",
    "\n",
    "# Определение параметров для поиска\n",
    "param_grid = {\n",
    "    'svm__C': [0.1, 1, 10, 100],\n",
    "    'svm__kernel': ['linear', 'rbf'],\n",
    "    'svm__gamma': ['scale', 'auto', 0.1, 0.01]\n",
    "}\n",
    "\n",
    "# Поиск лучших гиперпараметров с кросс-валидацией\n",
    "grid_search = GridSearchCV(pipeline, param_grid, cv=5, scoring='f1')\n",
    "grid_search.fit(X, y)\n",
    "\n",
    "print(f\"Лучшие параметры: {grid_search.best_params_}\")\n",
    "print(f\"Лучший f1-score: {grid_search.best_score_:.4f}\")\n",
    "\n",
    "# Оценка лучшей модели с использованием кросс-валидации\n",
    "best_model = grid_search.best_estimator_\n",
    "cv_scores = cross_val_score(best_model, X, y, cv=5, scoring='f1')\n",
    "print(f\"Средний f1-score при 5-кратной кросс-валидации: {cv_scores.mean():.4f}\")\n",
    "\n",
    "# Обучение и предсказание для визуализации матрицы неточностей\n",
    "best_model.fit(X, y)\n",
    "y_pred = best_model.predict(X)\n",
    "\n",
    "# Визуализация матрицы неточностей\n",
    "cm = confusion_matrix(y, y_pred)\n",
    "disp = ConfusionMatrixDisplay(confusion_matrix=cm, display_labels=['Негативный', 'Позитивный'])\n",
    "disp.plot(cmap='Blues')\n",
    "plt.title('Матрица неточностей')\n",
    "plt.show()\n",
    "\n",
    "# Анализ ошибок классификации\n",
    "misclassified = data[y != y_pred].copy()\n",
    "misclassified['predicted'] = best_model.predict(X[y != y_pred])\n",
    "print(\"\\nОшибочно классифицированные примеры:\")\n",
    "for idx, row in misclassified.iterrows():\n",
    "    print(f\"Текст: '{row['text']}'\")\n",
    "    print(f\"Истинная метка: {'Позитивный' if row['label'] == 1 else 'Негативный'}\")\n",
    "    print(f\"Предсказанная метка: {'Позитивный' if row['predicted'] == 1 else 'Негативный'}\")\n",
    "    print(\"-\" * 50)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "d40ac44b-349b-4e7d-9b0e-2aca1856c1b9",
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "import pandas as pd\n",
    "from sklearn.model_selection import train_test_split\n",
    "from sklearn.ensemble import VotingClassifier\n",
    "from sklearn.svm import SVC\n",
    "from sklearn.tree import DecisionTreeClassifier\n",
    "from sklearn.linear_model import LogisticRegression\n",
    "from sklearn.metrics import classification_report, accuracy_score\n",
    "from sentence_transformers import SentenceTransformer\n",
    "import matplotlib.pyplot as plt\n",
    "from sklearn.decomposition import PCA\n",
    "from sklearn.manifold import TSNE\n",
    "\n",
    "# Пример данных (категории новостей)\n",
    "data = pd.DataFrame({\n",
    "    'text': [\n",
    "        'Парламент принял новый закон о налогообложении',\n",
    "        'Сборная выиграла чемпионат мира по футболу',\n",
    "        'Ученые обнаружили новый вид рыб в Тихом океане',\n",
    "        'Индекс S&P 500 показал рекордный рост',\n",
    "        'Депутаты обсуждают поправки в конституцию',\n",
    "        'Баскетбольная команда выиграла национальный кубок',\n",
    "        'Исследователи разработали новый метод лечения рака',\n",
    "        'Центральный банк снизил ключевую ставку',\n",
    "        'Президент подписал указ о новых экономических мерах',\n",
    "        'Олимпийская сборная завоевала 10 золотых медалей',\n",
    "        'Ученые запустили зонд к Марсу',\n",
    "        'Биржевые индексы упали на фоне новостей из Китая'\n",
    "    ],\n",
    "    'category': [\n",
    "        'политика', 'спорт', 'наука', 'экономика',\n",
    "        'политика', 'спорт', 'наука', 'экономика',\n",
    "        'политика', 'спорт', 'наука', 'экономика'\n",
    "    ]\n",
    "})\n",
    "\n",
    "# Загрузка модели Sentence-BERT\n",
    "sentence_model = SentenceTransformer('distiluse-base-multilingual-cased-v1')\n",
    "\n",
    "# Получение эмбеддингов предложений\n",
    "embeddings = sentence_model.encode(data['text'].tolist())\n",
    "\n",
    "# Кодирование категорий\n",
    "from sklearn.preprocessing import LabelEncoder\n",
    "le = LabelEncoder()\n",
    "y = le.fit_transform(data['category'])\n",
    "\n",
    "# Разделение на обучающую и тестовую выборки\n",
    "X_train, X_test, y_train, y_test = train_test_split(embeddings, y, test_size=0.3, random_state=42, stratify=y)\n",
    "\n",
    "# Создание ансамбля классификаторов\n",
    "estimators = [\n",
    "    ('svm', SVC(probability=True, kernel='linear')),\n",
    "    ('dt', DecisionTreeClassifier()),\n",
    "    ('lr', LogisticRegression())\n",
    "]\n",
    "ensemble = VotingClassifier(estimators=estimators, voting='soft')\n",
    "\n",
    "# Обучение ансамбля\n",
    "ensemble.fit(X_train, y_train)\n",
    "\n",
    "# Оценка модели\n",
    "y_pred = ensemble.predict(X_test)\n",
    "print(f\"Точность ансамбля: {accuracy_score(y_test, y_pred)}\")\n",
    "print(classification_report(y_test, y_pred, target_names=le.classes_))\n",
    "\n",
    "# Визуализация эмбеддингов с помощью t-SNE\n",
    "pca = PCA(n_components=50)\n",
    "reduced_embeddings = pca.fit_transform(embeddings)\n",
    "\n",
    "tsne = TSNE(n_components=2, random_state=42)\n",
    "tsne_embeddings = tsne.fit_transform(reduced_embeddings)\n",
    "\n",
    "plt.figure(figsize=(10, 8))\n",
    "for cat_id, cat_name in enumerate(le.classes_):\n",
    "    idx = y == cat_id\n",
    "    plt.scatter(tsne_embeddings[idx, 0], tsne_embeddings[idx, 1], label=cat_name)\n",
    "\n",
    "plt.legend()\n",
    "plt.title('t-SNE визуализация эмбеддингов новостей')\n",
    "plt.xlabel('t-SNE 1')\n",
    "plt.ylabel('t-SNE 2')\n",
    "plt.show()\n",
    "\n",
    "# Проверка модели на новых текстах\n",
    "new_texts = [\n",
    "    'Правительство обсуждает новые меры поддержки малого бизнеса',\n",
    "    'Теннисист выиграл турнир Большого шлема',\n",
    "    'Исследователи выявили новые свойства графена',\n",
    "    'Инфляция снизилась до рекордных значений'\n",
    "]\n",
    "\n",
    "new_embeddings = sentence_model.encode(new_texts)\n",
    "predictions = ensemble.predict(new_embeddings)\n",
    "pred_proba = ensemble.predict_proba(new_embeddings)\n",
    "\n",
    "print(\"\\nПредсказания для новых текстов:\")\n",
    "for i, text in enumerate(new_texts):\n",
    "    predicted_class = le.classes_[predictions[i]]\n",
    "    class_probas = {le.classes_[j]: prob for j, prob in enumerate(pred_proba[i])}\n",
    "    top_classes = sorted(class_probas.items(), key=lambda x: x[1], reverse=True)\n",
    "    \n",
    "    print(f\"Текст: '{text}'\")\n",
    "    print(f\"Предсказанная категория: {predicted_class}\")\n",
    "    print(f\"Вероятности: \", end=\"\")\n",
    "    for cls, prob in top_classes:\n",
    "        print(f\"{cls}: {prob:.4f}, \", end=\"\")\n",
    "    print(\"\\n\" + \"-\" * 50)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "0de0f9df-ad3d-492b-8163-adcce7185b82",
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "import pandas as pd\n",
    "import torch\n",
    "import torch.nn as nn\n",
    "import torch.optim as optim\n",
    "from torch.utils.data import Dataset, DataLoader\n",
    "from sklearn.model_selection import train_test_split\n",
    "from sklearn.metrics import classification_report\n",
    "from sklearn.preprocessing import LabelEncoder\n",
    "from transformers import BertModel, BertTokenizer, AdamW, get_linear_schedule_with_warmup\n",
    "import matplotlib.pyplot as plt\n",
    "from tqdm import tqdm\n",
    "import seaborn as sns\n",
    "import nltk\n",
    "from nltk.corpus import stopwords\n",
    "import random\n",
    "\n",
    "# Устанавливаем seed для воспроизводимости\n",
    "def set_seed(seed):\n",
    "    random.seed(seed)\n",
    "    np.random.seed(seed)\n",
    "    torch.manual_seed(seed)\n",
    "    torch.cuda.manual_seed_all(seed)\n",
    "    \n",
    "set_seed(42)\n",
    "\n",
    "# Проверка наличия GPU\n",
    "device = torch.device(\"cuda\" if torch.cuda.is_available() else \"cpu\")\n",
    "print(f\"Используется устройство: {device}\")\n",
    "\n",
    "# Пример данных (большее количество для эксперта)\n",
    "# В реальном сценарии используйте реальный датасет\n",
    "data = pd.DataFrame({\n",
    "    'text': [\n",
    "        'Парламент принял новый закон о налогообложении предприятий малого бизнеса',\n",
    "        'Сборная России выиграла чемпионат мира по футболу в напряженном матче',\n",
    "        'Ученые обнаружили новый вид глубоководных рыб в Тихом океане',\n",
    "        'Индекс S&P 500 показал рекордный рост на фоне позитивных новостей',\n",
    "        'Депутаты обсуждают поправки в конституцию о социальных гарантиях',\n",
    "        'Баскетбольная команда выиграла национальный кубок в финальном матче',\n",
    "        'Исследователи разработали новый метод лечения онкологических заболеваний',\n",
    "        'Центральный банк снизил ключевую ставку до исторического минимума',\n",
    "        'Президент подписал указ о новых экономических мерах поддержки',\n",
    "        'Олимпийская сборная завоевала десять золотых медалей на соревнованиях',\n",
    "        'Ученые запустили исследовательский зонд к поверхности Марса',\n",
    "        'Биржевые индексы упали на фоне новостей из Китая о замедлении роста',\n",
    "        'Премьер-министр провел переговоры с лидерами европейских стран',\n",
    "        'Чемпион мира по боксу защитил свой титул в тяжелом бою',\n",
    "        'Астрономы наблюдали редкое космическое явление в галактике Андромеды',\n",
    "        'Нефтяные котировки выросли после заседания ОПЕК о сокращении добычи',\n",
    "        'Министр иностранных дел выступил с заявлением о ситуации на Ближнем Востоке',\n",
    "        'Хоккейная команда одержала победу в серии пенальти в финале чемпионата',\n",
    "        'Генетики расшифровали геном редкого вида насекомых из тропиков',\n",
    "        'Эксперты прогнозируют рост ВВП страны на 3% в следующем квартале'\n",
    "    ],\n",
    "    'category': [\n",
    "        'политика', 'спорт', 'наука', 'экономика',\n",
    "        'политика', 'спорт', 'наука', 'экономика',\n",
    "        'политика', 'спорт', 'наука', 'экономика',\n",
    "        'политика', 'спорт', 'наука', 'экономика',\n",
    "        'политика', 'спорт', 'наука', 'экономика'\n",
    "    ]\n",
    "})\n",
    "\n",
    "# Кодирование категорий\n",
    "le = LabelEncoder()\n",
    "data['label'] = le.fit_transform(data['category'])\n",
    "num_classes = len(le.classes_)\n",
    "\n",
    "# Класс для создания датасета PyTorch\n",
    "class TextClassificationDataset(Dataset):\n",
    "    def __init__(self, texts, labels, tokenizer, max_length=128):\n",
    "        self.texts = texts\n",
    "        self.labels = labels\n",
    "        self.tokenizer = tokenizer\n",
    "        self.max_length = max_length\n",
    "        \n",
    "    def __len__(self):\n",
    "        return len(self.texts)\n",
    "    \n",
    "    def __getitem__(self, idx):\n",
    "        text = self.texts[idx]\n",
    "        label = self.labels[idx]\n",
    "        \n",
    "        # Токенизация текста\n",
    "        encoding = self.tokenizer(\n",
    "            text,\n",
    "            add_special_tokens=True,\n",
    "            max_length=self.max_length,\n",
    "            return_token_type_ids=True,\n",
    "            padding='max_length',\n",
    "            truncation=True,\n",
    "            return_attention_mask=True,\n",
    "            return_tensors='pt'\n",
    "        )\n",
    "        \n",
    "        return {\n",
    "            'input_ids': encoding['input_ids'].flatten(),\n",
    "            'attention_mask': encoding['attention_mask'].flatten(),\n",
    "            'token_type_ids': encoding['token_type_ids'].flatten(),\n",
    "            'label': torch.tensor(label, dtype=torch.long)\n",
    "        }\n",
    "\n",
    "# Модель с механизмом внимания поверх BERT\n",
    "class BertWithAttention(nn.Module):\n",
    "    def __init__(self, bert_model_name, num_classes, dropout_rate=0.3):\n",
    "        super(BertWithAttention, self).__init__()\n",
    "        \n",
    "        # Загрузка предобученной модели BERT\n",
    "        self.bert = BertModel.from_pretrained(bert_model_name)\n",
    "        \n",
    "        # Размер скрытого состояния BERT\n",
    "        self.hidden_size = self.bert.config.hidden_size\n",
    "        \n",
    "        # Механизм внимания\n",
    "        self.attention = nn.Sequential(\n",
    "            nn.Linear(self.hidden_size, 128),\n",
    "            nn.Tanh(),\n",
    "            nn.Linear(128, 1),\n",
    "            nn.Softmax(dim=1)\n",
    "        )\n",
    "        \n",
    "        # Слои классификатора\n",
    "        self.classifier = nn.Sequential(\n",
    "            nn.Linear(self.hidden_size, 256),\n",
    "            nn.ReLU(),\n",
    "            nn.Dropout(dropout_rate),\n",
    "            nn.Linear(256, num_classes)\n",
    "        )\n",
    "    \n",
    "    def forward(self, input_ids, attention_mask, token_type_ids):\n",
    "        # Получаем выходы из BERT\n",
    "        outputs = self.bert(\n",
    "            input_ids=input_ids,\n",
    "            attention_mask=attention_mask,\n",
    "            token_type_ids=token_type_ids\n",
    "        )\n",
    "        \n",
    "        # Последние скрытые состояния (batch_size, sequence_length, hidden_size)\n",
    "        sequence_output = outputs.last_hidden_state\n",
    "        \n",
    "        # Применяем механизм внимания к последовательности токенов\n",
    "        attention_weights = self.attention(sequence_output)\n",
    "        context_vector = torch.sum(attention_weights * sequence_output, dim=1)\n",
    "        \n",
    "        # Классификация на основе взвешенного вектора контекста\n",
    "        logits = self.classifier(context_vector)\n",
    "        \n",
    "        return logits, attention_weights\n",
    "\n",
    "# Подготовка данных\n",
    "tokenizer = BertTokenizer.from_pretrained('bert-base-multilingual-cased')\n",
    "\n",
    "# Разделение на обучающую и тестовую выборки\n",
    "X_train, X_test, y_train, y_test = train_test_split(\n",
    "    data['text'].values, data['label'].values, \n",
    "    test_size=0.2, random_state=42, stratify=data['label']\n",
    ")\n",
    "\n",
    "# Создание датасетов\n",
    "train_dataset = TextClassificationDataset(X_train, y_train, tokenizer)\n",
    "test_dataset = TextClassificationDataset(X_test, y_test, tokenizer)\n",
    "\n",
    "# Создание загрузчиков данных\n",
    "batch_size = 4\n",
    "train_loader = DataLoader(train_dataset, batch_size=batch_size, shuffle=True)\n",
    "test_loader = DataLoader(test_dataset, batch_size=batch_size)\n",
    "\n",
    "# Инициализация модели\n",
    "model = BertWithAttention('bert-base-multilingual-cased', num_classes)\n",
    "model = model.to(device)\n",
    "\n",
    "# Параметры оптимизации\n",
    "optimizer = AdamW(\n",
    "    [\n",
    "        {\"params\": model.bert.parameters(), \"lr\": 2e-5},\n",
    "        {\"params\": model.classifier.parameters(), \"lr\": 1e-3},\n",
    "        {\"params\": model.attention.parameters(), \"lr\": 1e-3}\n",
    "    ],\n",
    "    weight_decay=0.01\n",
    ")\n",
    "\n",
    "# Количество шагов обучения\n",
    "num_epochs = 5\n",
    "total_steps = len(train_loader) * num_epochs\n",
    "\n",
    "# Планировщик скорости обучения с разогревом\n",
    "scheduler = get_linear_schedule_with_warmup(\n",
    "    optimizer,\n",
    "    num_warmup_steps=0,\n",
    "    num_training_steps=total_steps\n",
    ")\n",
    "\n",
    "# Функция обучения\n",
    "def train_epoch(model, data_loader, optimizer, scheduler, device):\n",
    "    model.train()\n",
    "    total_loss = 0\n",
    "    \n",
    "    progress_bar = tqdm(data_loader, desc=\"Training\")\n",
    "    for batch in progress_bar:\n",
    "        # Извлечение данных из батча\n",
    "        input_ids = batch['input_ids'].to(device)\n",
    "        attention_mask = batch['attention_mask'].to(device)\n",
    "        token_type_ids = batch['token_type_ids'].to(device)\n",
    "        labels = batch['label'].to(device)\n",
    "        \n",
    "        # Очистка градиентов\n",
    "        optimizer.zero_grad()\n",
    "        \n",
    "        # Прямой проход\n",
    "        logits, _ = model(input_ids, attention_mask, token_type_ids)\n",
    "        \n",
    "        # Функция потерь (кросс-энтропия)\n",
    "        loss_fn = nn.CrossEntropyLoss()\n",
    "        loss = loss_fn(logits, labels)\n",
    "        \n",
    "        # Обратное распространение ошибки\n",
    "        loss.backward()\n",
    "        \n",
    "        # Предотвращение взрыва градиентов\n",
    "        torch.nn.utils.clip_grad_norm_(model.parameters(), max_norm=1.0)\n",
    "        \n",
    "        # Обновление весов\n",
    "        optimizer.step()\n",
    "        scheduler.step()\n",
    "        \n",
    "        total_loss += loss.item()\n",
    "        progress_bar.set_postfix({\"loss\": loss.item()})\n",
    "    \n",
    "    return total_loss / len(data_loader)\n",
    "\n",
    "# Функция оценки\n",
    "def evaluate(model, data_loader, device):\n",
    "    model.eval()\n",
    "    predictions = []\n",
    "    true_labels = []\n",
    "    attention_maps = []\n",
    "    \n",
    "    with torch.no_grad():\n",
    "        for batch in tqdm(data_loader, desc=\"Evaluating\"):\n",
    "            # Извлечение данных из батча\n",
    "            input_ids = batch['input_ids'].to(device)\n",
    "            attention_mask = batch['attention_mask'].to(device)\n",
    "            token_type_ids = batch['token_type_ids'].to(device)\n",
    "            labels = batch['label'].to(device)\n",
    "            \n",
    "            # Прямой проход\n",
    "            logits, attention_weights = model(input_ids, attention_mask, token_type_ids)\n",
    "            \n",
    "            # Сохранение весов внимания\n",
    "            attention_maps.append(attention_weights.cpu().numpy())\n",
    "            \n",
    "            # Получение предсказаний\n",
    "            _, preds = torch.max(logits, dim=1)\n",
    "            \n",
    "            # Сохранение предсказаний и истинных меток\n",
    "            predictions.extend(preds.cpu().tolist())\n",
    "            true_labels.extend(labels.cpu().tolist())\n",
    "    \n",
    "    return predictions, true_labels, np.vstack(attention_maps)\n",
    "\n",
    "# Обучение модели\n",
    "train_losses = []\n",
    "\n",
    "for epoch in range(num_epochs):\n",
    "    print(f\"Эпоха {epoch+1}/{num_epochs}\")\n",
    "    \n",
    "    # Обучение на одной эпохе\n",
    "    train_loss = train_epoch(model, train_loader, optimizer, scheduler, device)\n",
    "    train_losses.append(train_loss)\n",
    "    \n",
    "    print(f\"Средняя потеря на обучении: {train_loss:.4f}\")\n",
    "    \n",
    "    # Оценка на тестовых данных\n",
    "    predictions, true_labels, _ = evaluate(model, test_loader, device)\n",
    "    \n",
    "    # Вывод метрик классификации\n",
    "    print(\"\\nМетрики на тестовой выборке:\")\n",
    "    print(classification_report(true_labels, predictions, target_names=le.classes_))\n",
    "    print(\"-\" * 50)\n",
    "\n",
    "# Визуализация процесса обучения\n",
    "plt.figure(figsize=(10, 6))\n",
    "plt.plot(range(1, num_epochs+1), train_losses, 'b-o', label='Потери на обучении')\n",
    "plt.title('Динамика функции потерь во время обучения')\n",
    "plt.xlabel('Эпоха')\n",
    "plt.ylabel('Потеря')\n",
    "plt.legend()\n",
    "plt.grid(True)\n",
    "plt.show()\n",
    "\n",
    "# Оценка модели на тестовых данных\n",
    "test_predictions, test_true_labels, attention_maps = evaluate(model, test_loader, device)\n",
    "\n",
    "# Визуализация внимания на тестовых примерах\n",
    "def visualize_attention(text, attention_weights, tokenizer):\n",
    "    # Токенизация текста\n",
    "    tokens = tokenizer.tokenize(tokenizer.decode(tokenizer.encode(text)))\n",
    "    \n",
    "    # Получение весов внимания для токенов\n",
    "    attention = attention_weights.reshape(-1)[:len(tokens)]\n",
    "    \n",
    "    # Создание визуализации\n",
    "    plt.figure(figsize=(10, 6))\n",
    "    sns.barplot(x=attention, y=tokens)\n",
    "    plt.title('Веса внимания для каждого токена')\n",
    "    plt.xlabel('Вес внимания')\n",
    "    plt.tight_layout()\n",
    "    plt.show()\n",
    "\n",
    "# Выбор случайного примера для визуализации\n",
    "sample_idx = np.random.randint(len(X_test))\n",
    "sample_text = X_test[sample_idx]\n",
    "sample_label = y_test[sample_idx]\n",
    "sample_pred = test_predictions[sample_idx]\n",
    "sample_attention = attention_maps[sample_idx // batch_size][sample_idx % batch_size]\n",
    "\n",
    "print(f\"Текст: '{sample_text}'\")\n",
    "print(f\"Истинная категория: {le.classes_[sample_label]}\")\n",
    "print(f\"Предсказанная категория: {le.classes_[sample_pred]}\")\n",
    "visualize_attention(sample_text, sample_attention, tokenizer)\n",
    "\n",
    "# Интерпретация результатов с помощью SHAP\n",
    "import shap\n",
    "\n",
    "# Создаем объяснительную модель\n",
    "explainer = shap.Explainer(lambda x: model(\n",
    "    torch.tensor(tokenizer(x, padding=True, truncation=True, return_tensors=\"pt\")[\"input_ids\"]).to(device),\n",
    "    torch.tensor(tokenizer(x, padding=True, truncation=True, return_tensors=\"pt\")[\"attention_mask\"]).to(device),\n",
    "    torch.tensor(tokenizer(x, padding=True, truncation=True, return_tensors=\"pt\")[\"token_type_ids\"]).to(device)\n",
    ")[0].detach().cpu().numpy(), tokenizer)\n",
    "\n",
    "# Выбираем несколько примеров для объяснения\n",
    "examples = X_test[:5]\n",
    "shap_values = explainer(examples)\n",
    "\n",
    "# Визуализация SHAP значений\n",
    "shap.plots.text(shap_values)"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.13.2"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
